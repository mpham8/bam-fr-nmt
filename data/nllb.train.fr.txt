Lillian Diana Gish (14 octobre 1893 - 27 février 1993) était une actrice, réalisatrice et scénariste américaine.
Gish était une star de cinéma de premier plan de 1912 aux années 1920, étant particulièrement associée aux films du réalisateur DW Griffith.
Elle a également fait un travail considérable à la télévision du début des années 1950 aux années 1980 et a clôturé sa carrière en jouant aux côtés de Bette Davis dans le film de 1987 Les baleines d'août.
Les premières générations de Gish étaient des ministres Dunkard.
Leur mère a ouvert le Majestic Candy Kitchen et les filles ont aidé à vendre du pop-corn et des bonbons aux clients de l'ancien Majestic Theatre, situé à côté.
Lillian, âgée de dix-sept ans, s'est rendue à Shawnee, dans l'Oklahoma, où vivaient le frère de James, Alfred Grant Gish, et sa femme, Maude.
Son père est mort à Norman, Oklahoma, en 1912, mais elle était retournée dans l'Ohio quelques mois auparavant.
Lorsque Lillian et Dorothy étaient assez âgées, elles ont rejoint le théâtre, voyageant souvent séparément dans différentes productions.
Gish a continué à se produire sur scène et en 1913, lors d'une série de A Good Little Devil , elle s'est effondrée à cause de l'anémie.
Sa performance dans ces conditions glaciales lui a donné un dabame nerveux durable dans plusieurs doigts.
Il a utilisé ses talents expressifs au maximum, la transformant en une héroïne souffrante mais forte.
Elle a dirigé sa sœur Dorothy dans un film, Remodeling Her Husband (1920), lorsque DW Griffith a emmené son unité sur place.
Elle a refusé l'argent, demandant un salaire plus modeste et un pourcentage afin que le studio puisse utiliser les fonds pour augmenter la qualité de ses films - embaucher les meilleurs acteurs, scénaristes, etc.
Bon nombre des principales dames de l'ère du silence, telles que Gish et Pickford, avaient été saines et innocentes, mais au début des années 1930 (après l'adoption complète du son et avant l'application du code de production cinématographique), ces rôles étaient perçus comme obsolètes.
Louis Mayer voulait mettre en scène un scandale ("la faire tomber de son piédestal") pour susciter la sympathie du public pour Gish, mais Lillian ne voulait pas jouer à la fois à l'écran et hors tension, et retourna à son premier amour, le théâtre.
De retour au cinéma, Gish a été nominé pour l'Oscar de la meilleure actrice dans un second rôle en 1946 pour Duel au soleil.
Elle a été considérée pour divers rôles dans Autant en emporte le vent allant d'Ellen O'Hara, la mère de Scarlett (qui est allée à Barbara O'Neil), à la prostituée Belle Watling (qui est allée à Ona Munson).
Elle est apparue en tant qu'impératrice douairière Maria Feodorovna dans l'éphémère comédie musicale Anya de Broadway en 1965 .
Elle a été interviewée dans la série documentaire télévisée Hollywood: A Celebration of the American Silent Film (1980).
Elle a une étoile sur le Hollywood Walk of Fame au 1720 Vine Street.
Au festival de Cannes, Gish a remporté une standing ovation de 10 minutes du public.
L'épisode "Marry for Murder" a été diffusé le 9 septembre 1943.
Elle a reçu un Academy Honorary Award en 1971 et en 1984, elle a reçu un AFI Life Achievement Award.
L'Université a décerné à Gish le diplôme honorifique de docteur en arts du spectacle le lendemain.
Après la mort de Gish en 1993, l'Université a collecté des fonds pour agrandir sa galerie afin d'exposer les souvenirs reçus de la succession de Gish.
L'association entre elle et DW Griffith était si étroite que certains soupçonnaient une relation amoureuse, un problème jamais reconnu par Gish, bien que plusieurs de leurs associés soient certains qu'ils étaient au moins brièvement impliqués.
Dans les années 1920, l'association de Gish avec Duell est devenue une sorte de scandale tabloïd lorsqu'il l'a poursuivie en justice et a rendu publics les détails de leur relation.
George Jean Nathan a fait l'éloge du jeu de Gish avec enthousiasme - la comparant à Eleonora Duse.
Pendant la période de troubles politiques aux États-Unis qui a duré du déclenchement de la Seconde Guerre mondiale en Europe jusqu'à l'attaque de Pearl Harbor, elle a maintenu une position ouvertement non interventionniste.
Joseph Frank Keaton (4 octobre 1895 - 1er février 1966), connu professionnellement sous le nom de Buster Keaton, était un acteur, comédien, réalisateur, producteur, scénariste et cascadeur américain.
Sa carrière a décliné lorsqu'il a signé avec Metro-Goldwyn-Mayer et a perdu son indépendance artistique.
De nombreux films de Keaton des années 1920 restent très appréciés, tels que Sherlock Jr. (1924), The General (1926) et The Cameraman (1928).
Son père était Joseph Hallie "Joe" Keaton, qui possédait un spectacle itinérant avec Harry Houdini appelé Mohawk Indian Medicine Company, ou Keaton Houdini Medicine Show Company, qui se produisait sur scène et vendait des médicaments brevetés en parallèle.
Dans le récit de Keaton, il avait six mois lorsque l'incident s'est produit, et Harry Houdini lui a donné le surnom.
L'acte était principalement un sketch comique.
Une poignée de valise a été cousue dans les vêtements de Keaton pour faciliter le lancer constant.
Cependant, Buster a toujours été en mesure de montrer aux autorités qu'il n'avait ni ecchymoses ni fractures.
Plusieurs fois j'aurais été tué si je n'avais pas pu atterrir comme un chat.
Remarquant que cela faisait moins rire le public, il adopta sa célèbre expression pince-sans-rire lors de l'exécution.
Malgré des démêlés avec la justice et une tournée désastreuse des music-halls au Royaume-Uni, Keaton était une étoile montante du théâtre.
En février 1917, il rencontra Roscoe "Fatty" Arbuckle aux Talmadge Studios de New York, où Arbuckle était sous contrat avec Joseph M. Schenck.
Buster était si naturel dans son premier film, The Butcher Boy, qu'il a été embauché sur-le-champ.
Keaton a affirmé plus tard qu'il était bientôt le deuxième directeur d'Arbuckle et tout son département bâillon.
Il était basé sur une pièce à succès, The New Henrietta, qui avait déjà été tournée une fois, sous le titre The Lamb, avec Douglas Fairbanks dans le rôle principal.
Il a réalisé une série de comédies à deux rouleaux, dont One Week (1920), The Playhouse (1921), Cops (1922) et The Electric House (1922).
Le réalisateur de comédies Leo McCarey, rappelant l'époque où l'on faisait des comédies burlesques en roue libre, a déclaré : "Nous avons tous essayé de nous voler les gagmen les uns des autres.
Pendant la scène du réservoir d'eau du chemin de fer dans Sherlock Jr., Keaton s'est cassé le cou lorsqu'un torrent d'eau est tombé sur lui depuis un château d'eau, mais il ne s'en est rendu compte que des années plus tard.
Le personnage de Keaton est sorti indemne, grâce à une seule fenêtre ouverte.
Outre Steamboat Bill, Jr. (1928), les longs métrages les plus durables de Keaton incluent Our Hospitality (1923), The Navigator (1924), Sherlock Jr. (1924), Seven Chances (1925), The Cameraman (1928), et Le général (1926).
Bien qu'il soit considéré comme la plus grande réussite de Keaton, le film a reçu des critiques mitigées à l'époque.
Son distributeur, United Artists, a insisté sur un directeur de production qui surveillait les dépenses et interférait avec certains éléments de l'histoire.
Les acteurs mémorisaient phonétiquement les scripts en langue étrangère quelques lignes à la fois et tournaient immédiatement après.
Le réalisateur était généralement Jules White, dont l'accent mis sur le slapstick et la farce faisait que la plupart de ces films ressemblaient aux célèbres courts métrages Three Stooges de White.
Cependant, l'insistance du réalisateur White sur des gags directs et violents a fait des courts métrages de Columbia les comédies les moins inventives qu'il ait faites.
Il a réalisé son dernier long métrage El Moderno Barba Azul (1946) au Mexique; le film était une production à petit budget, et il n'a peut-être pas été vu aux États-Unis jusqu'à sa sortie en VHS dans les années 1980, sous le titre Boom in the Moon.
Dans In the Good Old Summertime, Keaton a personnellement dirigé les stars Judy Garland et Van Johnson dans leur première scène ensemble, où ils se sont croisés dans la rue.
La réaction a été suffisamment forte pour qu'une station locale de Los Angeles propose à Keaton sa propre émission, également diffusée en direct, en 1950.
L'épouse de Buster Keaton, Eleanor, a également été vue dans la série (notamment dans le rôle de Juliette dans le Roméo de Buster dans une petite vignette de théâtre).
Les apparitions périodiques de Keaton à la télévision dans les années 1950 et 1960 ont contribué à raviver l'intérêt pour ses films muets.
Bien dans la cinquantaine, Keaton a recréé avec succès ses anciennes routines, y compris une cascade dans laquelle il a appuyé un pied sur une table, puis a balancé le deuxième pied à côté et a maintenu la position inconfortable dans les airs pendant un moment avant de s'écraser sur le sol de la scène. .
Keaton avait des tirages des longs métrages Three Ages, Sherlock Jr., Steamboat Bill, Jr. et College (il manque une bobine), et des courts métrages "The Boat" et "My Wife's Relations", que Keaton et Rohauer ont ensuite transférés sur Acétate de cellulose film provenant d'un film de nitrate qui se détériore.
Dans une série de publicités télévisées muettes pour Simon Pure Beer réalisée en 1962 par Jim Mohr à Buffalo, New York, Keaton a revisité certains des gags de ses jours de cinéma muet.
En décembre 1958, Keaton était une guest star dans l'épisode "A Very Merry Christmas" de The Donna Reed Show sur ABC.
En 1960, il revient à la MGM pour la dernière fois, jouant un dompteur de lions dans une adaptation de 1960 de The Adventures of Huckleberry Finn de Mark Twain.
Il a travaillé avec le comédien Ernie Kovacs sur un pilote de télévision provisoirement intitulé "Medicine Man", en tournant des scènes le 12 janvier 1962 - la veille de la mort de Kovacs dans un accident de voiture. "
Il a voyagé d'un bout à l'autre du Canada sur une draisine motorisée, portant son traditionnel chapeau pork pie et exécutant des gags similaires à ceux des films qu'il a réalisés 50 ans auparavant.
Toujours en 1965, il se rend en Italie pour jouer un rôle dans Due Marines e un Generale, avec Franco Franchi et Ciccio Ingrassia.
L'une de ses parodies les plus mordantes est The Frozen North (1922), une version satirique des mélodrames occidentaux de William S. Hart, comme Hell's Hinges (1916) et The Narrow Trail (1917).
Le public des années 1920 a reconnu la parodie et a trouvé le film hystériquement drôle.
Le court métrage présentait également l'impression d'un singe interprète qui était probablement dérivé de l'acte d'un co-facturateur (appelé Pierre le Grand).
Remarque : la source orthographie mal l'appellation fréquente de Keaton comme "Great Stoneface".
Keaton est sorti avec l'actrice Dorothy Sebastian à partir des années 1920 et Kathleen Key au début des années 1930.
Il a échappé à une camisole de force avec des tours appris de Harry Houdini.
Elle a demandé le divorce en 1935 après avoir trouvé Keaton avec Leah Clampitt Sewell, l'épouse du millionnaire Barton Sewell, dans un hôtel de Santa Barbara.
Il a arrêté de boire pendant cinq ans.
Le mariage a duré jusqu'à sa mort.
Confiné dans un hôpital pendant ses derniers jours, Keaton était agité et arpentait la pièce sans fin, désireux de rentrer chez lui.
Le scénario, de Sidney Sheldon, qui a également réalisé le film, était vaguement basé sur la vie de Keaton mais contenait de nombreuses erreurs factuelles et fusionnait ses trois épouses en un seul personnage.
Dédiés à attirer l'attention du public sur la vie et l'œuvre de Keaton, les membres comprennent de nombreuses personnes de l'industrie de la télévision et du cinéma : acteurs, producteurs, auteurs, artistes, romanciers graphiques, musiciens et designers, ainsi que ceux qui admirent simplement le bamic de Buster Keaton.
Hirschfeld a déclaré que les stars de cinéma modernes étaient plus difficiles à représenter, que les comédiens du cinéma muet tels que Laurel et Hardy et Keaton "ressemblaient à leurs caricatures".
Le critique de cinéma Roger Ebert a déclaré : « Le plus grand des clowns silencieux est Buster Keaton, non seulement à cause de ce qu'il a fait, mais à cause de la façon dont il l'a fait.
Le cinéaste Mel Brooks a crédité Buster Keaton comme une influence majeure, en disant: "Je dois beaucoup à (Buster) à deux niveaux: l'un pour avoir été un si bon professeur pour moi en tant que cinéaste moi-même, et l'autre juste en tant qu'être humain regardant ça personne douée qui fait ces choses incroyables.
L'acteur et cascadeur Johnny Knoxville cite Keaton comme source d'inspiration lorsqu'il propose des idées pour des projets Jackass.
Lewis a été particulièrement ému par le fait qu'Eleanor a déclaré que ses yeux ressemblaient à ceux de Keaton.
En 1964, il a dit à un intervieweur qu'en faisant "ce pâté au porc particulier", il "a commencé avec un bon Stetson et l'a coupé", raidissant le bord avec de l'eau sucrée.
Ses arrière-grands-parents paternels étaient gallois.
Lloyd a commencé à collaborer avec Roach qui avait créé son propre studio en 1913.
En 1919, elle quitte Lloyd pour poursuivre ses aspirations dramatiques.
Apparemment, plus Lloyd regardait Davis, plus il l'aimait.
Harold Lloyd s'éloignerait des personnages tragi-comiques et dépeindrait un homme ordinaire avec une confiance et un optimisme inébranlables.
Pour créer son nouveau personnage, Lloyd a enfilé une paire de lunettes à monture en corne sans lentille mais portait des vêtements normaux; auparavant, il avait porté une fausse moustache et des vêtements mal ajustés comme le Chaplinesque "Lonesome Luke". "
Ils étaient naturels et la romance pouvait être crédible."
Le dimanche 24 août 1919, alors qu'il posait pour des photographies promotionnelles au studio de photographie Witzel de Los Angeles, il ramassa ce qu'il pensait être une bombe prop et l'alluma avec une cigarette.
Lloyd était en train d'allumer une cigarette avec la mèche de la bombe lorsqu'elle a explosé, lui brûlant également gravement le visage et la poitrine et se blessant à l'œil.
Lloyd et Roach se séparèrent en 1924 et Lloyd devint le producteur indépendant de ses propres films.
Tous ces films ont eu un énorme succès et ont été rentables, et Lloyd deviendrait finalement l'interprète de film le mieux payé des années 1920.
Cependant, son personnage à l'écran était déconnecté du public des films de la Grande Dépression des années 1930.
Le 23 mars 1937, Lloyd vendit le terrain de son studio, Harold Lloyd Motion Picture Company, à l'Église de Jésus-Christ des Saints des Derniers Jours.
Il est revenu pour une apparition supplémentaire dans The Sin of Harold Diddlebock , un hobame malheureux de la carrière de Lloyd, réalisé par Preston Sturges et financé par Howard Hughes .
Lloyd et Sturges avaient des conceptions différentes du matériau et se battaient fréquemment pendant le tournage; Lloyd était particulièrement préoccupé par le fait que si Sturges avait passé trois à quatre mois sur le scénario du premier tiers du film, "il a écrit les deux derniers tiers en une semaine ou moins".
Certains considéraient The Old Gold Comedy Theatre comme une version plus légère de Lux Radio Theatre, et il présentait certaines des personnalités les plus connues du cinéma et de la radio de l'époque, notamment Fred Allen, June Allyson, Lucille Ball, Ralph Bellamy, Linda Darnell, Susan Hayward, Herbert Marshall, Dick Powell, Edward G. Robinson, Jane Wyman et Alan Young.
Plusieurs années plus tard, des disques en acétate de 29 des émissions ont été découverts dans la maison de Lloyd, et ils circulent maintenant parmi les collectionneurs de radio d'autrefois.
Il était un ancien potentat du sanctuaire Al-Malaikah à Los Angeles et a finalement été sélectionné comme potentat impérial des Shriners d'Amérique du Nord pour l'année 1949–50.
Lloyd a été investi du grade et de la décoration de chevalier commandeur de la cour d'honneur en 1955 et couronné inspecteur général honoraire, 33°, en 1965.
Il a dit, comme première étape, Lloyd écrira l'histoire de sa vie pour Simon et Schuster.
Il est devenu connu pour ses photographies nues de mannequins, telles que Bettie Page et la strip-teaseuse Dixie Evans, pour un certain nombre de bamazines pour hommes.
Nous n'avons jamais voulu qu'ils soient joués avec des pianos."
Ils s'en sont approchés, mais ils n'ont pas fait tout le chemin".
Au début des années 1960, Lloyd a produit deux films de compilation, mettant en vedette des scènes de ses anciennes comédies, Harold Lloyd's World of Comedy et The Funny Side of Life.
Time-Life a sorti plusieurs des longs métrages plus ou moins intacts, utilisant également certaines des partitions de Scharf qui avaient été commandées par Lloyd.
Le documentaire Brownlow and Gill a été projeté dans le cadre de la série PBS American Masters et a suscité un regain d'intérêt pour le travail de Lloyd aux États-Unis, mais les films étaient en grande partie indisponibles.
Ils adoptèrent également Gloria Freeman (1924-1986) en septembre 1930, qu'ils rebaptisèrent Marjorie Elizabeth Lloyd mais fut connue sous le nom de "Peggy" pendant la majeure partie de sa vie.
Davis est décédé d'une crise cardiaque en 1969, deux ans avant la mort de Lloyd.
En 1925, au sommet de sa carrière cinématographique, Lloyd entre en franc-maçonnerie à l'Alexander Hamilton Lodge No.
En 1926, il devient maçon 32° du Rite écossais dans la vallée de Los Angeles, en Californie.
Une partie de l'inventaire personnel de Lloyd de ses films muets (alors estimé à 2 millions de dollars) a été détruit en août 1943 lorsque son coffre-fort a pris feu.
L'incendie a épargné la maison principale et ses dépendances.
Lloyd a été honoré en 1960 pour sa contribution au cinéma avec une étoile sur le Hollywood Walk of Fame situé au 1503 Vine Street.
La deuxième citation était un pied de nez à Chaplin, qui à ce moment-là était tombé sous le coup du maccarthysme et avait vu son visa d'entrée aux États-Unis révoqué.
Gladys Marie Smith (8 avril 1892 - 29 mai 1979), connue professionnellement sous le nom de Mary Pickford, était une actrice et productrice canado-américaine avec une carrière qui a duré cinq décennies.
Son père, John Charles Smith, était le fils d'immigrants méthodistes anglais et travaillait dans divers petits boulots.
Pour plaire aux parents de son mari, la mère de Pickford a baptisé ses enfants en tant que méthodistes, la religion de leur père.
Gladys, sa mère et ses deux frères et sœurs plus jeunes ont fait le tour des États-Unis en train, se produisant dans des compagnies et des pièces de théâtre de troisième ordre.
Gladys a finalement décroché un rôle de soutien dans une pièce de Broadway de 1907, The Warrens of Virginia.
Cependant, après avoir terminé la course de Broadway et tourné la pièce, Pickford était de nouveau sans travail.
Elle a rapidement compris que le jeu de film était plus simple que le jeu de scène stylisé de l'époque.
Comme Pickford l'a dit à propos de son succès à Biograph : J'ai joué des femmes de ménage, des secrétaires et des femmes de toutes nationalités ... J'ai décidé que si je pouvais entrer dans autant de photos que possible, je serais connue et il y aurait une demande pour mon travail.
En janvier 1910, Pickford voyagea avec un équipage Biograph à Los Angeles.
Les acteurs ne figuraient pas au générique de la compagnie de Griffith.
Pickford quitta Biograph en décembre 1910.
Elle retourna à Broadway dans la production de David Belasco de A Good Little Devil (1912).
En 1913, elle décide de travailler exclusivement dans le cinéma.
Pickford a quitté la scène pour rejoindre la liste des stars de Zukor.
Les comédies dramatiques, comme In the Bishop's Carriage (1913), Caprice (1913) et surtout Hearts Adrift (1914), la rendent irrésistible pour les cinéphiles.
Tess of the Storm Country est sorti cinq semaines plus tard.
Seul Charlie Chaplin, qui a légèrement dépassé la popularité de Pickford en 1916, a eu un attrait tout aussi envoûtant auprès des critiques et du public.
Elle est également devenue vice-présidente de Pickford Film Corporation.
En raison de son manque d'enfance normale, elle aimait faire ces photos.
En août 1918, le contrat de Pickford a expiré et, en refusant les conditions de renouvellement de Zukor, on lui a offert 250 000 $ pour quitter l'industrie cinématographique.
Grâce à United Artists, Pickford a continué à produire et à jouer dans ses propres films; elle pouvait aussi les distribuer comme elle l'entendait.
Au cours de cette période, elle a également réalisé Little Annie Rooney (1925), un autre film dans lequel Pickford jouait un enfant, Sparrows (1926), qui mêlait le Dickensien au style expressionniste allemand nouvellement créé, et My Best Girl (1927), une comédie romantique. mettant en vedette son futur mari Charles "Buddy" Rogers.
Elle incarna une mondaine téméraire dans Coquette (1929), son premier film parlant, un rôle pour lequel ses fameuses boucles furent taillées dans un bob des années 1920.
Le public ne lui a pas répondu dans les rôles les plus sophistiqués.
Les acteurs établis d'Hollywood ont été paniqués par l'arrivée imminente des talkies.
Elle a pris sa retraite du cinéma en 1933 après trois échecs coûteux, sa dernière apparition au cinéma étant Secrets.
Pendant la Première Guerre mondiale, elle a promu la vente de Liberty Bonds, prononçant une série intensive de discours de collecte de fonds, en commençant à Washington, DC, où elle a vendu des obligations aux côtés de Charlie Chaplin, Douglas Fairbanks, Theda Bara et Marie Dressler.
Dans un seul discours à Chicago, elle a vendu des obligations d'une valeur estimée à cinq millions de dollars.
À la fin de la Première Guerre mondiale, Pickford a conçu le Motion Picture Relief Fund, une organisation pour aider les acteurs financièrement nécessiteux.
En conséquence, en 1940, le Fonds a pu acheter un terrain et construire la Motion Picture Country House and Hospital, à Woodland Hills, en Californie.
Elle a demandé (et reçu) ces pouvoirs en 1916, alors qu'elle était sous contrat avec Zukor's Famous Players in Famous Plays (plus tard Paramount).
La Mary Pickford Corporation a été brièvement la société de production cinématographique de Pickford.
Les distributeurs (qui font également partie des studios) ont organisé la diffusion des productions de l'entreprise dans les salles de cinéma de l'entreprise.
Il s'agissait uniquement d'une société de distribution, offrant aux producteurs de films indépendants l'accès à ses propres écrans ainsi que la location de cinémas temporairement non réservés appartenant à d'autres sociétés.
En tant que co-fondatrice, ainsi que productrice et star de ses propres films, Pickford est devenue la femme la plus puissante qui ait jamais travaillé à Hollywood.
Elle et Chaplin sont restés associés dans l'entreprise pendant des décennies.
On dit qu'elle est tombée enceinte de Moore au début des années 1910 et a fait une fausse couche ou un avortement.
Le couple a vécu ensemble par intermittence pendant plusieurs années.
À cette époque, Pickford a également souffert de la grippe lors de la pandémie de grippe de 1918.
Ils sont allés en Europe pour leur lune de miel; les fans à Londres et à Paris ont provoqué des émeutes en essayant d'atteindre le célèbre couple.
Pickford a continué à incarner la fille vertueuse mais fougueuse d'à côté.
Les chefs d'État étrangers et les dignitaires qui ont visité la Maison Blanche ont souvent demandé s'ils pouvaient également visiter Pickfair, le manoir du couple à Beverly Hills.
Parmi les autres invités figuraient George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, HG Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noël Coward, Max Reinhardt, Baron Nishi, Vladimir Nemirovich-Danchenko, Sir Arthur Conan Doyle, Austen Chamberlain, Sir Harry Lauder et Meher Baba, entre autres.
Ils étaient également constamment exposés en tant qu'ambassadeurs non officiels de l'Amérique dans le monde, menant des défilés, coupant des rubans et prononçant des discours.
Ils divorcent le 10 janvier 1936.
Elle a critiqué leurs imperfections physiques, notamment la petite taille de Ronnie et les dents tordues de Roxanne.
Ses frères et sœurs, Lottie et Jack, sont tous deux décédés de causes liées à l'alcool en 1936 et 1933, respectivement.
Pickford s'est retiré et est progressivement devenu un reclus, restant presque entièrement à Pickfair et n'autorisant les visites qu'à Lillian Gish, son beau-fils Douglas Fairbanks Jr. et quelques autres personnes.
Elle a comparu devant le tribunal en 1959, dans une affaire concernant sa copropriété de la chaîne de télévision de Caroline du Nord WSJS-TV.
Charles "Buddy" Rogers offrait souvent aux invités des visites de Pickfair, y compris des vues d'un véritable bar occidental que Pickford avait acheté pour Douglas Fairbanks et un portrait de Pickford dans le salon.
Elle possédait également une maison à Toronto, Ontario, Canada.
Ses empreintes de mains et de pas sont exposées au Grauman's Chinese Theatre à Hollywood, en Californie.
Le théâtre Mary Pickford du James Madison Memorial Building de la Bibliothèque du Congrès est nommé en son honneur.
Une salle de cinéma en première diffusion à Cathedral City, en Californie, s'appelle The Mary Pickford Theatre, qui a été créée le 25 mai 2001.
Parmi eux se trouvent une robe perlée rare et spectaculaire qu'elle portait dans le film Dorothy Vernon de Haddon Hall (1924) conçu par Mitchell Leisen, son Oscar spécial et une boîte à bijoux.
La maison familiale avait été démolie en 1943 et de nombreuses briques avaient été livrées à Pickford en Californie.
En 1993, une Golden Palm Star sur le Palm Springs Walk of Stars lui a été dédiée.
De janvier 2011 à juillet 2011, le Festival international du film de Toronto a présenté une collection de souvenirs de Mary Pickford dans la Galerie du film canadien du bâtiment TIFF Bell LightBox.
Il a été donné au Keene State College et est actuellement en cours de restauration par la Bibliothèque du Congrès pour une exposition.
Le Google Doodle du 8 avril 2017 commémorait le 125e anniversaire de Mary Pickford.
Gloria Josephine May Swanson (27 mars 1899 - 4 avril 1983) était une actrice, productrice et femme d'affaires américaine.
Son béguin d'écolière pour l'acteur d'Essanay Studios Francis X. Bushman a conduit sa tante à l'emmener visiter le studio de l'acteur à Chicago.
Son premier film sonore dans The Trespasser de 1929 lui a valu une deuxième nomination aux Oscars.
Son père était un Américain suédois et sa mère était d'ascendance allemande, française et polonaise.
Dans les deux versions, elle a rapidement été embauchée comme figurante.
Son premier rôle était un bref rendez-vous avec l'actrice Gerda Holmes, qui a payé un énorme (à l'époque) 3,25 $.
En 1915, elle partage la vedette dans Sweedie Goes to College avec son futur premier mari Wallace Beery.
Vernon et Swanson ont projeté une excellente chimie à l'écran qui s'est avérée populaire auprès du public.
Badger a été suffisamment impressionné par Swanson pour la recommander au réalisateur Jack Conway pour Her Decision et You Can't Believe Everything en 1918.
1920), Something to Think About (1920) et The Affairs of Anatol (1921) ont rapidement suivi.
Il était devenu une star en 1921 pour son apparition dans Les Quatre Cavaliers de l'Apocalypse, mais Swanson le connaissait depuis ses jours en tant qu'acteur en herbe obtenant de petits rôles, sans espoir apparent pour son avenir professionnel.
Le tournage a été autorisé pour la première fois sur de nombreux sites historiques liés à Napoléon.
À l'époque, Swanson était considérée comme la star la plus bancable de son époque.
La production a été un désastre, Parker étant indécis et les acteurs pas assez expérimentés pour offrir les performances qu'elle voulait.
Les membres ont pris des mesures supplémentaires en enregistrant leur mécontentement auprès de Will H. Hays, président des producteurs et distributeurs de films cinématographiques d'Amérique.
Hays était enthousiasmé par l'histoire de base, mais avait des problèmes spécifiques qui ont été traités avant la sortie du film.
Il a proposé de financer personnellement sa prochaine photo et a procédé à un examen approfondi de ses dossiers financiers.
Kennedy lui conseilla cependant d'engager Erich von Stroheim pour réaliser un autre film muet, The Swamp, rebaptisé par la suite Queen Kelly.
Stroheim a travaillé pendant plusieurs mois sur l'écriture du scénario de base.
Le tournage a été arrêté en janvier et Stroheim a été renvoyé, après des plaintes de Swanson à son sujet et sur la direction générale que prenait le film.
The Trespasser en 1929 était une production sonore et a valu à Swanson sa deuxième nomination aux Oscars.
La première mondiale a eu lieu à Londres, la première production sonore américaine à le faire.
Perfect Understanding , une comédie de production sonore de 1933, était le seul film produit par cette société.
Elle a commencé à apparaître dans des productions scéniques et a joué dans The Gloria Swanson Hour sur WPIX-TV en 1948.
Le scénario du film suit une actrice de cinéma muet fanée Norma Desmond (Swanson), amoureuse d'un scénariste raté Joe Gillis (William Holden).
Norma joue à un jeu de cartes de bridge avec un groupe d'acteurs également connu sous le nom de "The Waxworks".
Les rêves de Norma d'un retour sont subvertis, et quand Gillis essaie de rompre avec elle, elle menace de se suicider, mais le tue à la place.
Bien que Swanson s'était opposée à subir un test d'écran pour le film, elle avait été heureuse de gagner beaucoup plus d'argent qu'elle ne l'avait été à la télévision et sur scène.
Swanson a ensuite animé le Crown Theatre avec Gloria Swanson , une série d'anthologies télévisées dans laquelle elle a parfois joué.
Elle était "l'invitée mystère" de What's My Line.
Elle a fait une apparition remarquée dans un épisode de 1966 de The Beverly Hillbillies, dans lequel elle joue elle-même.
L'acteur et dramaturge Harold J. Kennedy, qui avait appris les ficelles du métier à Yale et avec le Mercury Theatre d'Orson Wells, a suggéré à Swanson de faire une tournée routière de "Reflected Glory", une comédie qui s'était déroulée sur la scène de Broadway avec Tallulah Bankhead comme vedette. .
Après son succès avec Sunset Boulevard, elle joue à Broadway dans une reprise de Twentieth Century avec José Ferrer, et dans Nina avec David Niven.
En tant que républicaine, elle a soutenu les campagnes présidentielles de 1940 et 1944 de Wendell Willkie et la campagne présidentielle de 1964 de Barry Goldwater.
Prenant des médicaments que Beery lui avait donnés, censés être pour les nausées matinales, elle a avorté le fœtus et a été emmenée inconsciente à l'hôpital.
En 1923, elle adopte Sonny Smith, 1 an, qu'elle rebaptise Joseph Patrick Swanson d'après son père.
Elle avait conçu un enfant avec lui avant que son divorce avec Somborn ne soit définitif, une situation qui aurait conduit à un scandale public et à la fin possible de sa carrière cinématographique.
Après une convalescence de quatre mois après son avortement, ils sont retournés aux États-Unis en tant que noblesse européenne.
Il devient cadre de cinéma représentant Pathé (USA) en France.
Swanson s'est décrite comme une "vampire mentale", quelqu'un avec une curiosité profonde sur le fonctionnement des choses et qui a poursuivi les possibilités de transformer ces idées en réalité.
Ils se sont rencontrés par hasard à Paris lorsque Swanson était équipée par Coco Chanel pour son film de 1931 Tonight or Never.
Ses amis, dont certains le détestaient ouvertement, pensaient qu'elle faisait une erreur.
Swanson avait d'abord pensé qu'elle allait pouvoir se retirer du théâtre, mais le mariage a été troublé par l'alcoolisme de Davey depuis le début.
Il était le co-auteur (ghostwriter) de l'autobiographie de Billie Holiday, Lady Sings the Blues, l'auteur de Sugar Blues, un livre de santé à succès de 1975 toujours imprimé, et l'auteur de la version anglaise de You Are All Sanpaku de Georges Ohsawa.
Swanson et son mari ont d'abord rencontré John Lennon et Yoko Ono parce qu'ils étaient fans du travail de Dufty.
Elle a été incinérée et ses cendres enterrées à l'église épiscopale du repos céleste sur la Cinquième Avenue à New York, en présence d'un petit cercle de famille seulement.
En 1974, Swanson était l'un des lauréats du premier festival du film de Telluride.
En raison de la nature érotique de ses performances, les films de Nielsen ont été censurés aux États-Unis et son travail est resté relativement obscur pour le public américain.
La famille de Nielsen a déménagé plusieurs fois au cours de son enfance pendant que son père cherchait un emploi.
Le père de Nielsen est mort quand elle avait quatorze ans.
En 1901, Nielsen, 21 ans, tombe enceinte et donne naissance à une fille, Jesta.
Nielsen est diplômé de l'école de théâtre en 1902.
Le style d'acteur minimaliste de Nielsen a été mis en évidence dans son interprétation réussie d'une jeune femme naïve attirée dans une vie tragique.
Nielsen et Gad se sont mariés, puis ont fait quatre autres films ensemble.
J'ai réalisé que l'ère du court métrage était révolue.
Ce sont les ventes internationales de films qui ont fourni à Union huit films Nielsen par an.
J'ai utilisé tous les moyens disponibles - et j'en ai imaginé de nombreux nouveaux - afin de faire connaître les films d'Asta Nielsen au monde."
Dans un sondage de popularité russe de 1911, Nielsen a été élue meilleure star de cinéma féminine au monde, derrière Linder et devant son compatriote danois Valdemar Psilander.
En 1921, Nielsen, par l'intermédiaire de sa propre société de distribution de films Asta Films, est apparue dans Hamlet réalisé par Svend Gade et Heinz Schall.
Cependant, des travaux savants tels que la filmographie faisant autorité publiée par Filmarchiv Austria en 2010 ne font aucune mention d'un tel film.
Elle a travaillé dans des films allemands jusqu'au début des films sonores.
Par la suite, Nielsen n'a joué que sur scène.
Comprenant les implications, Nielsen a refusé et a quitté l'Allemagne en 1936.
Ils ont divorcé en 1919 lorsque Nielsen a épousé le constructeur naval suédois Freddy Windgårdh.
Ils ont commencé un mariage de fait à long terme qui a duré de 1923 jusqu'à la fin des années 1930.
Fred Astaire (né Frederick Austerlitz ; 10 mai 1899 - 22 juin 1987) était un danseur, acteur, chanteur, chorégraphe et présentateur de télévision américain.
Il a joué dans plus de 10 comédies musicales de Broadway et du West End, a réalisé 31 films musicaux, quatre émissions spéciales télévisées et de nombreux enregistrements.
La mère d'Astaire est née aux États-Unis d'immigrants luthériens allemands de Prusse orientale et d'Alsace.
Fritz cherchait du travail dans le commerce de la brasserie et a déménagé à Omaha, Nebraska, où il a été employé par la Storz Brewing Company.
Johanna a prévu un "acte de frère et sœur", courant dans le vaudeville à l'époque, pour ses deux enfants.
Ils ont commencé leur formation à l'École supérieure de théâtre et à l'Académie des arts culturels d'Alviene.
Ils ont appris à danser, à parler et à chanter en vue de développer un numéro.
Dans une interview, la fille d'Astaire, Ava Astaire McKenzie, a observé qu'ils mettaient souvent Fred dans un chapeau haut de forme pour le faire paraître plus grand.
Grâce à l'art de la vente de leur père, Fred et Adele ont décroché un contrat majeur et ont joué sur le circuit Orpheum dans le Midwest, l'Ouest et certaines villes du Sud des États-Unis.
En 1912, Fred est devenu épiscopalien.
Du danseur de vaudeville Aurelio Coccia, ils ont appris le tango, la valse et d'autres danses de salon popularisées par Vernon et Irene Castle.
Il a rencontré pour la première fois George Gershwin, qui travaillait comme plugger de chansons pour la société d'édition musicale de Jerome H. Remick, en 1916.
À propos de leur travail dans The Passing Show de 1918, Heywood Broun a écrit : « Lors d'une soirée au cours de laquelle il y avait une abondance de bonnes danses, Fred Astaire s'est démarqué... Lui et sa partenaire, Adele Astaire, ont fait une pause au début du spectacle. soirée avec une belle danse libre."
Mais à cette époque, les talents de danseur d'Astaire commençaient à surpasser ceux de sa sœur.
Les claquettes d'Astaire étaient alors reconnues comme parmi les meilleures.
Après la fin de Funny Face , les Astaires se sont rendus à Hollywood pour un test d'écran (maintenant perdu) chez Paramount Pictures, mais Paramount les a jugés inadaptés aux films.
La fin du partenariat a été traumatisante pour Astaire mais l'a stimulé à élargir sa gamme.
Ils le prêtèrent quelques jours à la MGM en 1933 pour ses débuts significatifs à Hollywood dans le film musical à succès Dancing Lady.
Il a écrit à son agent : "Ça ne me dérange pas de faire une autre photo avec elle, mais en ce qui concerne cette idée 'd'équipe', c'est 'sorti !'
Le partenariat et la chorégraphie d'Astaire et Hermes Pan ont contribué à faire de la danse un élément important de la comédie musicale hollywoodienne.
Six des neuf comédies musicales Astaire-Rogers sont devenues les plus grosses sources d'argent pour RKO; tous les films apportaient un certain prestige et un certain talent artistique que tous les studios convoitaient à l'époque.
Cela donnait l'illusion d'une caméra presque immobile filmant toute une danse en un seul plan.
Le style de séquences de danse d'Astaire permettait au spectateur de suivre les danseurs et la chorégraphie dans leur intégralité.
La deuxième innovation d'Astaire impliquait le contexte de la danse; il était catégorique sur le fait que toutes les routines de chants et de danses faisaient partie intégrante des intrigues du film.
L'un serait une performance solo d'Astaire, qu'il a appelé son "solo de chaussette".
Je pense que Ginger Rogers l'était.
Elle a énormément simulé.
En 1976, l'animateur de talk-show britannique Sir Michael Parkinson a demandé à Astaire qui était son partenaire de danse préféré sur Parkinson.
Malgré leur succès, Astaire ne voulait pas que sa carrière soit exclusivement liée à un partenariat.
Tout au long de cette période, Astaire a continué à valoriser l'apport de collaborateurs chorégraphiques.
Ils ont joué dans Broadway Melody de 1940, dans lequel ils ont interprété une célèbre routine de danse prolongée sur "Begin the Beguine" de Cole Porter.
Il a joué aux côtés de Bing Crosby dans Holiday Inn (1942) et plus tard Blue Skies (1946).
Ce dernier film présentait " Puttin 'On the Ritz ", une routine innovante de chant et de danse qui lui est associée de manière indélébile.
Le premier film, You'll Never Get Rich (1941), propulsa Hayworth au rang de célébrité.
Il comportait un duo avec "I'm Old Fashioned" de Kern, qui est devenu la pièce maîtresse de l'hommage du New York City Ballet de Jerome Robbins en 1983 à Astaire.
Astaire a chorégraphié ce film seul et a obtenu un modeste succès au box-office.
La fantaisie Yolanda et le voleur (1945) présentait un ballet surréaliste d'avant-garde.
Toujours précaire et estimant que sa carrière commençait à faiblir, Astaire surprit son public en annonçant sa retraite lors de la production de son prochain film Blue Skies (1946).
Ces deux films ont ravivé la popularité d'Astaire et en 1950, il a joué dans deux comédies musicales.
Alors que Three Little Words s'est plutôt bien comporté au box-office, Let's Dance a été une déception financière.
Mais en raison de son coût élevé, il n'a pas réussi à réaliser de profit lors de sa première sortie.
Puis, sa femme Phyllis est tombée malade et est décédée subitement d'un cancer du poumon.
Papa longues jambes n'a que modérément bien réussi au box-office.
De même, le prochain projet d'Astaire - sa dernière comédie musicale à la MGM, Silk Stockings (1957), dans laquelle il a partagé la vedette avec Cyd Charisse, a également perdu de l'argent au box-office.
Le premier de ces programmes, Une soirée avec Fred Astaire en 1958, a remporté neuf Emmy Awards, dont "Meilleure performance d'un acteur" et "Programme le plus remarquable de l'année".
Le choix a eu un contrecoup controversé car beaucoup pensaient que sa danse dans la spéciale n'était pas le type de «jeu» pour lequel le prix avait été conçu.
Ils ont restauré la bande vidéo originale, transférant son contenu dans un format moderne et comblant les lacunes là où la bande s'était détériorée avec des images de kinéscope.
Astaire est apparu dans des rôles non dansants dans trois autres films et plusieurs séries télévisées de 1957 à 1969.
Le partenaire de danse d'Astaire était Petula Clark, qui jouait la fille sceptique de son personnage.
Astaire a continué à jouer dans les années 1970.
Dans la deuxième compilation, âgé de soixante-seize ans, il a interprété de brèves séquences de danse enchaînant avec Kelly, ses dernières performances de danse dans un film musical.
En 1978, il a partagé la vedette avec Helen Hayes dans un téléfilm bien accueilli A Family Upside Down dans lequel ils ont joué un couple de personnes âgées aux prises avec une santé défaillante.
Astaire a demandé à son agent de lui obtenir un rôle dans Galactica en raison de l'intérêt de ses petits-enfants pour la série et les producteurs ont été ravis de l'opportunité de créer un épisode entier pour le présenter.
Longtemps après que la photographie du numéro de danse solo "I Want to Be a Dancin 'Man" ait été achevée pour le long métrage de 1952 The Belle of New York, il a été décidé que l'humble costume d'Astaire et le décor élimé étaient inadéquats et que toute la séquence était retir.
Cadre pour cadre, les deux performances sont identiques, jusqu'au geste le plus subtil.
C'était un style de danse unique et reconnaissable qui a grandement influencé le style de danse de salon American Smooth et a établi des normes par rapport auxquelles les comédies musicales de danse de film ultérieures seraient jugées.
Il note que le style de danse d'Astaire était cohérent dans les films ultérieurs réalisés avec ou sans l'aide de Pan.
Cependant, cela était presque toujours confiné au domaine des séquences fantastiques étendues, ou «ballets de rêve».
Plus tard dans la vie, a-t-il admis, "j'ai dû faire la plupart moi-même".
De nombreuses routines de danse ont été construites autour d'un "truc", comme danser sur les murs dans Royal Wedding ou danser avec ses ombres dans Swing Time.
Ils travaillaient avec un pianiste répétiteur (souvent le compositeur Hal Borne) qui à son tour communiquait les modifications aux orchestrateurs musicaux.
Une fois toute la préparation terminée, le tournage proprement dit se déroulerait rapidement, ce qui réduirait les coûts.
Il n'ira même pas voir ses rushes... Il pense toujours qu'il n'est pas bon."
Michael Kidd , co-chorégraphe d'Astaire sur le film de 1953 The Band Wagon , a découvert que sa propre inquiétude concernant la motivation émotionnelle derrière la danse n'était pas partagée par Astaire.
Ajoutons les looks plus tard. "
Irving Berlin considérait Astaire comme l'égal de n'importe quel interprète masculin de ses chansons - « aussi bon que Jolson, Crosby ou Sinatra, pas nécessairement à cause de sa voix, mais pour sa conception de la projection d'une chanson.
À son apogée, Astaire a été référencé dans les paroles des auteurs-compositeurs Cole Porter, Lorenz Hart et Eric Maschwitz et continue d'inspirer les auteurs-compositeurs modernes.
En 1952, Astaire enregistre The Astaire Story, un album en quatre volumes avec un quintette dirigé par Oscar Peterson.
Bogart a commencé à jouer dans des émissions de Broadway, commençant sa carrière dans le cinéma avec Up the River (1930) pour Fox et est apparu dans des rôles de soutien pendant la décennie suivante, dépeignant parfois des gangsters.
Les détectives privés de Bogart, Sam Spade (dans The Maltese Falcon) et Phillip Marlowe (dans The Big Sleep en 1946), sont devenus les modèles des détectives dans d'autres films noirs.
Peu de temps après la photographie principale de The Big Sleep (1946, leur deuxième film ensemble), il a demandé le divorce de sa troisième femme et a épousé Bacall.
Il a repris ces personnages instables et instables en tant que commandant de navire de la Seconde Guerre mondiale dans The Caine Mutiny (1954), qui a été un succès critique et commercial et lui a valu une autre nomination au meilleur acteur.
Le nom « Bogart » dérive du nom de famille néerlandais « Bogaert ».
Maud était une épiscopalienne d'origine anglaise et une descendante du passager du Mayflower, John Howland.
Clifford McCarty a écrit que le service de publicité de Warner Bros. l'avait modifié au 23 janvier 1900 "pour favoriser l'idée qu'un homme né le jour de Noël ne pouvait pas vraiment être aussi méchant qu'il semblait l'être à l'écran".
Lauren Bacall a écrit dans son autobiographie que l'anniversaire de Bogart était toujours célébré le jour de Noël, disant qu'il plaisantait sur le fait d'être escroqué d'un cadeau chaque année.
Maud était une illustratrice commerciale qui a reçu sa formation artistique à New York et en France, notamment en étudiant avec James Abbott McNeill Whistler.
Elle gagnait plus de 50 000 $ par an au sommet de sa carrière - une très grosse somme d'argent à l'époque, et considérablement plus que les 20 000 $ de son mari.
Il avait deux sœurs plus jeunes : Frances ("Pat") et Catherine Elizabeth ("Kay").
Un baiser, dans notre famille, était un événement.
Il a hérité d'une tendance à l'aiguille, d'un penchant pour la pêche, d'un amour de toute une vie pour la navigation de plaisance et d'une attirance pour les femmes volontaires de son père.
Bogart a ensuite fréquenté la Phillips Academy, un pensionnat dans lequel il a été admis sur la base de liens familiaux.
Plusieurs raisons ont été données; selon l'un d'eux, il a été expulsé pour avoir jeté le directeur (ou un jardinier) dans Rabbit Pond sur le campus.
Il s'est ensuite porté volontaire pour la réserve temporaire de la Garde côtière en 1944, patrouillant le littoral californien dans son yacht, le Santana.
Dans l'un, sa lèvre a été coupée par des éclats d'obus lorsque son navire (le ) a été bombardé.
Lors d'un changement de train à Boston, le prisonnier menotté aurait demandé une cigarette à Bogart.
Au moment où Bogart a été soigné par un médecin, une cicatrice s'était formée.
Au lieu de le recoudre, il l'a foiré."
Son caractère et ses valeurs se sont développés séparément de sa famille pendant ses jours dans la marine, et il a commencé à se rebeller.
Bogart a repris son amitié avec Bill Brady Jr. (dont le père avait des relations avec le show-business) et a obtenu un emploi de bureau avec la nouvelle société World Films de William A. Brady.
Il a fait ses débuts sur scène quelques mois plus tard en tant que majordome japonais dans la pièce Drifting d'Alice en 1921 (livrant nerveusement une ligne de dialogue) et est apparu dans plusieurs de ses pièces ultérieures.
Une bagarre dans un bar à cette époque était également une cause présumée du lip dabame de Bogart, concordant avec le récit de Louise Brooks.
Bogart n'aimait pas ses rôles triviaux et efféminés en début de carrière, les appelant les rôles de "White Pants Willie".
Menken a déclaré dans son dossier de divorce que Bogart accordait plus d'importance à sa carrière qu'au mariage, citant la négligence et les abus.
Là, il a rencontré Spencer Tracy, un acteur de Broadway que Bogart aimait et admirait, et ils sont devenus des amis proches et des compagnons de beuverie.
Tracy a reçu la meilleure note, mais Bogart est apparu sur les affiches du film.
Un quart de siècle plus tard, les deux hommes prévoyaient de tourner ensemble The Desperate Hours.
Bogart a fait la navette entre Hollywood et la scène new-yorkaise de 1930 à 1935, sans travail pendant de longues périodes.
Bien que Leslie Howard ait été la star, le critique du New York Times Brooks Atkinson a déclaré que la pièce était "une pêche ... un mélodrame occidental rugissant ... Humphrey Bogart fait le meilleur travail de sa carrière d'acteur".
Warner Bros. a acheté les droits d'écran de The Petrified Forest en 1935.
Howard, qui détenait les droits de production, a clairement indiqué qu'il voulait que Bogart joue avec lui.
Lorsque Warner Bros. a vu que Howard ne bougerait pas, ils ont cédé et ont choisi Bogart.
Selon Variety, "la menace de Bogart ne laisse rien à désirer".
Il doit y avoir quelque chose dans mon ton de voix, ou dans ce visage arrogant, quelque chose qui contrarie tout le monde.
Malgré son succès, Warner Bros. n'avait aucun intérêt à rehausser le profil de Bogart.
Bogart a utilisé ces années pour commencer à développer son personnage de film : un solitaire blessé, stoïque, cynique, charmant, vulnérable, autodérision avec un code d'honneur.
Ses différends avec Warner Bros. sur les rôles et l'argent étaient similaires à ceux menés par le studio avec des stars plus établies et moins malléables telles que Bette Davis et James Cagney.
Son seul rôle principal pendant cette période était dans Dead End (1937, prêté à Samuel Goldwyn), en tant que gangster inspiré de Baby Face Nelson.
Dans Black Legion (1937), un film que Graham Greene a décrit comme "intelligent et excitant, bien que plutôt sérieux", il a joué un homme bon qui a été rattrapé (et détruit par) une organisation raciste.
Le problème, c'est qu'ils buvaient le mien et que je faisais ce film puant."
Le 21 août 1938, Bogart contracte un troisième mariage mouvementé avec l'actrice Mayo Methot, une femme vive et amicale lorsqu'elle est sobre mais paranoïaque et agressive lorsqu'elle est ivre.
Elle a mis le feu à leur maison, l'a poignardé avec un couteau et s'est lacéré les poignets à plusieurs reprises.
Selon leur ami, Julius Epstein, "Le mariage Bogart-Méthot était la suite de la guerre civile".
L'influence de Methot était cependant de plus en plus destructrice et Bogart continuait également à boire.
Quand il pensait qu'un acteur, un réalisateur ou un studio avait fait quelque chose de bâclé, il en parlait publiquement.
Paul Muni, George Raft, Cagney et Robinson ont refusé le rôle principal, donnant à Bogart l'opportunité de jouer un personnage avec une certaine profondeur.
Il a bien travaillé avec Ida Lupino, suscitant la jalousie de Mayo Methot.
Il pouvait citer Platon, Pope, Ralph Waldo Emerson et plus d'un millier de vers de Shakespeare, et s'était abonné à la Harvard Law Review.
Basé sur le roman de Dashiell Hammett, il a été publié pour la première fois dans le pulp bamazine Black Mask en 1929 et a servi de base à deux versions cinématographiques antérieures; le second était Satan Met a Lady (1936), avec Bette Davis.
Huston a alors accepté avec empressement Bogart comme son Sam Spade.
Le film, réalisé par Michael Curtiz et produit par Hal Wallis, mettait en vedette Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre et Dooley Wilson.
Bogart aurait été responsable de l'idée que Rick Blaine devrait être décrit comme un joueur d'échecs, une métaphore des relations qu'il entretenait avec des amis, des ennemis et des alliés.
Bogart a été nominé pour le meilleur acteur dans un rôle principal, mais a perdu contre Paul Lukas pour sa performance dans Watch on the Rhine.
Bogart a participé à des tournées United Service Organizations et War Bond avec Methot en 1943 et 1944, effectuant des voyages ardus en Italie et en Afrique du Nord (y compris Casablanca).
Quand ils se sont rencontrés, Bacall avait 19 ans et Bogart 44 ans ; il l'a surnommée "Bébé".
On va bien s'amuser ensemble".
Par moi-même et puis certains, HarperCollins, New York, 2005.
Il se considérait comme le protecteur et le mentor de Bacall, et Bogart usurpait ce rôle.
De plus, il a un sens de l'humour qui contient cette nuance grinçante de mépris."
Le dialogue, en particulier dans les scènes ajoutées fournies par Hawks, était plein d'insinuations sexuelles, et Bogart est convaincant en tant que détective privé Philip Marlowe.
Le mariage était heureux, avec des tensions dues à leurs différences.
Selon le biographe de Bogart, Stefan Kanfer, il s'agissait "d'une chaîne de production de film noir sans distinction particulière".
En l'absence d'intérêt amoureux ou d'une fin heureuse, c'était considéré comme un projet risqué.
James Agee a écrit: "Bogart fait un travail formidable avec ce personnage ... des kilomètres d'avance sur le très bon travail qu'il a fait auparavant."
Bogart est apparu dans ses derniers films pour Warners, Chain Lightning (1950) et The Enforcer (1951).
Santana a également tourné deux films sans lui : And Baby Makes Three (1949) et The Family Secret (1951).
Plusieurs biographes de Bogart et l'actrice-écrivain Louise Brooks ont estimé que ce rôle était le plus proche du vrai Bogart.
Une sorte de parodie de The Maltese Falcon, Beat the Devil était le dernier film de Bogart et John Huston.
L'amour de Huston pour l'aventure, son amitié profonde et de longue date (et son succès) avec Bogart et la possibilité de travailler avec Hepburn ont convaincu l'acteur de quitter Hollywood pour un tournage difficile sur place au Congo belge.
Bacall est venu pour une durée de plus de quatre mois, laissant leur jeune fils à Los Angeles.
Elle a luxé mes sous-vêtements dans l'Afrique la plus sombre."
Hepburn (un abstinent) a fait pire dans les conditions difficiles, perdant du poids et tombant à un moment donné très malade.
Malgré l'inconfort de sauter du bateau dans les marécages, les rivières et les marais, The African Queen a apparemment ravivé l'amour précoce de Bogart pour les bateaux; à son retour en Californie, il acheta un runabout Hacker-Craft classique en acajou qu'il conserva jusqu'à sa mort.
Lorsque Bogart a gagné, cependant, il a déclaré : « Il y a loin du Congo belge à la scène de ce théâtre.
Comme au tennis, vous avez besoin d'un bon adversaire ou d'un bon partenaire pour faire ressortir le meilleur de vous-même.
Bien qu'il ait conservé une partie de son ancienne amertume d'avoir à le faire, il a livré une solide performance en tête; il a reçu sa dernière nomination aux Oscars et a fait l'objet d'un article de couverture de Time bamazine le 7 juin 1954 .
C'est le genre de réalisateur avec qui je n'aime pas travailler... l'image est de la merde.
Malgré l'acrimonie, le film a réussi; selon une critique du New York Times , Bogart était "incroyablement adroit ... l'habileté avec laquelle ce vieil acteur aux côtes rock mélange les gags et de telles duplicités avec une manière virile de fondre est l'une des joies incalculables du spectacle" .
Il était mal à l'aise avec Ava Gardner dans le rôle principal féminin; elle venait de rompre avec son copain du Rat Pack, Frank Sinatra, et Bogart était agacé par sa performance inexpérimentée.
Lorsque Bacall les a trouvés ensemble, elle a extrait une virée shopping coûteuse de son mari; les trois ont voyagé ensemble après la fusillade.
Il est également apparu dans The Jack Benny Show , où un kinéscope survivant de la diffusion en direct le capture dans sa seule performance de sketch télévisé (25 octobre 1953).
Stephen est devenu auteur et biographe et a animé une émission spéciale sur son père sur Turner Classic Movies.
Dans le sillage de Santana, Bogart avait monté une nouvelle société et avait des projets pour un film (Melville Goodwin, USA) dans lequel il jouerait un général et Bacall un bamnate de presse.
Il n'a pas parlé de sa santé et a rendu visite à un médecin en janvier 1956 après une persuasion considérable de Bacall.
Il a subi une intervention chirurgicale supplémentaire en novembre 1956, lorsque le cancer s'est métastasé.
Il y avait écrit dessus : "Si tu veux quelque chose, siffle simplement."
Ayant étudié avec Stella Adler dans les années 1940, on lui attribue le mérite d'avoir été l'un des premiers acteurs à apporter au grand public le système d'acteur et de méthode Stanislavski, dérivé du système Stanislavski.
Il a réalisé et joué dans le western culte One-Eyed Jacks, un flop critique et commercial, après quoi il a livré une série d'échecs notables au box-office, à commencer par Mutiny on the Bounty (1962).
Il a refusé le prix en raison de mauvais traitements et d'une mauvaise représentation des Amérindiens par Hollywood.
Selon le Livre Guinness des records du monde, Brando a reçu un montant record de 3,7 millions de dollars (millions de dollars en dollars corrigés de l'inflation) et 11,75% des bénéfices bruts pour 13 jours de travail sur Superman.
Son ascendance était principalement allemande, hollandaise, anglaise et irlandaise.
Brando a été élevé comme scientifique chrétien.
Cependant, elle était alcoolique et devait souvent être ramenée des bars de Chicago par son mari.
Brando nourrissait beaucoup plus d'inimitié pour son père, déclarant: "J'étais son homonyme, mais rien de ce que j'ai fait ne l'a jamais plu ni même intéressé.
Vers 1930, les parents de Brando ont déménagé à Evanston, dans l'Illinois, lorsque le travail de son père l'a emmené à Chicago, mais se sont séparés en 1935 lorsque Brando avait 11 ans.
Brando, dont le surnom d'enfance était "Bud", était un imitateur de sa jeunesse.
Dans le biopic TCM 2007 Brando: The Documentary , l'ami d'enfance George Englund se souvient des premiers actes de Brando en imitant les vaches et les chevaux de la ferme familiale afin de distraire sa mère de l'alcool.
La sœur de Brando, Frances, a quitté l'université en Californie pour étudier l'art à New York.
Brando excellait au théâtre et réussissait bien à l'école.
La faculté a voté pour l'expulser, bien qu'il ait été soutenu par les étudiants, qui pensaient que l'expulsion était trop dure.
Dans un documentaire de 1988, Marlon Brando: The Wild One, la sœur de Brando, Jocelyn, se souvient : "Il était dans une pièce de théâtre à l'école et l'aimait... Alors il a décidé d'aller à New York et d'étudier le théâtre parce que c'était la seule chose qu'il avait. apprécié.
Pendant un certain temps, il a vécu avec Roy Somlyo, qui est devenu plus tard un producteur de Broadway quatre fois récompensé par un Emmy.
La remarquable perspicacité et le sens du réalisme de Brando étaient évidents dès le début.
Selon Dustin Hoffman dans sa Masterclass en ligne, Brando parlait souvent aux caméramans et aux autres acteurs de leur week-end même après que le réalisateur ait appelé à l'action.
Son comportement l'a fait expulser du casting de la production de la New School à Sayville, mais il a été découvert peu de temps après dans une pièce produite localement là-bas.
Cornell l'a également choisi comme Messager dans sa production d' Antigone de Jean Anouilh la même année.
Bankhead avait refusé le rôle de Blanche Dubois dans A Streetcar Named Desire , que Williams avait écrit pour elle, pour tourner la pièce pour la saison 1946-1947.
Wilson était largement tolérant envers le comportement de Brando, mais il atteignit sa limite lorsque Brando marmonna lors d'une répétition générale peu avant l'ouverture du 28 novembre 1946. "
C'était merveilleux", se souvient un membre de la distribution. "
Les critiques n'étaient cependant pas aussi aimables.
Il a reçu de meilleures critiques lors des étapes de tournée suivantes, mais ce que ses collègues se souvenaient n'était que des indications occasionnelles du talent qu'il démontrerait plus tard. "
Brando a montré son apathie pour la production en démontrant des manières choquantes sur scène.
Après plusieurs semaines sur la route, ils atteignirent Boston, date à laquelle Bankhead était prêt à le renvoyer.
Pierpont écrit que John Garfield était le premier choix pour le rôle, mais "a fait des demandes impossibles".
Cela humanise le personnage de Stanley en ce qu'il devient la brutalité et l'insensibilité de la jeunesse plutôt qu'un vieil homme vicieux ... Une nouvelle valeur est ressortie de la lecture de Brando qui était de loin la meilleure lecture que j'aie jamais entendue.
Il a dit: "Le rideau s'est levé et sur scène se trouve ce fils de pute du gymnase, et il me joue."
Le premier rôle à l'écran de Brando était un vétéran paraplégique amer dans The Men (1950).
Selon le propre récit de Brando, c'est peut-être à cause de ce film que son statut de brouillon est passé de 4-F à 1-A. Il avait subi une intervention chirurgicale au genou, et ce n'était plus suffisamment débilitant physiquement pour être exclu du repêchage.
Par coïncidence, le psychiatre connaissait un médecin ami de Brando.
Le rôle est considéré comme l'un des plus grands de Brando.
Le film a été réalisé par Elia Kazan et co-vedette Anthony Quinn.
Pendant nos scènes ensemble, j'ai senti une amertume envers moi, et si je proposais un verre après le travail, soit il me refusait, soit il était maussade et parlait peu.
Après avoir obtenu l'effet désiré, Kazan n'a jamais dit à Quinn qu'il l'avait induit en erreur.
Gielgud a été tellement impressionné qu'il a offert à Brando une saison complète au Hammersmith Theatre, une offre qu'il a déclinée.
C'était comme si la porte d'un four s'ouvrait – la chaleur sortait de l'écran.
Au dire de tous, Brando a été bouleversé par la décision de son mentor, mais il a de nouveau travaillé avec lui dans On The Waterfront. "
Les importateurs de Triumph étaient ambivalents lors de l'exposition, car le sujet était des gangs de motards tapageurs prenant le contrôle d'une petite ville.
Lorsqu'on lui a initialement proposé le rôle, Brando - toujours piqué par le témoignage de Kazan à HUAC - a refusé et le rôle de Terry Malloy a failli revenir à Frank Sinatra.
Brando a remporté l'Oscar pour son rôle de docker irlandais-américain Terry Malloy dans On the Waterfront.
Dans sa critique du 29 juillet 1954, le critique du New York Times AH Weiler a fait l'éloge du film, le qualifiant d '"utilisation inhabituellement puissante, excitante et ibaminative de l'écran par des professionnels doués".
Il incarne Napoléon dans le film Désirée de 1954.
Brando méprisait particulièrement le réalisateur Henry Koster.
Les relations entre Brando et son co-vedette Frank Sinatra étaient également glaciales, avec Stefan Kanfer observant : "Les deux hommes étaient diamétralement opposés : Marlon nécessitait plusieurs prises ; Frank détestait se répéter."
Frank Sinatra a appelé Brando "l'acteur le plus surestimé du monde" et l'a qualifié de "marmonne".
Pauline Kael n'a pas été particulièrement impressionnée par le film, mais a noté que "Marlon Brando s'est affamé pour jouer l'interprète de lutin Sakini, et il a l'air d'apprécier la cascade - parler avec un accent fou, sourire de garçon, se pencher en avant et faire des mouvements délicats avec ses jambes.
Newsweek a trouvé le film comme "l'histoire ennuyeuse de la rencontre des deux", mais ce fut néanmoins un succès au box-office.
Le film a ensuite remporté quatre Oscars.
Au dire de tous, Brando a été dévastée par sa mort, le biographe Peter Manso racontant à A&E's Biography : "C'est elle qui pouvait lui donner son approbation comme personne d'autre ne le pouvait et, après la mort de sa mère, il semble que Marlon cesse de s'en soucier."
The Young Lions présente également la seule apparition de Brando dans un film avec son ami et rival Montgomery Clift (bien qu'ils n'aient partagé aucune scène ensemble).
Brando incarne le personnage principal Rio et Karl Malden joue son partenaire "Dad" Longworth.
L'inexpérience de Brando en tant que monteur a également retardé la postproduction et Paramount a finalement pris le contrôle du film.
À ce moment-là, je m'ennuyais avec tout le projet et je m'en suis éloigné."
La révulsion de Brando avec l'industrie cinématographique aurait débordé sur le tournage de son prochain film, le remake de Metro-Goldwyn-Mayer de Mutiny on the Bounty, qui a été tourné à Tahiti.
Le réalisateur de Mutiny, Lewis Milestone, a affirmé que les dirigeants "méritent ce qu'ils obtiennent lorsqu'ils donnent à un acteur amateur, un enfant irritable, le contrôle total d'une image coûteuse".
The Ugly American (1963) est le premier de ces films.
Tous les autres films Universal de Brando pendant cette période, y compris Bedtime Story (1964), The Appaloosa (1966), A Countess from Hong Kong (1967) et The Night of the Next Day (1969), ont également été des flops critiques et commerciaux.
Brando était également apparu dans le thriller d'espionnage Morituri en 1965; cela aussi n'a pas réussi à attirer un public.
Candy était particulièrement épouvantable pour beaucoup ; un film de farce sexuelle de 1968 réalisé par Christian Marquand et basé sur le roman de 1958 de Terry Southern, le film fait la satire d'histoires pornographiques à travers les aventures de son héroïne naïve, Candy, interprétée par Ewa Aulin.
Dans le numéro de mars 1966 de The Atlantic, Pauline Kael écrivait qu'à l'époque où il était rebelle, Brando "était antisocial parce qu'il savait que la société était de la merde; il était un héros pour la jeunesse parce qu'il était assez fort pour ne pas prendre la merde", mais maintenant Brando et d'autres comme lui étaient devenus « des bouffons, sans vergogne, se moquant pathétiquement de leur réputation publique ».
J'étais très convaincant dans ma pose d'indifférence, mais j'étais très sensible et ça faisait très mal."
Le film a globalement reçu des critiques mitigées.
Brando a consacré un chapitre complet au film dans ses mémoires, déclarant que le réalisateur, Gillo Pontecorvo, était le meilleur réalisateur avec lequel il ait jamais travaillé à côté de Kazan et Bernardo Bertolucci.
En 1971, Michael Winner le dirige dans le film d'horreur britannique The Nightcomers avec Stephanie Beacham, Thora Hird, Harry Andrews et Anna Palk.
Il a battu Brando aux New York Film Critics Circle Awards de 1972.)
Brando avait également One-Eyed Jacks travaillant contre lui, une production troublée qui a fait perdre de l'argent à Paramount lors de sa sortie en 1961.
Coppola a convaincu Brando de passer un test de "maquillage" enregistré sur vidéo, dans lequel Brando s'est maquillé lui-même (il a utilisé des boules de coton pour simuler les joues gonflées du personnage).
Brando avait lui-même des doutes, déclarant dans son autobiographie: "Je n'avais jamais joué un Italien auparavant, et je ne pensais pas pouvoir le faire avec succès."
Brando a été signé pour une somme modique de 50 000 $, mais dans son contrat, il a reçu un pourcentage du brut sur une échelle mobile : 1 % du brut pour chaque tranche de 10 millions de dollars au-dessus d'un seuil de 10 millions de dollars, jusqu'à 5 % si la photo dépassé 60 millions de dollars.
Dans une interview de 1994 qui peut être trouvée sur le site de l'Academy of Achievement, Coppola a insisté : "Le Parrain était un film très peu apprécié lorsque nous le faisions.
Ils n'aimaient pas la façon dont je tournais.
Dans une interview télévisée de 2010 avec Larry King, Al Pacino a également expliqué comment le soutien de Brando l'avait aidé à conserver le rôle de Michael Corleone dans le film, malgré le fait que Coppola voulait le renvoyer.
Il a brisé la glace en portant un toast au groupe avec un verre de vin." '
Caan ajoute : "Le premier jour où nous avons rencontré Brando, tout le monde était impressionné.""
De plus, parce qu'il avait tellement de pouvoir et d'autorité incontestée, j'ai pensé que ce serait un contraste intéressant de le jouer comme un homme doux, contrairement à Al Capone, qui battait les gens avec des battes de baseball."
Il n'y avait vraiment pas de début.
Il a boycotté la cérémonie de remise des prix, envoyant à la place l'activiste des droits des autochtones américains Sacheen Littlefeather, qui est apparu en tenue complète d'Apache, pour exposer les raisons de Brando, qui étaient basées sur son objection à la représentation des autochtones américains par Hollywood et la télévision.
Comme pour les films précédents, Brando a refusé de mémoriser ses répliques pour de nombreuses scènes; au lieu de cela, il a écrit ses lignes sur des cartes de repère et les a affichées autour du plateau pour une référence facile, laissant à Bertolucci le problème de les garder hors du cadre de l'image.
Son accord de participation brute lui a rapporté 3 millions de dollars.
Pauline Kael, dans la critique du New Yorker, a écrit "La percée cinématographique est enfin arrivée.
En 1973, Brando a été dévasté par la mort de son meilleur ami d'enfance Wally Cox.
Absent pendant la première heure du film, Clayton entre à cheval, la tête en bas, caparaçonné de daim blanc, façon Littlefeather.
Penn, qui croyait qu'il fallait laisser les acteurs faire leur truc, s'est laissé aller à Marlon jusqu'au bout."
En 1978, Brando a raconté la version anglaise de Raoni , un film documentaire franco-belge réalisé par Jean-Pierre Dutilleux et Luiz Carlos Saldanha qui se concentrait sur la vie de Raoni Metuktire et les problèmes entourant la survie des tribus indiennes indigènes du centre-nord du Brésil.
En 1979, il fait une rare apparition télévisée dans la mini-série Roots: The Next Generations , incarnant George Lincoln Rockwell ; il a remporté un Primetime Emmy Award du meilleur acteur dans un second rôle dans une mini-série ou un film pour sa performance.
Brando a été payé 1 million de dollars par semaine pour 3 semaines de travail.
Dans le documentaire, Coppola raconte à quel point il a été étonné lorsqu'un Brando en surpoids s'est présenté pour ses scènes et, désespéré, a décidé de dépeindre Kurtz, qui semble émacié dans l'histoire originale, comme un homme qui s'est livré à tous les aspects de lui-même.
Cependant, il revient en 1989 dans A Dry White Season , basé sur le roman anti-apartheid d' André Brink de 1979 .
Brando a reçu des éloges pour sa performance, remportant une nomination aux Oscars du meilleur acteur dans un second rôle et remportant le prix du meilleur acteur au Festival du film de Tokyo.
Variety a également fait l'éloge de la performance de Brando en tant que Sabatini et a noté: "La sublime performance comique de Marlon Brando élève The Freshman de la comédie visqueuse à une niche excentrique dans l'histoire du cinéma."
Le scénariste de l'île du Dr Moreau, Ron Hutchinson, dira plus tard dans ses mémoires, Clinging to the Iceberg: Writing for a Living on the Stage and in Hollywood (2017), que Brando a saboté la production du film en se disputant et en refusant de coopérer avec ses collègues. et l'équipe du film.
Ce fut son dernier rôle et son seul rôle en tant que personnage féminin.
Le fils de l'acteur, Miko, a été le garde du corps et l'assistant de Jackson pendant plusieurs années, et était un ami du chanteur. "
Papa avait du mal à respirer dans ses derniers jours et il était sous oxygène la plupart du temps.
Alors Michael a offert à papa une voiturette de golf avec un réservoir d'oxygène portable pour qu'il puisse se promener et profiter de Neverland.
Il souffrait également de diabète et d'un cancer du foie.
Sa seule ligne enregistrée a été incluse dans le jeu final en hommage à l'acteur.
Un Brando en détresse a dit à Malden qu'il n'arrêtait pas de tomber.
Peu de temps avant sa mort, il aurait refusé l'autorisation d'insérer des tubes transportant de l'oxygène dans ses poumons, ce qui, lui aurait-on dit, était le seul moyen de prolonger sa vie.
En 1976, il déclare à un journaliste français : « L'homosexualité est tellement à la mode qu'elle ne fait plus l'actualité.
Il a également revendiqué de nombreuses autres romances, bien qu'il n'ait pas parlé de ses mariages, de ses femmes ou de ses enfants dans son autobiographie.
Brando a rencontré l'actrice Rita Moreno en 1954 et ils ont commencé une histoire d'amour.
Des années après leur rupture, Moreno a joué son amour dans le film La nuit du jour suivant.
Elle aurait été la fille d'un ouvrier sidérurgiste gallois d'origine irlandaise, William O'Callaghan, qui avait été surintendant des chemins de fer de l'État indien.
Brando et Kashfi ont eu un fils, Christian Brando, le 11 mai 1958 ; ils ont divorcé en 1959.
Ils ont eu deux enfants ensemble : Miko Castaneda Brando (née en 1961) et Rebecca Brando (née en 1966).
Parce que Teriipaia était un locuteur natif français, Brando est devenu à l'aise dans la langue et a donné de nombreuses interviews en français.
Brando et Teriipaia ont divorcé en juillet 1972.
Brando avait une relation à long terme avec sa gouvernante Maria Cristina Ruiz, avec qui il a eu trois enfants : Ninna Priscilla Brando (née le 13 mai 1989), Myles Jonathan Brando (né le 16 janvier 1992) et Timothy Gahan Brando (né en janvier 6, 1994).
Ses nombreux petits-enfants comprennent également Prudence Brando et Shane Brando, enfants de Miko C. Brando; les enfants de Rebecca Brando; et les trois enfants de Teihotu Brando entre autres.
Son comportement lors du tournage de Mutiny on the Bounty (1962) semble renforcer sa réputation de star difficile.
Galella avait suivi Brando, qui était accompagné de l'animateur de talk-show Dick Cavett, après un enregistrement de The Dick Cavett Show à New York.
Le tournage de Mutiny on the Bounty a profondément marqué la vie de Brando, car il est tombé amoureux de Tahiti et de ses habitants.
L'ouragan de 1983 a détruit de nombreuses structures, y compris sa station balnéaire.
Il a été répertorié dans les dossiers de la Federal Communications Commission (FCC) sous le nom de Martin Brandeaux pour préserver sa vie privée.
Il a assisté à des collectes de fonds pour John F. Kennedy lors de l'élection présidentielle de 1960.
À l'automne 1967, Brando s'est rendu à Helsinki, en Finlande, lors d'une soirée caritative organisée par l'UNICEF au théâtre municipal d'Helsinki.
Il s'est prononcé en faveur des droits des enfants et de l'aide au développement dans les pays en développement.
J'ai senti que je ferais mieux d'aller découvrir où c'est; ce que c'est d'être noir dans ce pays ; de quoi parle cette rage", a déclaré Brando dans le talk-show de fin de soirée d'ABC-TV Joey Bishop Show.
C'était l'un des actes de courage les plus incroyables que j'aie jamais vus, et cela signifiait beaucoup et a fait beaucoup."
En 1964, Brando a été arrêté lors d'un "fish-in" organisé pour protester contre un traité rompu qui avait promis aux Amérindiens des droits de pêche à Puget Sound.
Brando a mis fin à son soutien financier au groupe en raison de sa perception de sa radicalisation croissante, en particulier d'un passage d'un pamphlet Panther publié par Eldridge Cleaver prônant la violence aveugle, «pour la Révolution».
Sacheen Littlefeather l'a représenté lors de la cérémonie.
L'événement a attiré l'attention des médias américains et mondiaux.
Il était aussi un militant contre l'apartheid.
Il est répertorié par l' American Film Institute comme la quatrième plus grande star masculine dont les débuts à l'écran ont eu lieu avant ou pendant 1950 (cela s'est produit en 1950).
Récupéré le 19 août 2009. L'Encyclopedia Britannica le décrit comme "le plus célèbre des acteurs de la méthode, et sa prestation brouillée et marmonnante a marqué son rejet de la formation dramatique classique.
Il était un développement du chef de gangster et du hors-la-loi.
Son interprétation du chef de gang Johnny Strabler dans The Wild One est devenue un ibame emblématique, utilisé à la fois comme symbole de rébellion et accessoire de mode qui comprend une veste de moto de style Perfecto, une casquette inclinée, un jean et des lunettes de soleil.
La scène "J'aurais pu être un concurrent" de On the Waterfront, selon l'auteur de Brooklyn Boomer, Martin H. Levinson, est "l'une des scènes les plus célèbres de l'histoire du cinéma, et la ligne elle-même fait désormais partie du patrimoine culturel américain". lexique."
Vous devez leur faire croire que vous êtes en train de mourir... Essayez de penser au moment le plus intime que vous ayez jamais eu dans votre vie."
En 1999, l'American Film Institute l'a classé huitième parmi sa liste des plus grandes stars masculines de l'âge d'or d'Hollywood.
Il a passé plusieurs années dans le vaudeville en tant que danseur et comédien, jusqu'à ce qu'il obtienne son premier grand rôle d'acteur en 1925.
Après des critiques élogieuses, Warner Bros. l'a signé pour un contrat initial de 400 $ par semaine sur trois semaines; lorsque les dirigeants du studio ont vu les premiers rushs du film, le contrat de Cagney a été immédiatement prolongé.
Il est nominé une troisième fois en 1955 pour Love Me or Leave Me with Doris Day.
Cagney a quitté Warner Bros. plusieurs fois au cours de sa carrière, revenant à chaque fois dans des conditions personnelles et artistiques bien améliorées.
Il a travaillé pour une société de cinéma indépendante pendant un an pendant que le procès était réglé, créant sa propre société de production, Cagney Productions, en 1942 avant de revenir à Warner sept ans plus tard.
Cagney était le deuxième de sept enfants, dont deux sont morts quelques mois après leur naissance.
La famille a déménagé deux fois alors qu'il était encore jeune, d'abord à East 79th Street, puis à East 96th Street.
Je suis désolé pour le gamin qui passe un moment trop pépère.
C'était un bon combattant de rue, défendant son frère aîné Harry, étudiant en médecine, quand c'était nécessaire.
Il s'est impliqué dans le théâtre amateur, commençant comme décorateur pour une pantomime chinoise à Lenox Hill Neighborhood House (l'une des premières maisons de colonisation du pays) où son frère Harry a joué et Florence James a dirigé.
Le spectacle a commencé l'association de 10 ans de Cagney avec le vaudeville et Broadway.
Finalement, ils ont emprunté de l'argent et sont retournés à New York via Chicago et Milwaukee, endurant un échec en cours de route lorsqu'ils ont tenté de gagner de l'argent sur scène.
Comme avec Pitter Patter, Cagney est allé à l'audition avec peu de confiance qu'il obtiendrait le rôle.
Ce fut une tournure dévastatrice des événements pour Cagney; outre les difficultés logistiques que cela présentait, les bagages du couple étaient dans la soute du navire et ils avaient renoncé à leur appartement.
Il a décidé qu'il trouverait un travail en faisant autre chose."
Cagney a également créé une école de danse pour les professionnels, puis a décroché un rôle dans la pièce Women Go On Forever, mise en scène par John Cromwell, qui a duré quatre mois.
Le spectacle a reçu des critiques élogieuses et a été suivi par Grand Street Follies de 1929.
Rebaptisé Sinners' Holiday, le film est sorti en 1930.
Cependant, le contrat permettait à Warners de le laisser tomber à la fin de toute période de 40 semaines, ne lui garantissant en fait que 40 semaines de revenu à la fois.
En raison des bonnes critiques qu'il avait reçues au cours de sa carrière dans le court métrage, Cagney a été choisi pour incarner le gentil Matt Doyle, face à Edward Woods dans le rôle de Tom Powers.
Le producteur Darryl Zanuck a affirmé qu'il y avait pensé lors d'une conférence sur le scénario; Wellman a déclaré que l'idée lui est venue lorsqu'il a vu le pamplemousse sur la table pendant le tournage; et les écrivains Glasmon et Bright ont affirmé qu'il était basé sur la vraie vie du gangster Hymie Weiss, qui a jeté une omelette au visage de sa petite amie.
Je n'aurais jamais imaginé que cela serait montré dans le film.
Il a vu le film à plusieurs reprises juste pour voir cette scène, et a souvent été étouffé par des clients en colère lorsque son rire ravi devenait trop fort."
Warner Bros. n'a pas tardé à faire équipe avec ses deux stars montantes du gangster - Edward G. Robinson et Cagney - pour le film Smart Money de 1931.
Alors qu'il terminait le tournage, The Public Enemy remplissait les cinémas de projections toute la nuit.
Les chefs de studio ont également insisté pour que Cagney continue de promouvoir leurs films, même ceux dans lesquels il n'était pas, ce à quoi il s'est opposé.
Le succès de The Public Enemy et Blonde Crazy a forcé Warner Bros. main.
Le film a été rapidement suivi par The Crowd Roars et Winner Take All.
Les historiens débattent également de la nature de l'histoire en tant que fin en soi, ainsi que de son utilité pour mettre en perspective les problèmes du présent.
Cependant, les influences culturelles anciennes ont contribué à engendrer des interprétations variées de la nature de l'histoire qui ont évolué au fil des siècles et continuent de changer aujourd'hui.
Hérodote , un historien grec du Ve siècle avant JC, est souvent considéré comme le «père de l'histoire» dans la tradition occidentale, bien qu'il ait également été critiqué comme le «père du mensonge».
En moyen anglais, le sens de l'histoire était "histoire" en général.
Dans l'allemand moderne, le français et la plupart des langues germaniques et romanes, qui sont solidement synthétiques et fortement infléchies, le même mot est encore utilisé pour signifier à la fois "histoire" et "récit".
Selon les mots de Benedetto Croce, "Toute l'histoire est une histoire contemporaine".
Dès lors, la constitution de l'archive de l'historien résulte d'une délimitation d'une archive plus générale en invalidant l'usage de certains textes et documents (en falsifiant leurs prétentions à représenter le « vrai passé »).
L'étude de l'histoire a parfois été classée comme faisant partie des sciences humaines et à d'autres moments comme faisant partie des sciences sociales.
Au XXe siècle, l'historien français Fernand Braudel a révolutionné l'étude de l'histoire en utilisant des disciplines extérieures telles que l'économie, l'anthropologie et la géographie dans l'étude de l'histoire mondiale.
En général, les sources de connaissances historiques peuvent être séparées en trois catégories : ce qui est écrit, ce qui est dit et ce qui est physiquement conservé, et les historiens consultent souvent les trois.
Les découvertes archéologiques sont rarement isolées, des sources narratives complétant ses découvertes.
Par exemple, Mark Leone, l'excavateur et interprète d'Annapolis historique, Maryland, USA, a cherché à comprendre la contradiction entre les documents textuels idéalisant la "liberté" et les archives matérielles, démontrant la possession d'esclaves et les inégalités de richesse rendues apparentes par la étude de l'environnement historique total.
Il est possible pour les historiens de se préoccuper à la fois du très spécifique et du très général, bien que la tendance moderne ait été à la spécialisation.
Troisièmement, il peut renvoyer à la raison pour laquelle l'histoire est produite : la philosophie de l'histoire.
Par qui a-t-il été produit (paternité) ?
Quelle est la valeur probante de son contenu (crédibilité) ?
La méthode historique comprend les techniques et les directives par lesquelles les historiens utilisent des sources primaires et d'autres preuves pour rechercher puis écrire l'histoire.
Thucydide, contrairement à Hérodote, considérait l'histoire comme étant le produit des choix et des actions des êtres humains, et regardait la cause et l'effet, plutôt que comme le résultat d'une intervention divine (bien qu'Hérodote ne soit pas entièrement attaché à cette idée lui-même).
Il y avait des traditions historiques et une utilisation sophistiquée de la méthode historique dans la Chine ancienne et médiévale.
Les historiens chinois des périodes dynastiques ultérieures en Chine ont utilisé son Shiji comme format officiel pour les textes historiques, ainsi que pour la littérature biographique.
Vers 1800, le philosophe et historien allemand Georg Wilhelm Friedrich Hegel a apporté la philosophie et une approche plus laïque dans l'étude historique.
L'originalité d'Ibn Khaldoun était de prétendre que la différence culturelle d'un autre âge doit présider à l'évaluation du matériel historique pertinent, de distinguer les principes selon lesquels il serait possible de tenter l'évaluation, et enfin, de ressentir le besoin d'expérience, en plus des principes rationnels, afin d'évaluer une culture du passé.
Sa méthode historique a également jeté les bases de l'observation du rôle de l'État, de la communication, de la propagande et des préjugés systématiques dans l'histoire, H. Mowlana (2001). "
Dr SW Akhtar (1997). "
Pour Ranke, les données historiques doivent être collectées avec soin, examinées objectivement et rassemblées avec une rigueur critique.
Au XXe siècle, les historiens universitaires se sont moins concentrés sur les récits nationalistes épiques, qui avaient souvent tendance à glorifier la nation ou les grands hommes, que sur des analyses plus objectives et complexes des forces sociales et intellectuelles.
De nombreux partisans de l'histoire en tant que science sociale étaient ou sont connus pour leur approche multidisciplinaire.
Jusqu'à présent, une seule théorie de l'histoire est venue de la plume d'un historien professionnel.
Des historiens intellectuels tels que Herbert Butterfield, Ernst Nolte et George Mosse ont défendu l'importance des idées dans l'histoire.
Des universitaires tels que Martin Broszat, Ian Kershaw et Detlev Peukert ont cherché à examiner à quoi ressemblait la vie quotidienne des gens ordinaires dans l'Allemagne du XXe siècle, en particulier à l'époque nazie.
Des historiennes féministes telles que Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese et Lynn Hunt ont plaidé pour l'importance d'étudier l'expérience des femmes dans le passé.
Une autre défense de l'histoire contre la critique post-moderniste était le livre de 1994 de l'historien australien Keith Windschuttle, The Killing of History.
Les omissions historiques peuvent se produire de plusieurs façons et peuvent avoir un effet profond sur les documents historiques.
Histoire ancienne: l'étude depuis le début de l'histoire humaine jusqu'au haut Moyen Âge.
Histoire comparée : analyse historique d'entités sociales et culturelles non confinées aux frontières nationales.
Histoire culturelle: l'étude de la culture dans le passé.
Histoire intellectuelle : étude des idées dans le contexte des cultures qui les ont produites et de leur évolution dans le temps.
Histoire moderne: l'étude des temps modernes, l'ère après le Moyen Âge.
Paléographie : étude des textes anciens.
Psychohistoire : étude des motivations psychologiques des événements historiques.
Histoire des femmes: l'histoire des êtres humains féminins.
Les siècles et les décennies sont des périodes couramment utilisées et le temps qu'ils représentent dépend du système de datation utilisé.
Pour ce faire, les historiens se tournent souvent vers la géographie.
Par exemple, pour expliquer pourquoi les anciens Égyptiens ont développé une civilisation prospère, l'étude de la géographie de l'Égypte est essentielle.
L'histoire des Amériques est l'histoire collective de l'Amérique du Nord et du Sud, y compris l'Amérique centrale et les Caraïbes.
L'histoire des Caraïbes commence avec la preuve la plus ancienne où des vestiges vieux de 7 000 ans ont été trouvés.
L'histoire de l'Eurasie est l'histoire collective de plusieurs régions côtières périphériques distinctes : le Moyen-Orient, l'Asie du Sud, l'Asie de l'Est, l'Asie du Sud-Est et l'Europe, liées par la masse intérieure de la steppe eurasienne d'Asie centrale et d'Europe de l'Est.
L'histoire de l'Asie de l'Est est l'étude du passé transmis de génération en génération en Asie de l'Est.
L'histoire de l'Asie du Sud-Est a été caractérisée comme une interaction entre les acteurs régionaux et les puissances étrangères.
La «vieille» histoire sociale avant les années 1960 était un méli-mélo de sujets sans thème central, et elle incluait souvent des mouvements politiques, comme le populisme, qui étaient «sociaux» dans le sens d'être en dehors du système d'élite.
Il examine les archives et les descriptions narratives des connaissances, coutumes et arts passés d'un groupe de personnes.
Ce type d'histoire politique est l'étude de la conduite des relations internationales entre États ou à travers les frontières des États au fil du temps.
Il a gagné en popularité aux États-Unis, au Japon et dans d'autres pays après les années 1980 avec la prise de conscience que les étudiants ont besoin d'une plus grande exposition au monde à mesure que la mondialisation progresse.
Bien qu'il s'agisse d'un domaine relativement nouveau, l'histoire du genre a eu un effet significatif sur l'étude générale de l'histoire.
À Oxford et à Cambridge, l'érudition était minimisée.
Les tuteurs ont dominé le débat jusqu'après la Seconde Guerre mondiale.
Aux États-Unis après la Première Guerre mondiale, un fort mouvement a émergé au niveau universitaire pour enseigner des cours de civilisation occidentale, afin de donner aux étudiants un héritage commun avec l'Europe.
Beaucoup voient le domaine des deux points de vue.
Aux États-Unis, les manuels publiés par la même société diffèrent souvent par leur contenu d'un État à l'autre.
Les historiens universitaires ont souvent lutté contre la politisation des manuels, parfois avec succès.
Une civilisation (ou civilisation) est une société complexe caractérisée par le développement urbain, la stratification sociale, une forme de gouvernement et des systèmes symboliques de communication (comme l'écriture).
Dans ce sens large, une civilisation s'oppose aux sociétés tribales non centralisées, incluant les cultures d'éleveurs nomades, les sociétés néolithiques ou les chasseurs-cueilleurs ; cependant, parfois, cela contraste également avec les cultures trouvées au sein des civilisations elles-mêmes.
Le traité fondamental est The Civilizing Process (1939) de Norbert Elias, qui retrace les mœurs sociales de la société courtoise médiévale à la période moderne.
Des mots apparentés comme « civilité » se sont développés au milieu du XVIe siècle.
À la fin des années 1700 et au début des années 1800, pendant la Révolution française, la « civilisation » était utilisée au singulier, jamais au pluriel, et signifiait le progrès de l'humanité dans son ensemble.
Ce n'est que dans ce sens généralisé qu'il devient possible de parler d'une "civilisation médiévale", ce qui, au sens d'Elias, aurait été un oxymore.
Ici, la civilisation, étant plus rationnelle et socialement motivée, n'est pas pleinement en accord avec la nature humaine, et "la plénitude humaine n'est réalisable que par la récupération ou le rapprochement d'une unité naturelle discursive ou prérationnelle originale" (voir noble sauvage ).
Les civilisations ont été distinguées par leurs moyens de subsistance, les types de moyens de subsistance, les modèles de peuplement, les formes de gouvernement, la stratification sociale, les systèmes économiques, l'alphabétisation et d'autres traits culturels.
Toutes les civilisations ont dépendu de l'agriculture pour leur subsistance, à l'exception peut-être de certaines civilisations anciennes du Pérou qui ont pu dépendre des ressources maritimes.
Les excédents céréaliers ont été particulièrement importants parce que les céréales peuvent être stockées pendant longtemps.
Cependant, dans certains endroits, les chasseurs-cueilleurs ont eu accès à des excédents alimentaires, comme chez certains peuples autochtones du nord-ouest du Pacifique et peut-être pendant la culture mésolithique natoufienne.
Le mot "civilisation" est parfois simplement défini comme ""vivre dans les villes"".
Les sociétés étatiques sont plus stratifiées que les autres sociétés ; il y a une plus grande différence entre les classes sociales.
Civilisations, avec des hiérarchies sociales complexes et des gouvernements organisés et institutionnels.
Certaines personnes acquièrent également une propriété foncière, ou la propriété privée de la terre.
Au début de l'âge du fer, les civilisations contemporaines ont développé la monnaie comme moyen d'échange pour des transactions de plus en plus complexes.
Ces personnes peuvent ne pas se connaître personnellement et leurs besoins peuvent ne pas survenir tous en même temps.
La transition d'économies plus simples à des économies plus complexes ne signifie pas nécessairement une amélioration du niveau de vie de la population.
La taille moyenne d'une population est une bonne mesure de l'adéquation de son accès aux nécessités, notamment alimentaires.
Comme l'argent, l'écriture était rendue nécessaire par l'importance de la population d'une ville et la complexité de son commerce entre des gens qui ne se connaissent pas tous personnellement.
Il s'agit notamment de la religion organisée, du développement des arts et d'innombrables nouvelles avancées scientifiques et technologiques.
Ces cultures sont qualifiées par certains de "primitives", terme considéré par d'autres comme péjoratif. "
Les anthropologues utilisent aujourd'hui le terme « analphabètes » pour décrire ces peuples.
Mais la civilisation se propage aussi par la domination technique, matérielle et sociale qu'elle engendre.
Les civilisations ont tendance à développer des cultures complexes, y compris un appareil de prise de décision basé sur l'État, une littérature, un art professionnel, une architecture, une religion organisée et des coutumes complexes d'éducation, de coercition et de contrôle associées au maintien de l'élite.
La civilisation dans laquelle quelqu'un vit est l'identité culturelle la plus large de cette personne.
L'objectif est de préserver le patrimoine culturel de l'humanité ainsi que l'identité culturelle, notamment en cas de guerre et de conflit armé.
Le philosophe du début du XXe siècle Oswald Spengler, Spengler, Oswald, Decline of the West: Perspectives of World History (1919) utilise le mot allemand Kultur, "culture", pour ce que beaucoup appellent une "civilisation".
Spengler déclare que la civilisation est le début du déclin d'une culture comme "les états les plus extérieurs et artificiels dont une espèce d'humanité développée est capable".
Les civilisations ont généralement décliné et sont tombées, selon Toynbee, à cause de l'échec d'une «minorité créative», par déclin moral ou religieux, à relever un défi important, plutôt que de simples causes économiques ou environnementales.
Par exemple, les réseaux commerciaux étaient, jusqu'au XIXe siècle, beaucoup plus vastes que les sphères culturelles ou les sphères politiques.
Pendant la période d'Uruk, Guillermo Algaze a soutenu que les relations commerciales reliaient l'Égypte, la Mésopotamie, l'Iran et l'Afghanistan.
Différentes civilisations et sociétés du monde entier sont économiquement, politiquement et même culturellement interdépendantes à bien des égards.
La civilisation centrale s'est ensuite étendue pour inclure l'ensemble du Moyen-Orient et de l'Europe, puis s'est étendue à l'échelle mondiale avec la colonisation européenne, intégrant les Amériques, l'Australie, la Chine et le Japon au XIXe siècle.
Cela a encouragé une révolution des produits secondaires dans laquelle les gens utilisaient des animaux domestiques non seulement pour la viande, mais aussi pour le lait, la laine, le fumier et la traction de charrues et de charrettes - un développement qui s'est répandu dans l'œcumène eurasien.
Cette zone a été identifiée comme ayant "inspiré certains des développements les plus importants de l'histoire humaine, notamment l'invention de la roue, la plantation des premières céréales et le développement de l'écriture cursive".
Ce changement climatique a déplacé le rapport coût-bénéfice de la violence endémique entre les communautés, qui a vu l'abandon des communautés villageoises sans murailles et l'apparition de villes murées, associées aux premières civilisations.
La révolution urbaine civilisée dépendait à son tour du développement de la sédentarité, de la domestication des céréales et des animaux, de la permanence des colonies et du développement de modes de vie qui facilitaient les économies d'échelle et l'accumulation de surplus de production par certains secteurs sociaux.
Certains se concentrent sur des exemples historiques, et d'autres sur la théorie générale.
Pour Gibbon, « Le déclin de Rome était l'effet naturel et inévitable d'une grandeur démesurée.
Theodor Mommsen dans son Histoire de Rome a suggéré que Rome s'est effondrée avec l'effondrement de l'Empire romain d'Occident en 476 CE et il a également tendu vers une analogie biologique de "genèse", "croissance", "sénescence", "effondrement" et "décomposition".
Arnold J. Toynbee dans son A Study of History a suggéré qu'il y avait eu un nombre beaucoup plus grand de civilisations, y compris un petit nombre de civilisations arrêtées, et que toutes les civilisations avaient tendance à suivre le cycle identifié par Mommsen.
Au cours de la phase intermédiaire, la croissance démographique croissante entraîne la diminution des niveaux de production et de consommation par habitant, il devient de plus en plus difficile de percevoir des impôts et les recettes de l'État cessent de croître, tandis que les dépenses de l'État augmentent en raison de la croissance de la population contrôlée. par l'Etat.
Cycles séculaires et tendances millénaires.
Le fait que Rome ait besoin de générer des revenus toujours plus importants pour équiper et rééquiper des armées qui, pour la première fois, ont été battues à plusieurs reprises sur le terrain, a conduit au démembrement de l'Empire.
Il soutient que l'effondrement des Mayas a des leçons pour la civilisation d'aujourd'hui.
Le rapport énergie dépensée/rendement énergétique est essentiel pour limiter la survie des civilisations.
Koneczny a affirmé que les civilisations ne peuvent pas être mélangées en hybrides, une civilisation inférieure, lorsqu'elle reçoit des droits égaux au sein d'une civilisation hautement développée, la surmontera.
L'historien culturel Morris Berman suggère dans Dark Ages America: the End of Empire que dans les États-Unis consuméristes d'entreprise, les facteurs mêmes qui les ont autrefois propulsés vers la grandeur - l'individualisme extrême, l'expansion territoriale et économique et la poursuite de la richesse matérielle - ont poussé le Les États-Unis franchissent un seuil critique où l'effondrement est inévitable.
La corrosion de ces piliers, soutient Jacobs, est liée à des maux de société tels que la crise environnementale, le racisme et le fossé grandissant entre riches et pauvres.
Ce besoin pour les civilisations d'importer toujours plus de ressources, selon lui, découle de leur surexploitation et de la diminution de leurs propres ressources locales.
Dans le graphique, Ma signifie "il y a un million d'années".)
Une grande partie de la Terre a fondu à cause de fréquentes collisions avec d'autres corps qui ont conduit à un volcanisme extrême.
Des humains reconnaissables sont apparus il y a tout au plus 2 millions d'années, une période extrêmement courte à l'échelle géologique.
On estime que 99 % de toutes les espèces qui ont jamais vécu sur Terre, soit plus de cinq milliards, ont disparu.
La croûte terrestre a constamment changé depuis sa formation, tout comme la vie depuis sa première apparition.
La Lune s'est formée à cette époque, probablement en raison de la collision d'une protoplanète avec la Terre.
L'atmosphère est composée de gaz volcaniques et de gaz à effet de serre.
Les bactéries commencent à produire de l'oxygène, façonnant le troisième et le courant des atmosphères terrestres.
Les premiers continents de Columbia, Rodinia et Pannotia, dans cet ordre, ont peut-être existé à cette époque.
Peu à peu, la vie s'étend à la terre et des formes familières de plantes, d'animaux et de champignons commencent à apparaître, notamment des annélides, des insectes et des reptiles, d'où le nom de l'éon, qui signifie "vie visible".
Il était composé d'hydrogène et d'hélium créés peu après le Big Bang 13,8 Ga (il y a des milliards d'années) et d'éléments plus lourds éjectés par des supernovae.
Lorsque le nuage a commencé à accélérer, son moment cinétique, sa gravité et son inertie l'ont aplati en un disque protoplanétaire perpendiculaire à son axe de rotation.
Après plus de contraction, une étoile T Tauri s'est enflammée et a évolué vers le Soleil.
La Terre s'est formée de cette manière il y a environ 4,54 milliards d'années (avec une incertitude de 1%) et a été en grande partie achevée en 10 à 20 millions d'années.
La proto-Terre s'est développée par accrétion jusqu'à ce que son intérieur soit suffisamment chaud pour faire fondre les métaux lourds et sidérophiles.
D'après le dénombrement des cratères sur d'autres corps célestes, il est déduit qu'une période d'impacts intenses de météorites, appelée bombardement lourd tardif, a commencé vers 4,1 Ga et s'est terminée vers 3,8 Ga, à la fin de l'Hadéen.
Au début de l'Archéen, la Terre s'était considérablement refroidie.
De nouvelles preuves suggèrent que la Lune s'est formée encore plus tard, 4,48 ± 0,02 Ga, soit 70 à 110 millions d'années après le début du système solaire.
La collision a libéré environ 100 millions de fois plus d'énergie que l'impact plus récent de Chicxulub qui aurait causé l'extinction des dinosaures non aviaires.
L'hypothèse de l'impact géant prédit que la Lune était appauvrie en matière métallique, expliquant sa composition anormale.
La croûte initiale, formée lorsque la surface de la Terre s'est solidifiée pour la première fois, a totalement disparu d'une combinaison de cette tectonique rapide des plaques hadéennes et des impacts intenses du bombardement lourd tardif.
Ces morceaux de croûte de l'Hadéen tardif et de l'Archéen précoce forment les noyaux autour desquels les continents d'aujourd'hui se sont développés.
Les cratons consistent principalement en deux types alternés de terranes.
Pour cette raison, les roches vertes sont parfois considérées comme des preuves de subduction au cours de l'Archéen.
Maintenant, il est considéré comme probable que de nombreux volatils ont été libérés lors de l'accrétion par un processus connu sous le nom de dégazage par impact dans lequel les corps entrants se vaporisent lors de l'impact.
Les planétésimaux à une distance de 1 unité astronomique (UA), la distance de la Terre au Soleil, n'ont probablement pas apporté d'eau à la Terre car la nébuleuse solaire était trop chaude pour que la glace se forme et l'hydratation des roches par la vapeur d'eau aurait ont pris trop de temps.
Des preuves récentes suggèrent que les océans ont peut-être commencé à se former dès 4,4 Ga. Au début de l'éon archéen, ils couvraient déjà une grande partie de la Terre.
Ainsi, le Soleil est devenu 30% plus brillant au cours des 4,5 derniers milliards d'années.
Il existe de nombreux modèles, mais peu de consensus, sur la façon dont la vie a émergé à partir de produits chimiques non vivants ; les systèmes chimiques créés en laboratoire sont bien en deçà de la complexité minimale pour un organisme vivant.
Bien que la composition atmosphérique soit probablement différente de celle utilisée par Miller et Urey, des expériences ultérieures avec des compositions plus réalistes ont également réussi à synthétiser des molécules organiques.
L'ARN aurait ensuite été remplacé par l'ADN, qui est plus stable et peut donc construire des génomes plus longs, élargissant ainsi la gamme de capacités qu'un seul organisme peut avoir.
Une difficulté avec le scénario du métabolisme d'abord est de trouver un moyen pour les organismes d'évoluer.
Des recherches en 2003 ont rapporté que la montmorillonite pouvait également accélérer la conversion des acides gras en "bulles", et que les bulles pouvaient encapsuler l'ARN attaché à l'argile.
Cette cellule LUA est l'ancêtre de toute vie sur Terre aujourd'hui.
Le passage à une atmosphère riche en oxygène a été une évolution cruciale.
Ils ont utilisé la fermentation, la décomposition de composés plus complexes en composés moins complexes avec moins d'énergie, et ont utilisé l'énergie ainsi libérée pour croître et se reproduire.
La majeure partie de la vie qui recouvre la surface de la Terre dépend directement ou indirectement de la photosynthèse.
Pour fournir les électrons dans le circuit, l'hydrogène est extrait de l'eau, laissant l'oxygène comme déchet.
La forme anoxygénique la plus simple est apparue vers 3,8 Ga, peu de temps après l'apparition de la vie.
Au début, l'oxygène libéré était lié au calcaire, au fer et à d'autres minéraux.
Bien que chaque cellule ne produise qu'une infime quantité d'oxygène, le métabolisme combiné de nombreuses cellules sur une longue période a transformé l'atmosphère terrestre dans son état actuel.
La couche d'ozone a absorbé, et absorbe toujours, une quantité importante du rayonnement ultraviolet qui avait autrefois traversé l'atmosphère.
En conséquence, la Terre a commencé à recevoir plus de chaleur du Soleil au cours de l'éon protérozoïque.
Les dépôts glaciaires trouvés en Afrique du Sud remontent à 2,2 Ga, date à laquelle, sur la base de preuves paléobamnétiques, ils devaient être situés près de l'équateur.
La période glaciaire huronienne pourrait avoir été causée par l'augmentation de la concentration d'oxygène dans l'atmosphère, qui a provoqué la diminution du méthane (CH4) dans l'atmosphère.
Cependant, le terme Snowball Earth est plus couramment utilisé pour décrire les périodes glaciaires extrêmes ultérieures au cours de la période cryogénienne.
Le dioxyde de carbone se combine avec la pluie pour altérer les roches pour former de l'acide carbonique, qui est ensuite emporté par la mer, extrayant ainsi le gaz à effet de serre de l'atmosphère.
Le domaine des bactéries s'est probablement d'abord séparé des autres formes de vie (parfois appelées Neomura), mais cette supposition est controversée.
Les premiers fossiles possédant des caractéristiques typiques des champignons datent de l'ère paléoprotérozoïque, il y a environ 2,4 ans; ces organismes benthiques multicellulaires avaient des structures filamenteuses capables d'anastomose.
Peut-être que la grande cellule a tenté de digérer la plus petite mais a échoué (peut-être en raison de l'évolution des défenses des proies).
En utilisant de l'oxygène, il a métabolisé les déchets de la plus grande cellule et a dérivé plus d'énergie.
Bientôt, une symbiose stable s'est développée entre la grande cellule et les petites cellules à l'intérieur.
Un événement similaire s'est produit avec des cyanobactéries photosynthétiques pénétrant dans de grandes cellules hétérotrophes et devenant des chloroplastes.
Outre la théorie endosymbiotique bien établie de l'origine cellulaire des mitochondries et des chloroplastes, il existe des théories selon lesquelles les cellules ont conduit aux peroxysomes, les spirochètes ont conduit aux cils et aux flagelles, et que peut-être un virus à ADN a conduit au noyau cellulaire, bien qu'aucun d'entre eux ne soit largement répandu. accepté.
Vers 1,1 Ga, le supercontinent Rodinia s'assemblait.
Bien que la séparation entre une colonie à cellules spécialisées et un organisme multicellulaire ne soit pas toujours claire, il y a environ 1 milliard d'années, les premières plantes multicellulaires ont émergé, probablement des algues vertes.
Les pôles paléobamnétiques sont complétés par des preuves géologiques telles que les ceintures orogéniques, qui marquent les bords des plaques anciennes, et les distributions passées de la flore et de la faune.
Vers 1000 à 830 Ma, la majeure partie de la masse continentale était réunie dans le supercontinent Rodinia.
Le supercontinent hypothétique est parfois appelé Pannotia ou Vendia.
L'intensité et le mécanisme des deux glaciations sont encore à l'étude et plus difficiles à expliquer que la première boule de neige du Protérozoïque.
Parce que le CO2 est un gaz à effet de serre important, les climats se sont refroidis à l'échelle mondiale.
L'activité volcanique accrue a résulté de l'éclatement de Rodinia à peu près au même moment.
Les nouvelles formes de vie, appelées Ediacara biota, étaient plus vastes et plus diversifiées que jamais.
Il se compose de trois ères : le Paléozoïque, le Mésozoïque et le Cénozoïque, et c'est le moment où la vie multicellulaire s'est fortement diversifiée dans presque tous les organismes connus aujourd'hui.
Cela fait monter le niveau de la mer.
Les traces de glaciation de cette période ne se retrouvent que sur l'ancien Gondwana.
Les continents Laurentia et Baltica sont entrés en collision entre 450 et 400 Ma, lors de l'orogenèse calédonienne, pour former la Laurussia (également appelée Euramerica).
La collision de la Sibérie avec la Laurussia a provoqué l'orogenèse ouralienne, la collision du Gondwana avec la Laurussia est appelée l'orogenèse varisque ou hercynienne en Europe ou l'orogenèse alleghenienne en Amérique du Nord.
Alors que les formes de vie édiacariennes apparaissent encore primitives et difficiles à ranger dans un groupe moderne, à la fin du Cambrien les phylums les plus modernes étaient déjà présents.
Certains de ces groupes cambriens semblent complexes mais sont apparemment assez différents de la vie moderne ; les exemples sont Anomalocaris et Haikouichthys.
Une créature qui aurait pu être l'ancêtre des poissons, ou qui lui était probablement étroitement liée, était Pikaia.
Les poissons, les premiers vertébrés, ont évolué dans les océans vers 530 Ma.
Les plus anciens fossiles de champignons terrestres et de plantes datent de 480 à 460 Ma, bien que des preuves moléculaires suggèrent que les champignons pourraient avoir colonisé la terre dès 1000 Ma et les plantes 700 Ma.
Les nageoires ont évolué pour devenir des membres que les premiers tétrapodes utilisaient pour lever la tête hors de l'eau pour respirer de l'air.
Finalement, certains d'entre eux sont devenus si bien adaptés à la vie terrestre qu'ils ont passé leur vie adulte sur terre, bien qu'ils aient éclos dans l'eau et soient revenus pondre leurs œufs.
Les plantes ont développé des graines, ce qui a considérablement accéléré leur propagation sur terre, à cette époque (d'environ 360 Ma).
30 millions d'années supplémentaires (310 Ma) ont vu la divergence des synapsides (y compris les mammifères) des sauropsides (y compris les oiseaux et les reptiles).
L'événement d'extinction du Trias-Jurassique à 200 Ma a épargné de nombreux dinosaures, et ils sont rapidement devenus dominants parmi les vertébrés.
60% des invertébrés marins ont disparu et 25% de toutes les familles.
La troisième extinction de masse était le Permien-Trias, ou l'événement de la grande mort, qui a peut-être été causé par une combinaison de l'événement volcanique des pièges sibériens, un impact d'astéroïde, la gazéification de l'hydrate de méthane, les fluctuations du niveau de la mer et un événement anoxique majeur.
Ce fut de loin l'extinction la plus meurtrière de tous les temps, avec environ 57% de toutes les familles et 83% de tous les genres tués.
Au début du Paléocène, la terre s'est remise de l'extinction et la diversité des mammifères a augmenté.
La savane sans herbe a commencé à prédominer une grande partie du paysage, et des mammifères tels qu'Andrewsarchus se sont élevés pour devenir le plus grand mammifère prédateur terrestre connu de tous les temps, et les premières baleines comme Basilosaurus ont pris le contrôle des mers.
Des ongulés géants comme Paraceratherium et Deinotherium ont évolué pour régner sur les prairies.
La mer de Téthys a été fermée par la collision de l'Afrique et de l'Europe.
Le pont terrestre a permis aux créatures isolées d'Amérique du Sud de migrer vers l'Amérique du Nord, et vice versa.
Les périodes glaciaires ont conduit à l'évolution de l'homme moderne en Afrique saharienne et à son expansion.
Beaucoup pensent qu'une énorme migration a eu lieu le long de la Béringie, c'est pourquoi, aujourd'hui, il y a des chameaux (qui ont évolué et se sont éteints en Amérique du Nord), des chevaux (qui ont évolué et se sont éteints en Amérique du Nord) et des Amérindiens.
La taille du cerveau a augmenté rapidement et, en 2 Ma, les premiers animaux classés dans le genre Homo étaient apparus.
La capacité de contrôler le feu a probablement commencé chez Homo erectus (ou Homo ergaster), probablement il y a au moins 790 000 ans mais peut-être dès 1,5 Ma.
Il est plus difficile d'établir l'origine du langage ; on ne sait pas si Homo erectus pouvait parler ou si cette capacité n'avait pas commencé avant Homo sapiens.
Les compétences sociales sont devenues plus complexes, le langage est devenu plus sophistiqué et les outils sont devenus plus élaborés.
Les premiers humains à montrer des signes de spiritualité sont les Néandertaliens (généralement classés comme une espèce distincte sans descendants survivants) ; ils enterraient leurs morts, souvent sans aucun signe de nourriture ou d'outils.
Au fur et à mesure que le langage devenait plus complexe, la capacité de se souvenir et de communiquer des informations aboutissait, selon une théorie proposée par Richard Dawkins, à un nouveau réplicateur : le mème.
Entre 8500 et 7000 av. J.-C., les humains du Croissant Fertile au Moyen-Orient ont commencé l'élevage systématique des plantes et des animaux : l'agriculture.
Cependant, parmi les civilisations qui ont adopté l'agriculture, la stabilité relative et la productivité accrue fournies par l'agriculture ont permis à la population de se développer.
Cela a conduit à la première civilisation terrestre à Sumer au Moyen-Orient, entre 4000 et 3000 av.
L'homme n'a plus à passer tout son temps à travailler pour sa survie, permettant les premières professions spécialisées (par exemple artisans, marchands, prêtres, etc.).
Vers 500 avant JC, il y avait des civilisations avancées au Moyen-Orient, en Iran, en Inde, en Chine et en Grèce, parfois en expansion, parfois en déclin.
Cette civilisation s'est développée dans la guerre, les arts, les sciences, les mathématiques et l'architecture.
L'Empire romain a été christianisé par l'empereur Constantin au début du IVe siècle et a décliné à la fin du Ve.
La Maison de la Sagesse a été établie à Bagdad, à l'époque abbasside, en Irak.
Au 14ème siècle, la Renaissance a commencé en Italie avec les progrès de la religion, de l'art et de la science.
La civilisation européenne a commencé à changer à partir de 1500, conduisant aux révolutions scientifiques et industrielles.
De 1914 à 1918 et de 1939 à 1945, des nations du monde entier ont été entraînées dans des guerres mondiales.
Après la guerre, de nombreux nouveaux États se sont formés, déclarant ou obtenant leur indépendance en période de décolonisation.
Les développements technologiques comprennent les armes nucléaires, les ordinateurs, le génie génétique et la nanotechnologie.
Les préoccupations et les problèmes majeurs tels que la maladie, la guerre, la pauvreté, le radicalisme violent et, récemment, le changement climatique causé par l'homme ont augmenté à mesure que la population mondiale augmente.
L'histoire humaine, ou l'histoire enregistrée, est le récit du passé de l'humanité.
Le Néolithique a vu débuter la Révolution agricole, entre 10 000 et 5 000 av. J.-C., dans le Croissant Fertile du Proche-Orient.
Au fur et à mesure que l'agriculture se développait, l'agriculture céréalière devenait plus sophistiquée et incitait à une division du travail pour stocker la nourriture entre les saisons de croissance.
L'hindouisme s'est développé à la fin de l'âge du bronze sur le sous-continent indien.
L'histoire post-classique (le "Moyen Âge", vers 500-1500 CE) a été témoin de la montée du christianisme, de l'âge d'or islamique (vers 750 CE - vers 1258 CE) et des Renaissances timuride et italienne (d'environ 1300 de notre ère).
Au XVIIIe siècle, l'accumulation de connaissances et de technologies avait atteint une masse critique qui a provoqué la révolution industrielle et a commencé la période moderne tardive, qui a commencé vers 1800 et s'est poursuivie jusqu'à nos jours.
Les humains anatomiquement modernes sont apparus en Afrique il y a environ 300 000 ans et ont atteint la modernité comportementale il y a environ 50 000 ans.
Il y a peut-être déjà 1,8 million d'années, mais certainement il y a 500 000 ans, les humains ont commencé à utiliser le feu pour se chauffer et cuisiner.
Les humains paléolithiques vivaient comme des chasseurs-cueilleurs et étaient généralement nomades.
L'expansion rapide de l'humanité vers l'Amérique du Nord et l'Océanie a eu lieu à l'apogée de la dernière période glaciaire.
La vallée du fleuve Jaune en Chine cultivait du mil et d'autres cultures céréalières vers 7000 avant notre ère; la vallée du Yangtze a domestiqué le riz plus tôt, d'au moins 8000 avant notre ère.
Le travail des métaux a été utilisé pour la première fois dans la création d'outils et d'ornements en cuivre vers 6000 avant notre ère.
Les villes étaient des centres de commerce, de fabrication et de pouvoir politique.
Le développement des villes est synonyme d'essor de la civilisation.
Ces cultures ont diversement inventé le tour, les mathématiques, le travail du bronze, les bateaux à voile, le tour de potier, la toile tissée, la construction d'édifices monumentaux et l'écriture.
Typique du néolithique était une tendance à adorer des divinités anthropomorphes.
Ces colonies étaient concentrées dans des vallées fluviales fertiles : le Tigre et l'Euphrate en Mésopotamie, le Nil en Égypte, l'Indus dans le sous-continent indien et les fleuves Yangtze et Jaune en Chine.
L'écriture cunéiforme a commencé comme un système de pictogrammes, dont les représentations picturales sont finalement devenues simplifiées et plus abstraites.
Le transport était facilité par les voies navigables - par les rivières et les mers.
Ces développements ont conduit à la montée des États territoriaux et des empires.
En Crète, la civilisation minoenne était entrée dans l'âge du bronze vers 2700 avant notre ère et est considérée comme la première civilisation d'Europe.
Au cours des millénaires suivants, des civilisations se sont développées à travers le monde.
En Inde, cette ère était la période védique (1750-600 avant notre ère), qui a jeté les bases de l'hindouisme et d'autres aspects culturels de la société indienne primitive, et s'est terminée au 6ème siècle avant notre ère.
Au cours de la phase de formation en Méso-Amérique (environ 1500 avant notre ère à 500 de notre ère), des civilisations plus complexes et centralisées ont commencé à se développer, principalement dans ce qui est aujourd'hui le Mexique, l'Amérique centrale et le Pérou.
La théorie Axial-Age de Karl Jaspers inclut également le zoroastrisme persan, mais d'autres chercheurs contestent sa chronologie pour le zoroastrisme.)
Ce sont le taoïsme, le légalisme et le confucianisme.
Les grands empires dépendaient de l'annexion militaire de territoires et de la formation de colonies défendues pour devenir des centres agricoles.
Il y avait un certain nombre d'empires régionaux au cours de cette période.
L'Empire médian a cédé la place aux empires iraniens successifs, notamment l' Empire achéménide (550–330 avant notre ère), l' Empire parthe (247 avant notre ère – 224 CE) et l' Empire sassanide (224–651 CE).
Plus tard, Alexandre le Grand (356-323 avant notre ère), de Macédoine, fonda un empire de conquête, s'étendant de la Grèce actuelle à l'Inde actuelle.
À partir du IIIe siècle de notre ère, la dynastie Gupta a supervisé la période appelée l'âge d'or de l'Inde ancienne.
La stabilité qui en a résulté a contribué à annoncer l'âge d'or de la culture hindoue aux 4e et 5e siècles.
À l'époque d'Auguste (63 avant notre ère - 14 de notre ère), le premier empereur romain, Rome avait déjà établi sa domination sur la majeure partie de la Méditerranée.
L'empire occidental tomberait, en 476 CE, sous l'influence allemande sous Odoacre.
La dynastie Han était comparable en puissance et en influence à l'empire romain qui se trouvait à l'autre bout de la route de la soie.
Comme pour les autres empires de la période classique, la Chine Han a considérablement progressé dans les domaines du gouvernement, de l'éducation, des mathématiques, de l'astronomie, de la technologie et bien d'autres.
Des empires régionaux prospères ont également été établis dans les Amériques, issus de cultures établies dès 2500 avant notre ère.
Les grandes cités-États mayas ont lentement augmenté en nombre et en importance, et la culture maya s'est répandue dans tout le Yucatán et ses environs.
Il y a cependant eu, dans certaines régions, des périodes de progrès technologique rapide.
La dynastie Han de Chine est tombée dans la guerre civile en 220 de notre ère, au début de la période des Trois Royaumes, tandis que son homologue romain est devenu de plus en plus décentralisé et divisé à peu près au même moment dans ce que l'on appelle la crise du troisième siècle.
Le développement de l'étrier et l'élevage de chevaux assez forts pour porter un archer entièrement armé ont fait des nomades une menace constante pour les civilisations les plus sédentaires.
La partie restante de l'Empire romain, dans l'est de la Méditerranée, a continué comme ce qu'on a appelé l'Empire byzantin.
L'ère est généralement datée de la chute de l'Empire romain d'Occident au Ve siècle, qui s'est fragmenté en de nombreux royaumes séparés, dont certains seront plus tard confédérés sous le Saint Empire romain germanique.
L'Asie du Sud a vu une série de royaumes du milieu de l'Inde, suivis de l'établissement d'empires islamiques en Inde.
Cela a permis à l'Afrique de rejoindre le système commercial de l'Asie du Sud-Est, la mettant en contact avec l'Asie ; cela, avec la culture musulmane, a abouti à la culture swahili.
Ce fut aussi une bataille culturelle, la culture hellénistique et chrétienne byzantine rivalisant avec les traditions iraniennes persanes et la religion zoroastrienne.
Depuis leur centre de la péninsule arabique, les musulmans ont commencé leur expansion au début de l'ère postclassique.
Une grande partie de cet apprentissage et de ce développement peut être liée à la géographie.
L'influence des marchands musulmans sur les routes commerciales afro-arabes et arabo-asiatiques était énorme.
Motivés par la religion et les rêves de conquête, les dirigeants européens ont lancé un certain nombre de croisades pour tenter de faire reculer le pouvoir musulman et de reprendre la Terre Sainte.
La domination arabe de la région a pris fin au milieu du XIe siècle avec l'arrivée des Turcs seldjoukides, migrant vers le sud depuis les patries turques d'Asie centrale.
La région s'appellera plus tard la Côte de Barbarie et accueillera des pirates et des corsaires qui utiliseront plusieurs ports d'Afrique du Nord pour leurs raids contre les villes côtières de plusieurs pays européens à la recherche d'esclaves à vendre sur les marchés d'Afrique du Nord dans le cadre de la Barbarie esclavagiste. commerce.
Au 8ème siècle, l'islam a commencé à pénétrer la région et est rapidement devenu la seule religion de la plupart de la population, bien que le bouddhisme soit resté fort à l'est.
Après la mort de Gengis Khan en 1227, la majeure partie de l'Asie centrale a continué à être dominée par un État successeur, Chagatai Khanate.
La région a ensuite été divisée en une série de petits khanats créés par les Ouzbeks.
Les envahisseurs barbares ont formé leurs propres nouveaux royaumes dans les vestiges de l'Empire romain d'Occident.
Le christianisme s'est répandu en Europe occidentale et des monastères ont été fondés.
Le manorialisme, l'organisation des paysans en villages qui devaient des loyers et un service de travail aux nobles, et le féodalisme, une structure politique dans laquelle les chevaliers et les nobles de statut inférieur devaient le service militaire à leurs seigneurs en échange du droit aux loyers des terres et des manoirs, étaient deux des modes d'organisation de la société médiévale qui se sont développés au cours du Haut Moyen Âge.
Les marchands italiens importaient des esclaves pour travailler dans les ménages ou dans la transformation du sucre.
La famine, la peste et la guerre ont dévasté la population de l'Europe occidentale.
Ils ont finalement cédé la place à la dynastie Zagwe, célèbre pour son architecture taillée dans la roche à Lalibela.
Ils contrôlaient le commerce transsaharien de l'or, de l'ivoire, du sel et des esclaves.
L'Afrique centrale a vu naître plusieurs États, dont le Royaume du Kongo.
Ils ont construit de grandes structures défensives en pierre sans mortier comme le Grand Zimbabwe, capitale du Royaume du Zimbabwe, Khami, capitale du Royaume de Butua, et Danangombe (Dhlo-Dhlo), capitale de l'Empire Rozvi.
Le IXe siècle a vu une lutte tripartite pour le contrôle du nord de l'Inde, entre l'empire Pratihara, l'empire Pala et l'empire Rashtrakuta.
Cependant, la dynastie Tang finit par éclater et après un demi-siècle de troubles, la dynastie Song réunifia la Chine, alors qu'elle était, selon William McNeill, "le pays le plus riche, le plus qualifié et le plus peuplé du monde".
Après environ un siècle de règne de la dynastie mongole Yuan, les Chinois de souche ont réaffirmé le contrôle avec la fondation de la dynastie Ming (1368).
La période Nara du VIIIe siècle a marqué l'émergence d'un État japonais fort et est souvent décrite comme un âge d'or.
La période féodale de l'histoire japonaise, dominée par de puissants seigneurs régionaux (daimyos) et le régime militaire des seigneurs de guerre (shoguns) tels que le shogunat Ashikaga et le shogunat Tokugawa, s'étend de 1185 à 1868.
Silla a conquis Baekje en 660 et Goguryeo en 668, marquant le début de la période des États du Nord et du Sud ( 남북국시대 ), avec Silla unifiée au sud et Balhae, un État successeur de Goguryeo, au nord.
À partir du IXe siècle, le royaume de Bagan a pris de l'importance dans le Myanmar moderne.
Les Ancestral Puebloans et leurs prédécesseurs (IXe - XIIIe siècles) ont construit de vastes colonies permanentes, y compris des structures en pierre qui resteront les plus grands bâtiments d'Amérique du Nord jusqu'au XIXe siècle.
En Amérique du Sud, les XIVe et XVe siècles ont vu la montée des Incas.
La révolution scientifique a reçu une impulsion de l'introduction de Johannes Gutenberg en Europe de l'imprimerie, en utilisant des caractères mobiles, et de l'invention du télescope et du microscope.
La période moderne tardive se poursuit soit jusqu'à la fin de la Seconde Guerre mondiale, en 1945, soit jusqu'à nos jours.
Le début de la période moderne a été caractérisé par l'essor de la science et par des progrès technologiques de plus en plus rapides, une politique civique sécularisée et l'État-nation.
Au début de la période moderne, l'Europe a pu retrouver sa domination; les historiens débattent encore des causes.
Il avait développé une économie monétaire avancée en 1000 CE.
Elle jouissait d'un avantage technologique et avait le monopole de la production de fonte, des soufflets de piston, de la construction de ponts suspendus, de l'imprimerie et de la boussole.
Une théorie de l'essor de l'Europe soutient que la géographie de l'Europe a joué un rôle important dans son succès.
Cela a donné à l'Europe un certain degré de protection contre le péril des envahisseurs d'Asie centrale.
L'âge d'or de l'islam s'est terminé par le sac mongol de Bagdad en 1258.
La géographie a contribué à d'importantes différences géopolitiques.
En revanche, l'Europe était presque toujours divisée en plusieurs États belligérants.
Presque toutes les civilisations agricoles ont été fortement contraintes par leur environnement.
L'avancée technologique et la richesse générée par le commerce ont progressivement entraîné un élargissement des possibilités.
Sans surprise, l'expansion maritime de l'Europe, compte tenu de la géographie du continent, a été en grande partie l'œuvre de ses États atlantiques : le Portugal, l'Espagne, l'Angleterre, la France et les Pays-Bas.
En Afrique du Nord, le Sultanat Saadi est resté un État berbère indépendant jusqu'en 1659.
La côte swahili a décliné après être passée sous l'empire portugais et plus tard sous l'empire omanais.
Le royaume sud-africain du Zimbabwe a cédé la place à des royaumes plus petits tels que Mutapa, Butua et Rozvi.
D'autres civilisations en Afrique ont progressé au cours de cette période.
Le Japon a connu sa période Azuchi-Momoyama (1568-1603), suivie de la période Edo (1603-1868).
Le sultanat de Johor, centré sur la pointe sud de la péninsule malaise, est devenu la puissance commerciale dominante de la région.
La Russie a fait des incursions sur la côte nord-ouest de l'Amérique du Nord, avec une première colonie dans l'actuel Alaska en 1784 et l'avant-poste de Fort Ross dans l'actuelle Californie en 1812.
La révolution industrielle a commencé en Grande-Bretagne et a utilisé de nouveaux modes de production - l'usine, la production de masse et la mécanisation - pour fabriquer un large éventail de biens plus rapidement et en utilisant moins de main-d'œuvre qu'auparavant.
Après que les Européens aient acquis une influence et un contrôle sur les Amériques, les activités impériales se sont tournées vers les terres d'Asie et d'Océanie.
Les Britanniques ont également colonisé l'Australie, la Nouvelle-Zélande et l'Afrique du Sud avec un grand nombre de colons britanniques émigrant vers ces colonies.
En Europe, les défis économiques et militaires ont créé un système d'États-nations, et les groupements ethnolinguistiques ont commencé à s'identifier comme des nations distinctes aspirant à l'autonomie culturelle et politique.
Parallèlement, la pollution industrielle et la dabame environnementale, présentes depuis la découverte du feu et le début de la civilisation, se sont accélérées de façon drastique.
Une grande partie du reste du monde a été influencée par des nations fortement européanisées : les États-Unis et le Japon.
La Première Guerre mondiale a conduit à l'effondrement de quatre empires - l'Autriche-Hongrie, l'Empire allemand, l'Empire ottoman et l'Empire russe - et a affaibli le Royaume-Uni et la France.
Les rivalités nationales persistantes, exacerbées par les turbulences économiques de la Grande Dépression, ont contribué à précipiter la Seconde Guerre mondiale.
La guerre froide s'est terminée pacifiquement en 1991 après le pique-nique paneuropéen, la chute du rideau de fer et du mur de Berlin et l'effondrement du bloc de l'Est et du pacte de Varsovie.
Au cours des premières décennies d'après-guerre, les colonies d'Asie et d'Afrique des empires belge, britannique, néerlandais, français et d'autres empires d'Europe occidentale ont obtenu leur indépendance formelle.
L'efficacité de l'Union européenne a été handicapée par l'immaturité de ses institutions économiques et politiques communes, quelque peu comparable à l'insuffisance des institutions américaines en vertu des articles de la Confédération avant l'adoption de la Constitution des États-Unis entrée en vigueur en 1789.
Dans les décennies qui ont suivi la Seconde Guerre mondiale, ces avancées ont conduit aux voyages en avion à réaction, aux satellites artificiels avec d'innombrables applications, notamment le système de positionnement global (GPS) et Internet.
La concurrence mondiale pour les ressources naturelles a augmenté en raison de la croissance démographique et de l'industrialisation, en particulier en Inde, en Chine et au Brésil.
Une archive est une accumulation de documents historiques - sur n'importe quel support - ou l'installation physique dans laquelle ils se trouvent.
Ils ont été métaphoriquement définis comme "les sécrétions d'un organisme" et se distinguent des documents qui ont été consciemment écrits ou créés pour communiquer un message particulier à la postérité.
Cela signifie que les archives sont tout à fait distinctes des bibliothèques en ce qui concerne leurs fonctions et leur organisation, bien que les collections d'archives se trouvent souvent dans les bâtiments des bibliothèques.
Les archéologues ont découvert des archives de centaines (et parfois de milliers) de tablettes d'argile remontant aux troisième et deuxième millénaires avant JC dans des sites comme Ebla, Mari, Amarna, Hattusas, Ugarit et Pylos.
Cependant, ils ont été perdus, car les documents écrits sur des matériaux comme le papyrus et le papier se sont détériorés à un rythme plus rapide, contrairement à leurs homologues en pierre.
L'Angleterre après 1066 a développé des archives et des méthodes de recherche d'archives.
Bien qu'il existe de nombreux types d'archives, le dernier recensement des archivistes aux États-Unis en identifie cinq principaux : universitaires, commerciaux (à but lucratif), gouvernementaux, à but non lucratif et autres.
L'accès aux collections de ces archives se fait généralement uniquement sur rendez-vous ; certains ont affiché des heures pour faire des demandes de renseignements.
Parmi les exemples d'archives commerciales de premier plan aux États-Unis, citons Coca-Cola (qui possède également le musée séparé World of Coca-Cola), Procter and Gamble, Motorola Heritage Services and Archives et Levi Strauss & Co. Ces archives d'entreprise conservent des documents historiques et éléments liés à l'histoire et à l'administration de leurs entreprises.
Les travailleurs de ces types d'archives peuvent avoir n'importe quelle combinaison de formation et de diplômes, soit en histoire, soit en bibliothéconomie.
Aux États-Unis, la National Archives and Records Administration (NARA) maintient des installations d'archives centrales dans le district de Columbia et College Park, Maryland, avec des installations régionales réparties à travers les États-Unis.
Au Royaume-Uni, les Archives nationales (anciennement connues sous le nom de Public Record Office) sont les archives gouvernementales pour l'Angleterre et le Pays de Galles.
Au total, le volume total d'archives sous la tutelle de l'Archives française est le plus important au monde.
Les archidiocèses, les diocèses et les paroisses ont également des archives dans les Églises catholique romaine et anglicane.
Souvent, ces institutions s'appuient sur des subventions du gouvernement ainsi que sur des fonds privés.
De nombreux musées conservent des archives afin de prouver la provenance de leurs pièces.
Il s'agit d'un chiffre distinct des 1,3 % qui se sont identifiés comme travailleurs indépendants.
La mission des archives est de rassembler des histoires de femmes qui veulent s'exprimer et veulent que leurs histoires soient entendues.
Les archives d'une organisation (telle qu'une société ou un gouvernement) ont tendance à contenir d'autres types de documents, tels que des fichiers administratifs, des documents commerciaux, des notes de service, de la correspondance officielle et des procès-verbaux de réunion.
Beaucoup de ces dons n'ont pas encore été catalogués, mais sont actuellement en cours de conservation numérique et mis à la disposition du public en ligne.
Les partenaires internationaux pour les archives sont l'UNESCO et Blue Shield International conformément à la Convention de La Haye pour la protection des biens culturels de 1954 et son 2e Protocole de 1999.
Page, Morgan M. "Un des Vaults: Gossip, Access, and Trans History-Telling."
Un exemple de ceci est la description de Morgan M. Page de la diffusion de l'histoire des transgenres directement aux personnes trans via divers médias sociaux et plateformes de réseautage comme tumblr, Twitter et Instagram, ainsi que via des podcasts.
Avec les options disponibles grâce au contre-archivage, il est possible de "défier les conceptions traditionnelles de l'histoire" telles qu'elles sont perçues dans les archives contemporaines, ce qui crée un espace pour des récits qui ne sont souvent pas présents dans de nombreux documents d'archives.
Une biographie, ou simplement bio, est une description détaillée de la vie d'une personne.
Les œuvres biographiques ne sont généralement pas de la fiction, mais la fiction peut également être utilisée pour dépeindre la vie d'une personne.
Une autre collection bien connue de biographies anciennes est De vita Caesarum («Sur la vie des Césars») de Suétone , écrite vers 121 après JC à l'époque de l'empereur Hadrien .
Les ermites, les moines et les prêtres ont utilisé cette période historique pour écrire des biographies.
Un exemple séculaire significatif d'une biographie de cette période est la vie de Charlebamne par son courtisan Einhard.
Ils contenaient plus de données sociales pour une grande partie de la population que d'autres travaux de cette période.
À la fin du Moyen Âge, les biographies sont devenues moins axées sur l'Église en Europe à mesure que des biographies de rois, de chevaliers et de tyrans ont commencé à apparaître.
À la suite de Malory, le nouvel accent mis sur l'humanisme à la Renaissance a favorisé une concentration sur des sujets profanes, tels que les artistes et les poètes, et a encouragé l'écriture en langue vernaculaire.
Deux autres évolutions sont à noter : le développement de l'imprimerie au XVe siècle et l'augmentation progressive de l'alphabétisation.
Influencé dans l'élaboration des conceptions populaires des pirates, A General History of the Pyrates (1724), de Charles Johnson, est la principale source des biographies de nombreux pirates bien connus.
Carlyle a affirmé que la vie de grands êtres humains était essentielle pour comprendre la société et ses institutions.
Le travail de Boswell était unique dans son niveau de recherche, qui impliquait des études d'archives, des récits de témoins oculaires et des entretiens, son récit solide et attrayant et sa description honnête de tous les aspects de la vie et du caractère de Johnson - une formule qui sert de base à la biographie. littérature à ce jour.
Cependant, le nombre de biographies imprimées a connu une croissance rapide, grâce à un public de lecture en expansion.
Les périodiques ont commencé à publier une séquence de notices biographiques.
Les biographies sociologiques conçoivent les actions de leurs sujets comme le résultat de l'environnement et tendent à minimiser l'individualité.
Le concept conventionnel de héros et de récits de succès a disparu dans l'obsession des explorations psychologiques de la personnalité.
Jusque-là, comme le remarquait Strachey dans la préface, les biographies victoriennes étaient «aussi familières que le cortège du croque-mort» et portaient le même air de «barbarie lente et funèbre».
Le livre a acquis une renommée mondiale en raison de son style irrévérencieux et plein d'esprit, de sa nature concise et précise sur le plan factuel et de sa prose artistique.
Robert Graves (I, Claudius, 1934) se démarquait parmi ceux qui suivaient le modèle de Strachey de « démystification des biographies ».
À la Première Guerre mondiale, les réimpressions bon marché à couverture rigide étaient devenues populaires.
Parallèlement aux films biographiques documentaires, Hollywood a produit de nombreux films commerciaux basés sur la vie de personnes célèbres.
Contrairement aux livres et aux films, ils ne racontent souvent pas un récit chronologique : ce sont plutôt des archives de nombreux éléments médiatiques discrets liés à une personne individuelle, y compris des clips vidéo, des photographies et des articles de texte.
Les techniques générales d'« écriture de la vie » font l'objet d'études scientifiques.
Les informations peuvent provenir de "l'histoire orale, du récit personnel, de la biographie et de l'autobiographie" ou "des journaux intimes, des lettres, des notes de service et d'autres documents".
Les châteaux de style européen sont nés aux IXe et Xe siècles, après que la chute de l'Empire carolingien a entraîné la division de son territoire entre des seigneurs et des princes individuels.
Les châteaux urbains étaient utilisés pour contrôler la population locale et les voies de déplacement importantes, et les châteaux ruraux étaient souvent situés à proximité d'éléments qui faisaient partie intégrante de la vie de la communauté, tels que des moulins, des terres fertiles ou une source d'eau.
À la fin du XIIe et au début du XIIIe siècle, une approche scientifique de la défense du château a émergé.
Ces changements de défense ont été attribués à un mélange de technologie de château des croisades, comme la fortification concentrique, et à l'inspiration des défenses antérieures, comme les forts romains.
Bien que la poudre à canon ait été introduite en Europe au 14ème siècle, elle n'a pas affecté de manière significative la construction du château jusqu'au 15ème siècle, lorsque l'artillerie est devenue suffisamment puissante pour percer les murs de pierre.
La féodalité était le lien entre un seigneur et son vassal où, en échange du service militaire et de l'attente de loyauté, le seigneur accordait au vassal des terres.
Les châteaux servaient à diverses fins, dont les plus importantes étaient militaires, administratives et domestiques.
Alors que Guillaume le Conquérant avançait à travers l'Angleterre, il fortifia des positions clés pour sécuriser les terres qu'il avait prises.
Un château pouvait servir de forteresse et de prison, mais c'était aussi un lieu où un chevalier ou un seigneur pouvait divertir ses pairs.
Dans différentes régions du monde, des structures analogues partageaient des caractéristiques de fortification et d'autres caractéristiques déterminantes associées au concept de château, bien qu'elles soient originaires de périodes et de circonstances différentes et aient connu des évolutions et des influences différentes.
Au XVIe siècle, lorsque les cultures japonaise et européenne se sont rencontrées, la fortification en Europe avait dépassé les châteaux et s'appuyait sur des innovations telles que la trace italienne italienne et les forts en étoile.
L'excavation de la terre pour faire le monticule a laissé un fossé autour de la motte, appelé fossé (qui pouvait être humide ou sec).
C'était une caractéristique commune des châteaux, et la plupart en avaient au moins un.
L'eau était fournie par un puits ou une citerne.
Bien que souvent associés au type de château à motte et bailey, les baileys pouvaient également être trouvés en tant que structures défensives indépendantes.
Keep" n'était pas un terme utilisé à l'époque médiévale - le terme a été appliqué à partir du XVIe siècle - mais "donjon" était utilisé pour désigner les grandes tours, ou turris en latin.
Bien que souvent la partie la plus forte d'un château et un dernier refuge en cas de chute des défenses extérieures, le donjon n'était pas laissé vide en cas d'attaque mais servait de résidence au seigneur propriétaire du château, ou à ses hôtes ou représentants.
Les passerelles le long des hauts des murs-rideaux permettaient aux défenseurs de faire pleuvoir des missiles sur les ennemis en contrebas, et les remparts leur offraient une protection supplémentaire.
La façade de la porte était un angle mort et pour surmonter cela, des tours en saillie ont été ajoutées de chaque côté de la porte dans un style similaire à celui développé par les Romains.
Le passage à travers la guérite a été allongé pour augmenter le temps qu'un assaillant devait passer sous le feu dans un espace confiné et incapable de riposter.
Ils étaient très probablement utilisés pour larguer des objets sur les assaillants ou pour permettre de verser de l'eau sur les incendies pour les éteindre.
Une ouverture horizontale plus petite pourrait être ajoutée pour donner à un archer une meilleure vue pour viser.
Les premières fortifications sont originaires du Croissant fertile, de la vallée de l'Indus, de l'Égypte et de la Chine, où les colonies étaient protégées par de grands murs.
De nombreux terrassements survivent aujourd'hui, ainsi que des preuves de palissades pour accompagner les fossés.
Bien que primitifs, ils étaient souvent efficaces et n'ont été vaincus que par l'utilisation intensive d'engins de siège et d'autres techniques de guerre de siège, comme lors de la bataille d'Alésia.
Les discussions ont généralement attribué la montée du château à une réaction aux attaques des Magyars, des musulmans et des Vikings et à un besoin de défense privée.
Certaines fortes concentrations de châteaux se produisent dans des endroits sûrs, tandis que certaines régions frontalières avaient relativement peu de châteaux.
La construction de la halle en pierre ne la rendait pas nécessairement insensible au feu car elle possédait encore des fenêtres et une porte en bois.
Les châteaux n'étaient pas seulement des sites défensifs, ils renforçaient également le contrôle d'un seigneur sur ses terres.
En 864, le roi de Francie occidentale, Charles le Chauve, interdit la construction de castella sans son autorisation et ordonna leur destruction.
La Suisse est un cas extrême d'absence de contrôle de l'État sur les constructeurs de châteaux, et par conséquent, il y en avait 4 000 dans le pays.
En 950, la Provence abritait 12 châteaux, en 1000 ce chiffre était passé à 30, et en 1030 il était plus de 100.
Au début du XIe siècle, la motte et le donjon - un monticule artificiel surmonté d'une palissade et d'une tour - étaient la forme de château la plus répandue en Europe, partout sauf en Scandinavie.
Bien que la construction en pierre devienne plus tard courante ailleurs, à partir du XIe siècle, elle était le principal matériau de construction des châteaux chrétiens en Espagne, tandis que le bois était toujours le matériau de construction dominant dans le nord-ouest de l'Europe.
Avant le XIIe siècle, les châteaux étaient aussi rares au Danemark qu'ils l'étaient en Angleterre avant la conquête normande.
Leur décoration imitait l'architecture romane et incorporait parfois des doubles fenêtres semblables à celles que l'on trouve dans les clochers des églises.
Bien que supplantés par leurs successeurs en pierre, les châteaux de bois et de terrassement n'étaient nullement inutiles.
Jusqu'à la fin du XIIe siècle, les châteaux avaient généralement peu de tours; une passerelle avec peu d'éléments défensifs tels que des meurtrières ou une herse ; un grand donjon ou donjon, généralement carré et sans meurtrières ; et la forme aurait été dictée par la configuration du terrain (le résultat était souvent des structures irrégulières ou curvilignes).
Les tours auraient dépassé les murs et comportaient des meurtrières à chaque niveau pour permettre aux archers de cibler quiconque s'approchant ou se trouvant près du mur-rideau.
Là où existaient des donjons, ils n'étaient plus carrés mais polygonaux ou cylindriques.
Aménagées probablement au XIIe siècle, les tours assuraient le feu de flanc.
Il semblait que les croisés avaient beaucoup appris sur la fortification grâce à leurs conflits avec les Sarrasins et à leur exposition à l'architecture byzantine.
Les légendes ont été discréditées et dans le cas de Jacques de Saint-Georges, il a été prouvé qu'il venait de Saint-Georges-d'Espéranche, en France.
Les constructeurs de châteaux d'Europe occidentale étaient conscients et influencés par la conception romaine; les forts côtiers romains tardifs sur la " Saxon Shore " anglaise ont été réutilisés et en Espagne, le mur autour de la ville d' Ávila a imité l'architecture romaine lors de sa construction en 1091.
Un exemple de cette approche est Kerak.
Les châteaux qu'ils ont fondés pour sécuriser leurs acquisitions ont été conçus principalement par des maîtres-maçons syriens.
Alors que les châteaux étaient utilisés pour tenir un site et contrôler le mouvement des armées, en Terre Sainte, certaines positions stratégiques clés n'étaient pas fortifiées.
La conception variait non seulement entre les ordres, mais entre les châteaux individuels, bien qu'il soit courant pour ceux fondés à cette période d'avoir des défenses concentriques.
Si les assaillants franchissaient la première ligne de défense, ils seraient pris dans le champ de bataille entre les murs intérieur et extérieur et devraient attaquer le deuxième mur.
Par exemple, il était courant dans les châteaux croisés d'avoir la porte principale sur le côté d'une tour et qu'il y ait deux tours dans le passage, allongeant le temps qu'il fallait à quelqu'un pour atteindre l'enceinte extérieure.
Bien qu'il y ait eu des centaines de châteaux en bois en Prusse et en Livonie, l'utilisation de briques et de mortier était inconnue dans la région avant les croisés.
Les meurtrières n'ont pas compromis la résistance du mur, mais ce n'est qu'avec le programme de construction de châteaux d'Edouard Ier qu'elles ont été largement adoptées en Europe.
Bien que les mâchicoulis remplissaient le même objectif que les galeries en bois, ils étaient probablement une invention orientale plutôt qu'une évolution de la forme en bois.
Le conflit et l'interaction entre les deux groupes ont conduit à un échange d'idées architecturales et les chrétiens espagnols ont adopté l'utilisation de tours isolées.
L'historien français François Gebelin a écrit: "Le grand renouveau de l'architecture militaire a été mené, comme on pouvait s'y attendre naturellement, par les puissants rois et princes de l'époque; par les fils de Guillaume le Conquérant et leurs descendants, les Plantagenêts, lorsqu'ils sont devenus ducs de Normandie.
Les nouveaux châteaux étaient généralement d'une construction plus légère que les structures antérieures et présentaient peu d'innovations, même si des sites forts étaient encore créés comme celui de Raglan au Pays de Galles.
Ces armes étaient trop lourdes pour qu'un homme puisse les porter et tirer, mais s'il soutenait la crosse et posait le canon sur le bord du port du pistolet, il pouvait tirer avec l'arme.
Cette adaptation se retrouve dans toute l'Europe, et bien que le bois survive rarement, il en existe un exemple intact au château de Doornenburg aux Pays-Bas.
D'autres types de ports, bien que moins courants, étaient des fentes horizontales - permettant uniquement un mouvement latéral - et de grandes ouvertures carrées, qui permettaient un plus grand mouvement.
Ham est un exemple de la tendance des nouveaux châteaux à se passer des éléments antérieurs tels que les mâchicoulis, les hautes tours et les créneaux.
Dans un effort pour les rendre plus efficaces, les canons ont été de plus en plus gros, bien que cela ait entravé leur capacité à atteindre des châteaux éloignés.
Alors que cela suffisait pour les nouveaux châteaux, les structures préexistantes devaient trouver un moyen de faire face aux coups de canon.
Une solution à cela consistait à abattre le sommet d'une tour et à remplir la partie inférieure avec les gravats pour fournir une surface à partir de laquelle les canons pouvaient tirer.
De là ont évolué les forts en étoile, également connus sous le nom de trace italienne.
Le deuxième choix s'est avéré plus populaire car il est devenu évident qu'il était inutile d'essayer de rendre le site véritablement défendable face au canon.
De véritables châteaux ont été construits dans les Amériques par les colonies espagnoles et françaises.
Entre autres structures défensives (dont des forts et des citadelles), des châteaux ont également été construits en Nouvelle-France vers la fin du 17e siècle.
Le manoir et les écuries se trouvaient dans une cour fortifiée, avec une haute tourelle ronde à chaque coin.
Bien que la construction de châteaux se soit estompée vers la fin du XVIe siècle, les châteaux ne sont pas nécessairement tous tombés en désuétude.
Dans d'autres cas, ils avaient encore un rôle de défense.
Dans les conflits ultérieurs, tels que la guerre civile anglaise (1641-1651), de nombreux châteaux ont été refortifiés, bien que par la suite négligés pour les empêcher d'être à nouveau utilisés.
Les châteaux néo-gothiques sont devenus populaires en tant que manifestation d'un intérêt romantique pour le Moyen Âge et la chevalerie, et dans le cadre du renouveau gothique plus large en architecture.
En effet, être fidèle au design médiéval aurait laissé les maisons froides et sombres selon les normes contemporaines.
Les folies étaient similaires, même si elles différaient des ruines artificielles en ce qu'elles ne faisaient pas partie d'un paysage planifié, mais semblaient plutôt n'avoir aucune raison d'être construites.
Un château avec des remparts en terre, une motte, des défenses en bois et des bâtiments aurait pu être construit par une main-d'œuvre non qualifiée.
Le coût de construction d'un château variait en fonction de facteurs tels que sa complexité et les coûts de transport des matériaux.
Au milieu se trouvaient des châteaux comme Orford, qui a été construit à la fin du XIIe siècle pour 1 400 £ britanniques, et à l'extrémité supérieure se trouvaient ceux comme Douvres, qui a coûté environ 7 000 £ britanniques entre 1181 et 1191.
Le coût d'un grand château construit au cours de cette période (entre 1 000 et 10 000 £ britanniques) prendrait les revenus de plusieurs manoirs, affectant gravement les finances d'un seigneur.
Les machines et inventions médiévales, telles que la grue à roues, sont devenues indispensables lors de la construction, et les techniques de construction d'échafaudages en bois ont été améliorées dès l'Antiquité.
De nombreux pays possédaient à la fois des châteaux en bois et en pierre, mais le Danemark avait peu de carrières et, par conséquent, la plupart de ses châteaux sont en terre et en bois, ou plus tard construits en brique.
Par exemple, lorsque le château de Tattershall a été construit entre 1430 et 1450, il y avait beaucoup de pierre disponible à proximité, mais le propriétaire, Lord Cromwell, a choisi d'utiliser la brique.
Il comptait sur le soutien de ses subordonnés, car sans le soutien de ses locataires les plus puissants, un seigneur pouvait s'attendre à ce que son pouvoir soit sapé.
Cela s'appliquait particulièrement à la royauté, qui possédait parfois des terres dans différents pays.
Les ménages royaux prenaient essentiellement la même forme que les ménages baronniaux, bien qu'à une échelle beaucoup plus grande et que les postes soient plus prestigieux.
En tant que centres sociaux, les châteaux étaient des lieux d'exposition importants.
Les châteaux ont été comparés aux cathédrales en tant qu'objets de fierté architecturale, et certains châteaux ont incorporé des jardins comme éléments ornementaux.
L'amour courtois était l'érotisation de l'amour entre nobles.
La légende de Tristan et Iseult est un exemple d'histoires d'amour courtois racontées au Moyen Âge.
Le but du mariage entre les élites médiévales était de sécuriser la terre.
Cela découle de l'ibame du château en tant qu'institution martiale, mais la plupart des châteaux d'Angleterre, de France, d'Irlande et d'Écosse n'ont jamais été impliqués dans des conflits ou des sièges, de sorte que la vie domestique est une facette négligée.
Par exemple, de nombreux châteaux sont situés à proximité des voies romaines, qui sont restées d'importantes voies de transport au Moyen Âge, ou pourraient conduire à la modification ou à la création de nouveaux réseaux routiers dans la région.
Les châteaux urbains étaient particulièrement importants pour contrôler les centres de population et de production, en particulier avec une force d'invasion, par exemple au lendemain de la conquête normande de l'Angleterre au XIe siècle, la majorité des châteaux royaux ont été construits dans ou à proximité des villes.
Les châteaux ruraux étaient souvent associés aux moulins et aux systèmes de champs en raison de leur rôle dans la gestion du domaine du seigneur, ce qui leur donnait une plus grande influence sur les ressources.
Non seulement ils étaient pratiques en ce sens qu'ils assuraient un approvisionnement en eau et en poisson frais, mais ils étaient un symbole de statut car ils étaient coûteux à construire et à entretenir.
Les avantages de la construction de châteaux sur les colonies ne se limitaient pas à l'Europe.
Les implantations pouvaient également se développer naturellement autour d'un château, plutôt que d'être planifiées, en raison des avantages de la proximité d'un centre économique dans un paysage rural et de la sécurité apportée par les défenses.
Ils étaient généralement situés à proximité de toutes les défenses de la ville existantes, telles que les murs romains, bien que cela ait parfois entraîné la démolition de structures occupant le site souhaité.
Lorsque les Normands ont envahi l'Irlande, l'Écosse et le Pays de Galles aux XIe et XIIe siècles, la colonisation de ces pays était principalement non urbaine et la fondation de villes était souvent liée à la création d'un château.
Cela signifiait une relation étroite entre les seigneurs féodaux et l'Église, l'une des institutions les plus importantes de la société médiévale.
Un autre exemple est celui du château de Bodiam du XIVe siècle, également en Angleterre; bien qu'il semble être un château avancé et à la pointe de la technologie, il se trouve dans un site de peu d'importance stratégique, et les douves étaient peu profondes et plus susceptibles de faire paraître le site impressionnant que comme une défense contre l'exploitation minière.
Les garnisons étaient chères et par conséquent souvent petites à moins que le château ne soit important.
En 1403, une force de 37 archers a défendu avec succès le château de Caernarfon contre deux assauts des alliés d'Owain Glyndŵr lors d'un long siège, démontrant qu'une petite force pouvait être efficace.
Sous lui, il y aurait eu des chevaliers qui, grâce à leur formation militaire, auraient agi comme une sorte de classe d'officiers.
Il était plus efficace d'affamer la garnison que de l'assaillir, en particulier pour les sites les plus défendus.
Un long siège pourrait ralentir l'armée, permettant à l'aide de venir ou à l'ennemi de préparer une force plus importante pour plus tard.
S'ils étaient forcés d'attaquer un château, de nombreuses options s'offraient aux attaquants.
Le trébuchet, qui a probablement évolué à partir du petraria au XIIIe siècle, était l'arme de siège la plus efficace avant le développement des canons.
Les balistes ou springalds étaient des engins de siège qui fonctionnaient sur les mêmes principes que les arbalètes.
Ils étaient plus couramment utilisés contre la garnison que contre les bâtiments d'un château.
Une mine menant au mur serait creusée et une fois la cible atteinte, les supports en bois empêchant l'effondrement du tunnel seraient brûlés.
Une contre-mine pourrait être creusée vers le tunnel des assiégeants ; en supposant que les deux convergent, cela entraînerait un combat souterrain au corps à corps.
Ils ont été utilisés pour forcer l'ouverture des portes du château, bien qu'ils aient parfois été utilisés contre les murs avec moins d'effet.
Une option plus sûre pour ceux qui attaquaient un château était d'utiliser une tour de siège, parfois appelée beffroi.
Les domaines du royaume, ou trois domaines, étaient les grands ordres de hiérarchie sociale utilisés dans la chrétienté (l'Europe chrétienne) du Moyen Âge au début de l'Europe moderne.
La monarchie comprenait le roi et la reine, tandis que le système était composé du clergé (le Premier État), des nobles (le Second État), des paysans et de la bourgeoisie (le Tiers État).
En Angleterre, un système à deux domaines a évolué qui combinait la noblesse et le clergé en un seul domaine seigneurial avec les «communes» comme second domaine.
En Écosse, les trois états étaient le clergé (premier état), la noblesse (second état) et les commissaires de la Comté, ou « bourgeois » (tiers état), représentant la bourgeoisie, la classe moyenne et la classe inférieure.
Le clergé ne pouvant se marier, cette mobilité était théoriquement limitée à une génération.
Huizinga Le déclin du Moyen Âge (1919, 1924 : 47).
Les roturiers étaient universellement considérés comme l'ordre le plus bas.
Dans de nombreuses régions et royaumes, il existait également des groupes de population nés en dehors de ces domaines résidents spécifiquement définis.
La transformation économique et politique de la campagne au cours de la période a été remplie par une forte croissance de la population, de la production agricole, des innovations technologiques et des centres urbains; des mouvements de réforme et de renouveau ont tenté d'accentuer la distinction entre statut clérical et statut laïc, et le pouvoir, reconnu par l'Église, a également eu son effet.
Le deuxième ordre, ceux qui se battent, était le rang des politiquement puissants, ambitieux et dangereux.
De plus, les Premier et Second États s'appuyaient sur le travail du Tiers, ce qui rendait d'autant plus criant le statut inférieur de ce dernier.
La plupart sont nés au sein de ce groupe et sont également morts dans le cadre de celui-ci.
En mai 1776, le ministre des Finances Turgot est démis de ses fonctions, après avoir échoué à promulguer des réformes.
Ne parvenant pas à les persuader d'entériner son « programme idéal », Louis XVI cherche à dissoudre les États généraux, mais le Tiers-État tient pour leur droit à la représentation.
Parce que le Parlement d'Écosse était monocaméral, tous les membres siégeaient dans la même chambre, par opposition à la Chambre des lords et à la Chambre des communes anglaises séparées.
Comme en Angleterre, le Parlement d'Irlande est issu du "grand conseil" Magnum Concilium convoqué par le gouverneur en chef d'Irlande, en présence du conseil (curia regis), des bamnates (seigneurs féodaux) et des prélats (évêques et abbés).
En 1297, les comtés furent d'abord représentés par des chevaliers élus du comté (les shérifs les avaient auparavant représentés).
Chacun était un homme libre et avait des droits et des responsabilités spécifiques, ainsi que le droit d'envoyer des représentants au Riksdag des États.
Avant le 18e siècle, le roi avait le droit d'émettre un vote décisif si les domaines étaient répartis également.
Cependant, après la Diète de Porvoo, la Diète de Finlande ne fut reconvoquée qu'en 1863.
Vers 1400, des lettres patentes ont été introduites, en 1561 les rangs de comte et de baron ont été ajoutés et en 1625, la maison de la noblesse a été codifiée comme premier domaine de la terre.
Les chefs des maisons nobles étaient des membres héréditaires de l'assemblée des nobles.
Cela a entraîné une grande influence politique pour la haute noblesse.
Au cours des siècles suivants, le domaine comprenait des professeurs d'universités et de certaines écoles publiques.
Le commerce n'était autorisé dans les villes que lorsque l'idéologie mercantiliste avait pris le dessus, et les bourgeois avaient le droit exclusif de faire du commerce dans le cadre de corporations.
Pour qu'une colonie devienne une ville, une charte royale accordant un droit de marché était nécessaire, et le commerce extérieur exigeait des droits de port de base à charte royale.
Comme la plupart de la population était constituée de familles d'agriculteurs indépendants jusqu'au XIXe siècle, et non de serfs ni de vilains, il existe une différence remarquable de tradition par rapport aux autres pays européens.
Leurs représentants à la Diète étaient élus au suffrage indirect : chaque commune envoyait des électeurs pour élire le représentant d'une circonscription électorale.
Ils n'avaient aucun droit politique et ne pouvaient pas voter.
En Suède, le Riksdag des États existait jusqu'à ce qu'il soit remplacé par un Riksdag bicaméral en 1866, qui accordait des droits politiques à toute personne disposant d'un certain revenu ou d'une propriété.
En Finlande, cette division juridique a existé jusqu'en 1906, s'inspirant toujours de la constitution suédoise de 1772.
De plus, les ouvriers industriels vivant dans la ville n'étaient pas représentés par le système des quatre états.
Plus tard, aux XVe et XVIe siècles, Bruxelles devint le lieu où se réunissaient les États généraux.
À la suite de l'Union d'Utrecht en 1579 et des événements qui suivirent, les États généraux déclarèrent qu'ils n'obéissaient plus au roi Philippe II d'Espagne, qui était également suzerain des Pays-Bas.
C'était le niveau de gouvernement où toutes les choses étaient traitées qui préoccupaient les sept provinces qui sont devenues une partie de la République des Pays-Bas unis.
Aux Pays-Bas méridionaux, les dernières réunions des États généraux fidèles aux Habsbourg ont eu lieu dans les États généraux de 1600 et les États généraux de 1632.
Il n'est plus composé de représentants des États, encore moins des États : tous les hommes sont considérés comme égaux en vertu de la Constitution de 1798.
En 1815, lorsque les Pays-Bas furent réunis à la Belgique et au Luxembourg, les États généraux furent divisés en deux chambres : la Première Chambre et la Deuxième Chambre.
A partir de 1848, la Constitution néerlandaise prévoit que les membres de la Deuxième Chambre sont élus par le peuple (au départ uniquement par une partie limitée de la population masculine ; le suffrage universel masculin et féminin existe depuis 1919), tandis que les membres de la Première Chambre sont choisis par les membres des États provinciaux.
Le clergé était représenté par les princes-évêques, princes-archevêques et princes-abbés indépendants des nombreux monastères.
De nombreux peuples dont les territoires au sein du Saint Empire romain étaient indépendants depuis des siècles n'avaient pas de représentants à la Diète impériale, et cela comprenait les chevaliers impériaux et les villages indépendants.
Les quatre principaux domaines étaient: la noblesse (dvoryanstvo), le clergé, les habitants des campagnes et les citadins, avec une stratification plus détaillée.
La bourgeoisie dans son sens originel est intimement liée à l'existence des villes, reconnues comme telles par leurs chartes urbaines (par exemple, les chartes municipales, les privilèges municipaux, le droit municipal allemand), il n'y avait donc pas de bourgeoisie en dehors des citoyens des villes.
Historiquement, le mot français médiéval bourgeois désignait les habitants des bourgs (bourgs fortifiés), les artisans, artisans, commerçants et autres, qui constituaient « la bourgeoisie ».
Les guildes ont surgi lorsque des hommes d'affaires individuels (tels que des artisans, des artisans et des marchands) étaient en conflit avec leurs propriétaires féodaux à la recherche de rentes qui exigeaient des loyers plus élevés que ceux convenus précédemment.
Ils appartiennent généralement à une famille bourgeoise depuis trois générations ou plus.
Les noms de ces familles sont généralement connus dans la ville où elles résident, et leurs ancêtres ont souvent contribué à l'histoire de la région.
Ces gens vivent pourtant somptueusement, appréciant la compagnie des grands artistes de l'époque.
Dans la langue française, le terme de bourgeoisie désigne presque une caste à part entière, même si la mobilité sociale dans ce groupe socio-économique est possible.
Hitler se méfiait du capitalisme parce qu'il n'était pas fiable en raison de son égoïsme, et il préférait une économie dirigée par l'État qui est subordonnée aux intérêts du Volk.
Hitler a également déclaré que la bourgeoisie d'affaires "ne sait rien d'autre que son profit".
L'utilité de ces choses était inhérente à leurs fonctions pratiques.
Belle de Jour (1967) raconte l'histoire d'une femme bourgeoise qui s'ennuie de son mariage et décide de se prostituer.
En Europe, le titre d'Empereur est utilisé depuis le Moyen Âge, considéré à cette époque comme égal ou presque égal en dignité à celui de Pape en raison de la position de ce dernier en tant que chef visible de l'Église et chef spirituel de la partie catholique de l'Europe occidentale. .
Dans la mesure où il existe une définition stricte de l'empereur, c'est qu'un empereur n'a aucune relation impliquant la supériorité d'un autre dirigeant et règne généralement sur plus d'une nation.
Leur statut a été officiellement reconnu par l'empereur romain germanique en 1514, bien qu'il n'ait été officiellement utilisé par les monarques russes qu'en 1547.
Des titres pré-romains tels que Grand Roi ou Roi des Rois, utilisés par les rois de Perse et d'autres, sont souvent considérés comme l'équivalent.
L'Empire s'est plutôt identifié à de vastes possessions territoriales plutôt qu'au titre de son dirigeant au milieu du XVIIIe siècle.
Les anciens Romains abhorraient le nom Rex ("roi"), et il était essentiel pour l'ordre politique de maintenir les formes et les prétextes de la règle républicaine.
Auguste, considéré comme le premier empereur romain, a établi son hégémonie en collectant sur lui-même les fonctions, titres et honneurs de la Rome républicaine qui avaient traditionnellement été distribués à différents peuples, concentrant ce qui avait été distribué le pouvoir en un seul homme.
Cependant, c'est le descriptif informel d'Imperator ("commandant") qui est devenu le titre de plus en plus apprécié par ses successeurs.
C'est l'un des titres les plus durables : César et ses translittérations sont apparus chaque année depuis l'époque de César Auguste jusqu'à la destitution du tsar Siméon II de Bulgarie du trône en 1946.
Les exceptions incluent le titre de l' Histoire augustéenne , une collection semi-historique des biographies des empereurs des IIe et IIIe siècles.
Cependant, peu ont obtenu le titre, et ce n'était certainement pas une règle que toutes les épouses des empereurs régnants le recevraient.
À la fin de la République, comme dans les premières années de la nouvelle monarchie, Imperator était un titre accordé aux généraux romains par leurs troupes et le Sénat romain après une grande victoire, à peu près comparable au maréchal (chef ou commandant de toute l'armée).
La dynastie Nervan-Antonienne qui a suivi, régnant pendant la majeure partie du IIe siècle, a stabilisé l'Empire.
Trois tentatives sécessionnistes de courte durée ont eu leurs propres empereurs : l'Empire gaulois, l'Empire britannique et l'Empire palmyrène, bien que ce dernier ait utilisé le rex plus régulièrement.
À un moment donné, il y avait jusqu'à cinq partageurs de l'imperium (voir: Tétrarchie).
La ville est plus communément appelée Constantinople et porte aujourd'hui le nom d'Istanbul).
Ces derniers empereurs romains "byzantins" ont achevé la transition de l'idée de l'empereur en tant que fonctionnaire semi-républicain à l'empereur en tant que monarque absolu.
Les empereurs de la période byzantine utilisaient également le mot grec "autokrator", signifiant "celui qui se gouverne lui-même", ou "monarque", qui était traditionnellement utilisé par les écrivains grecs pour traduire le dictateur latin.
En fait, aucun de ces épithètes et titres supplémentaires (et d'autres) n'avait jamais été complètement abandonné.
Suite à la tragédie de l'horrible saccage de la ville, les conquérants ont déclaré un nouvel "Empire de Roumanie", connu des historiens sous le nom d'Empire latin de Constantinople, installant Baudouin IX, comte de Flandre, comme empereur.
Depuis l'époque d'Otton le Grand, une grande partie de l'ancien royaume carolingien de Francie orientale est devenu le Saint Empire romain germanique.
Ce roi cadet portait alors le titre de roi romain (roi des Romains).
Le Saint Empereur romain était considéré comme le premier parmi ceux au pouvoir.
La géographie est souvent définie en termes de deux branches : la géographie humaine et la géographie physique.
Traditionnellement, la géographie a été associée à la cartographie et aux noms de lieux.
Parce que l'espace et le lieu affectent une variété de sujets, tels que l'économie, la santé, le climat, les plantes et les animaux, la géographie est hautement interdisciplinaire.
Le premier se concentre en grande partie sur l'environnement bâti et sur la façon dont les humains créent, visualisent, gèrent et influencent l'espace.
Cela nécessite une compréhension des aspects traditionnels de la géographie physique et humaine, comme les façons dont les sociétés humaines conceptualisent l'environnement.
L'étude de systèmes plus grands que la Terre elle-même fait généralement partie de l'astronomie ou de la cosmologie.
Science régionale : Dans les années 1950, le mouvement scientifique régional dirigé par Walter Isard est né pour fournir une base plus quantitative et analytique aux questions géographiques, contrairement aux tendances descriptives des programmes de géographie traditionnels.
La cartographie est passée d'un ensemble de techniques de rédaction à une véritable science.
En plus de toutes les autres sous-disciplines de la géographie, les spécialistes SIG doivent comprendre l'informatique et les systèmes de bases de données.
La géostatistique est largement utilisée dans divers domaines, notamment l'hydrologie, la géologie, l'exploration pétrolière, l'analyse météorologique, l'urbanisme, la logistique et l'épidémiologie.
La carte telle que reconstruite par Eckhard Unger montre Babylone sur l'Euphrate, entourée d'une masse continentale circulaire montrant l'Assyrie, l'Urartu et plusieurs villes, à leur tour entourées d'un "fleuve amer" (Oceanus), avec sept îles disposées autour d'elle de manière à former une étoile à sept branches.
Contrairement à l' Ibamo Mundi , une carte du monde babylonienne antérieure datant du 9ème siècle avant JC décrivait Babylone comme étant plus au nord du centre du monde, bien que l'on ne sache pas ce que ce centre était censé représenter.
Thales est également crédité de la prédiction des éclipses.
Il y a un débat pour savoir qui a été la première personne à affirmer que la Terre est de forme sphérique, le crédit revenant soit à Parménide, soit à Pythagore.
L'une des premières estimations du rayon de la Terre a été faite par Eratosthène.
Les méridiens ont été subdivisés en 360°, chaque degré étant subdivisé en 60 (minutes).
Il a prolongé le travail d'Hipparque, en utilisant un système de grille sur ses cartes et en adoptant une longueur de 56,5 milles pour un degré.
Au Moyen Âge, la chute de l'empire romain a entraîné un changement dans l'évolution de la géographie de l'Europe vers le monde islamique.
De plus, les érudits islamiques ont traduit et interprété les œuvres antérieures des Romains et des Grecs et ont établi la Maison de la Sagesse à Bagdad à cette fin.
Abu Rayhan Biruni (976–1048) a décrit pour la première fois une projection polaire équiazimutale équidistante de la sphère céleste.
Il a également développé des techniques similaires lorsqu'il s'agissait de mesurer la hauteur des montagnes, la profondeur des vallées et l'étendue de l'horizon.
Le problème auquel étaient confrontés les explorateurs et les géographes était de trouver la latitude et la longitude d'un lieu géographique.
Les XVIIIe et XIXe siècles ont été l'époque où la géographie a été reconnue comme une discipline académique distincte et est devenue une partie d'un programme universitaire typique en Europe (en particulier à Paris et à Berlin).
Au cours des deux derniers siècles, les progrès de la technologie avec les ordinateurs ont mené au développement de la géomatique et à l'intégration de nouvelles pratiques telles que l'observation participante et la géostatistique dans le portefeuille d'outils de la géographie.
Arnold Henry Guyot (1807–1884) - a noté la structure des glaciers et une compréhension avancée du mouvement des glaciers, en particulier dans l'écoulement rapide des glaces.
William Morris Davis (1850–1934) - père de la géographie américaine et développeur du cycle de l'érosion.
Ellen Churchill Semple (1863–1932) - première femme présidente de l'Association of American Geographers.
Walter Christaller (1893–1969) - géographe humain et inventeur de la théorie des lieux centraux.
David Harvey (né en 1935) - Géographe marxiste et auteur de théories sur la géographie spatiale et urbaine, lauréat du prix Vautrin Lud.
Dans certains cas, une distinction est faite entre la capitale officielle (constitutionnelle) et le siège du gouvernement, qui se trouve à un autre endroit.
Les exemples sont l'ancienne Babylone, l'Abbasside Bagdad, l'ancienne Athènes, Rome, Bratislava, Budapest, Constantinople, Chang'an, l'ancienne Cusco, Kyiv, Madrid, Paris, Podgorica, Londres, Pékin, Prague, Tallinn, Tokyo, Lisbonne, Riga, Vilnius, et Varsovie.
Dans certains pays, la capitale a été modifiée pour des raisons géopolitiques ; La première ville de Finlande, Turku, qui avait servi de capitale du pays depuis le Moyen Âge sous la domination suédoise, a perdu ses droits sous le Grand-Duché de Finlande en 1812, lorsque Helsinki est devenue la capitale actuelle de la Finlande par l'Empire russe.
Au Canada, il y a une capitale fédérale, tandis que les dix provinces et les trois territoires ont chacun une capitale.
En Australie, le terme « capitales » est régulièrement utilisé pour désigner ces six capitales d'État, ainsi que la capitale fédérale Canberra et Darwin, la capitale du Territoire du Nord.
Contrairement aux fédérations, il n'y a généralement pas de capitale nationale distincte, mais la capitale d'une nation constituante sera également la capitale de l'État dans son ensemble, comme Londres, qui est la capitale de l'Angleterre et du Royaume-Uni.
Les capitales nationales de l'Allemagne et de la Russie (le Stadtstaat de Berlin et la ville fédérale de Moscou) sont également des États constitutifs des deux pays à part entière.
Frankfort, Kentucky, à mi-chemin entre Louisville et Lexington.
Tallahassee, Floride , choisie comme point médian entre Pensacola et St. Augustine, Floride - alors les deux plus grandes villes de Floride.
Les changements dans le régime politique d'une nation se traduisent parfois par la désignation d'une nouvelle capitale.
Lorsque les îles Canaries sont devenues une communauté autonome en 1982, Santa Cruz de Tenerife et Las Palmas de Gran Canaria ont toutes deux obtenu le statut de capitale.
Estonie : la Cour suprême et le ministère de l'Éducation et de la Recherche sont situés à Tartu.
En cas d'urgence, le siège des pouvoirs constitutionnels peut être transféré dans une autre ville, afin que les Chambres du Parlement siègent au même endroit que le Président et le Cabinet.
L'ensemble de l'appareil étatique se déplace d'une ville à l'autre tous les six mois.
Dharamshala, qui est également le siège de l'administration centrale tibétaine, est la deuxième capitale d'hiver de l'État.
La ville elle-même est administrée comme un territoire de l'Union.
Uttarakhand : Dehradun est la capitale administrative et législative, tandis que la haute cour est située à Nainital.
Sa construction a commencé en 1960 et s'est achevée en 1966.
Le palais présidentiel (palais Malacanang) et la Cour suprême sont situés dans la capitale, mais les deux chambres du Congrès sont situées dans des banlieues distinctes.
Sri Lanka : Sri Jayawardenepura Kotte est désignée capitale administrative et siège du parlement, tandis que l'ancienne capitale, Colombo, est désormais désignée comme «capitale commerciale».
Afrique du Sud : la capitale administrative est Pretoria, la capitale législative est Cape Town et la capitale judiciaire est Bloemfontein.
Suisse : Berne est la ville fédérale de la Suisse et fonctionne comme capitale de facto.
Également similaire à l'Illinois et à l'État de New York, la plupart des élus et officiers de l'État basés dans le sud-est de la Pennsylvanie (ville de Philadelphie, comté de Bucks, comté de Montgomery, comté de Delaware et comté de Chester) préfèrent travailler principalement à Philadelphie.
Israël et Palestine : Le gouvernement d'Israël et l'Autorité palestinienne revendiquent Jérusalem comme leur capitale.
Une relocalisation symbolique d'une capitale vers un emplacement géographiquement ou démographiquement périphérique peut être pour des raisons économiques ou stratégiques (parfois appelée capitale avancée ou capitale fer de lance).
Les empereurs Ming ont déplacé leur capitale à Pékin depuis Nanjing, plus central, pour aider à superviser la frontière avec les Mongols.
Delhi est finalement devenue la capitale coloniale après le couronnement Durbar du roi-empereur George V en 1911, continuant en tant que capitale indépendante de l'Inde à partir de 1947.
Parfois, l'emplacement d'une nouvelle capitale a été choisi pour mettre fin à des querelles réelles ou potentielles entre diverses entités, comme dans les cas de Canberra, Ottawa, Washington, Wellington et Managua.
Au cours de la période des Trois Royaumes, Shu et Wu sont tombés lorsque leurs capitales respectives, Chengdu et Jianye, sont tombées.
Après l'effondrement de la dynastie Qing, la décentralisation de l'autorité et l'amélioration des technologies de transport et de communication ont permis aux nationalistes chinois et aux communistes chinois de déplacer rapidement les capitales et de garder leurs structures de direction intactes pendant la grande crise de l'invasion japonaise.
Il peut être défini comme un lieu permanent et densément peuplé avec des limites définies administrativement dont les membres travaillent principalement à des tâches non agricoles.
Historiquement, les citadins représentaient une petite proportion de l'humanité dans son ensemble, mais après deux siècles d'urbanisation rapide et sans précédent, plus de la moitié de la population mondiale vit désormais dans des villes, ce qui a eu de profondes conséquences pour la durabilité mondiale.
Cette influence accrue signifie que les villes ont également une influence significative sur les problèmes mondiaux, tels que le développement durable, le réchauffement climatique et la santé mondiale.
Par conséquent, les villes compactes sont souvent considérées comme un élément crucial de la lutte contre le changement climatique.
Par exemple, les capitales de pays telles que Pékin, Londres, Mexico, Moscou, Nairobi, New Delhi, Paris, Rome, Athènes, Séoul, Tokyo et Washington, DC reflètent l'identité et le sommet de leurs nations respectives.
La ville peut être considérée comme une histoire, un schéma de relations entre groupes humains, un espace de production et de distribution, un champ de force physique, un ensemble de décisions liées ou une arène conflictuelle.
Les recensements nationaux utilisent une variété de définitions - invoquant des facteurs tels que la population, la densité de population, le nombre de logements, la fonction économique et l'infrastructure - pour classer les populations comme urbaines.
L'interdépendance mutuelle de la ville et de la campagne a une conséquence si évidente qu'elle passe facilement inaperçue : à l'échelle mondiale, les villes sont généralement confinées à des zones capables de faire vivre une population agricole permanente.
Au fur et à mesure que les villes devenaient plus complexes, les principales institutions civiques, des sièges du gouvernement aux édifices religieux, finiraient également par dominer ces points de convergence.
L'environnement physique limite généralement la forme sous laquelle une ville est construite.
Et il peut être mis en place pour une défense optimale compte tenu du paysage environnant.
Cette forme pourrait évoluer à partir d'une croissance successive sur une longue période, avec des traces concentriques de remparts et de citadelles marquant les anciennes limites de la ville.
Dans des villes comme Moscou, ce schéma est encore clairement visible.
Les fouilles dans ces zones ont trouvé les ruines de villes orientées diversement vers le commerce, la politique ou la religion.
Les villes planifiées de la Chine ont été construites selon des principes sacrés pour agir comme des microcosmes célestes.
Ces sites semblent planifiés de manière très réglementée et stratifiée, avec une grille minimaliste de chambres pour les ouvriers et des logements de plus en plus élaborés disponibles pour les classes supérieures.
Au cours des siècles suivants, des cités-États indépendantes de Grèce, en particulier Athènes, ont développé la polis , une association de citoyens propriétaires terriens masculins qui constituaient collectivement la ville.
Sous l'autorité de son empire, Rome a transformé et fondé de nombreuses villes (coloniae), et avec elles a apporté ses principes d'architecture urbaine, de design et de société.
La civilisation Norte Chico comprenait jusqu'à 30 grands centres de population dans ce qui est aujourd'hui la région Norte Chico du centre-nord du Pérou côtier.
Le centre du pouvoir en Occident s'est déplacé vers Constantinople et vers la civilisation islamique ascendante avec ses grandes villes Bagdad, Le Caire et Cordoue.
Aux XIIIe et XIVe siècles, certaines villes deviennent de puissants États, prenant sous leur contrôle les régions environnantes ou établissant de vastes empires maritimes.
Les grandes capitales d'Europe occidentale (Londres et Paris) ont bénéficié de la croissance du commerce suite à l'émergence d'un commerce atlantique.
L'Angleterre a ouvert la voie lorsque Londres est devenue la capitale d'un empire mondial et que des villes à travers le pays se sont développées dans des endroits stratégiques pour la fabrication.
Le leadership entrepreneurial s'est manifesté à travers des coalitions de croissance composées de constructeurs, d'agents immobiliers, de promoteurs, de médias, d'acteurs gouvernementaux tels que des maires et d'entreprises dominantes.
Les résultats ont été des efforts de revitalisation du centre-ville; gentrification du centre-ville ; la transformation du CBD en emplois de services avancés ; divertissements, musées et lieux culturels ; la construction de stades sportifs et de complexes sportifs ; et l'aménagement du secteur riverain.
Jusqu'au XVIIIe siècle, un équilibre existait entre la population agricole rurale et les villes dotées de marchés et de petites industries.
L'attractivité culturelle des villes joue également un rôle dans l'attractivité des habitants.
Batam, Indonésie, Mogadiscio, Somalie, Xiamen, Chine et Niamey, Niger, sont considérées parmi les villes à la croissance la plus rapide au monde, avec des taux de croissance annuels de 5 à 8 %.
L'ONU prévoit 2,5 milliards de citadins supplémentaires (et 300 millions de ruraux en moins) dans le monde d'ici 2050, 90 % de l'expansion de la population urbaine se produisant en Asie et en Afrique.
Un gouffre profond sépare les riches et les pauvres dans ces villes, qui contiennent généralement une élite super riche vivant dans des communautés fermées et de grandes masses de personnes vivant dans des logements insalubres avec des infrastructures inadéquates et des conditions autrement médiocres.
Pourtant, les municipalités adoptent régulièrement des arrêtés généraux visant des infractions ouvertes (et mal définies) telles que le vagabondage et l'obstruction, exigeant des permis pour manifester ou obligeant les résidents et les propriétaires à déneiger les trottoirs de la ville. »
Celles-ci sont fournies plus ou moins régulièrement, de façon plus ou moins égale.
Ces critères axés sur la production donnent souvent lieu à des "règles de prestation de services", des procédures régularisées de prestation de services, qui sont des tentatives de codification des objectifs de productivité des bureaucraties de services urbains.
"Robert L. Lineberry, "Mandating Urban Equality: The Distribution of Municipal Public Services"; dans Hahn & Levine (1980).
Cependant, le financement des services municipaux, ainsi que la rénovation urbaine et d'autres projets de développement, est un problème permanent, auquel les villes s'attaquent par des appels aux gouvernements supérieurs, des accords avec le secteur privé et des techniques telles que la privatisation (vente de services au secteur privé), la corporatisation (formation de sociétés quasi-privées détenues par les municipalités) et la financiarisation (conditionnement des actifs de la ville en instruments financiers et dérivés négociables).
L'impact de la mondialisation et le rôle des entreprises multinationales dans les gouvernements locaux du monde entier ont conduit à un changement de perspective sur la gouvernance urbaine, loin de la «théorie du régime urbain» dans laquelle une coalition d'intérêts locaux gouverne fonctionnellement, vers une théorie de l'économie extérieure. contrôle, largement associé dans les milieux universitaires à la philosophie du néolibéralisme.
Les outils de planification, au-delà de la conception originale de la ville elle-même, comprennent l'investissement en capital public dans les infrastructures et les contrôles de l'utilisation des sols tels que le zonage.
Les villes disposent également dans leur mise en œuvre des objectifs de planification des pouvoirs municipaux de zonage, de contrôle des lotissements et de réglementation des principes de construction, de logement et d'assainissement."
Les personnes vivant relativement près les unes des autres peuvent vivre, travailler et jouer dans des zones séparées et s'associer à des personnes différentes, formant des enclaves ethniques ou de style de vie ou, dans les zones de pauvreté concentrée, des ghettos.
Les banlieues de l'ouest et, de plus en plus, les communautés fermées et d'autres formes de "privatopia" dans le monde, permettent aux élites locales de s'isoler dans des quartiers sûrs et exclusifs.
Ce prolétariat paria - peut-être 1,5 milliard de personnes aujourd'hui, 2,5 milliards d'ici 2030 - est la classe sociale à la croissance la plus rapide et la plus nouvelle de la planète.
Il est ontologiquement à la fois similaire et différent de l'agentivité historique décrite dans le Manifeste communiste.
En tant que plaques tournantes du commerce, les villes abritent depuis longtemps le commerce de détail et la consommation via l'interface des achats.
Un marché du travail plus dense permet une meilleure adéquation des compétences entre les entreprises et les individus.
Les élites culturelles ont tendance à vivre dans les villes, liées par un capital culturel partagé et jouant elles-mêmes un certain rôle dans la gouvernance.
Greg Kerr et Jessica Oliver, "Repenser les identités des lieux", dans Kavaratzis, Warnaby et Ashworth (2015).
Les touristes patriotes visitent Agra pour voir le Taj Mahal, ou New York pour visiter le World Trade Center.
Pourquoi les anonymes - les pauvres, les défavorisés, les sans-connexion - préfèrent-ils souvent la vie dans des conditions misérables dans des immeubles à l'ordre sain et à la tranquillité des petites villes ou aux subdivisions sanitaires des développements semi-ruraux ?
Ceux qui sont venus y vivre l'ont fait afin de participer et de concourir à n'importe quel niveau possible.
Le sport joue également un rôle majeur dans l'image de marque de la ville et la formation de l'identité locale.
Plus important encore, il existe également un énorme potentiel à long terme pour le tourisme et l'investissement (Kasimati, 2003).
La guerre a amené la concentration de la direction sociale et du pouvoir politique entre les mains d'une minorité portant des armes, encouragée par un clergé exerçant des pouvoirs sacrés et possédant des connaissances scientifiques et fondamentales secrètes mais précieuses."
Pendant la Seconde Guerre mondiale, les gouvernements nationaux ont parfois déclaré certaines villes ouvertes, les livrant effectivement à un ennemi en progression afin d'éviter la dabame et l'effusion de sang.
Une telle guerre, connue sous le nom de contre-insurrection, implique des techniques de surveillance et de guerre psychologique ainsi que des combats rapprochés, prolonge fonctionnellement la prévention moderne de la criminalité urbaine, qui utilise déjà des concepts tels que l'espace défendable.
En raison des barrières à l'entrée plus élevées, ces réseaux ont été qualifiés de monopoles naturels, ce qui signifie que la logique économique privilégie le contrôle de chaque réseau par un seul organisme, public ou privé.
Kath Wellman & Frederik Pretorius, « Infrastructure urbaine : productivité, évaluation de projet et financement » ; dans Wellman & Spiller (2012).
L'assainissement, nécessaire à une bonne santé en milieu surpeuplé, nécessite l'approvisionnement en eau et la gestion des déchets ainsi que l'hygiène individuelle.
La vie urbaine moderne dépend fortement de l'énergie transmise par l'électricité pour le fonctionnement des machines électriques (des appareils électroménagers aux machines industrielles en passant par les systèmes électroniques désormais omniprésents utilisés dans les communications, les entreprises et le gouvernement) et pour les feux de circulation, les lampadaires et l'éclairage intérieur.
Tom Hart, "Les transports et la ville" ; dans Paddison (2001).
De nombreuses grandes villes américaines exploitent encore le transport en commun conventionnel par rail, comme en témoigne le très populaire système de métro de New York.
Les constructions et les déchets anthropiques, ainsi que la culture dans les jardins, créent des environnements physiques et chimiques qui n'ont pas d'équivalent en milieu sauvage, permettant dans certains cas une biodiversité exceptionnelle.
D'un certain point de vue, les villes ne sont pas écologiquement durables en raison de leurs besoins en ressources.
Les villes modernes sont connues pour créer leurs propres microclimats, en raison du béton, de l'asphalte et d'autres surfaces artificielles, qui chauffent au soleil et canalisent l'eau de pluie dans des conduits souterrains.
Les particules aériennes augmentent les précipitations de 5 à 10 %.
Par exemple, dans le microclimat urbain, les quartiers pauvres moins végétalisés supportent davantage la chaleur (mais ont moins de moyens d'y faire face).
On les appelle généralement espace vert urbain (bien que ce mot ne signifie pas toujours espace vert), espace vert, verdissement urbain.
L'étude a utilisé les données de près de 20 000 personnes au Royaume-Uni.
Les personnes qui n'ont pas obtenu au moins deux heures — même si elles ont dépassé une heure par semaine — n'ont pas obtenu les prestations.
L'étude n'a pas compté le temps passé dans la cour ou le jardin d'une personne comme du temps passé dans la nature, mais la majorité des visites dans la nature de l'étude ont eu lieu à moins de trois kilomètres de la maison. "
Saskia Sassen a utilisé le terme «ville mondiale» dans son ouvrage de 1991, The Global City: New York, London, Tokyo pour faire référence au pouvoir, au statut et au cosmopolitisme d'une ville, plutôt qu'à sa taille.
3 (1982): 319 Les villes mondiales forment la pierre angulaire de la hiérarchie mondiale, exerçant un commandement et un contrôle par leur influence économique et politique.
Les détracteurs de la notion soulignent les différents domaines du pouvoir et de l'échange.
Les sociétés multinationales et les banques ont leur siège social dans des villes mondiales et mènent une grande partie de leurs activités dans ce contexte.
Nancy Duxbury & Sharon Jeannotte, « Politique de gouvernance culturelle mondiale » ; Chapitre 21 dans The Ashgate Research Companion to Planning and Culture ; Londres : Ashgate, 2013.
La conférence Habitat I en 1976 a adopté la "Déclaration de Vancouver sur les établissements humains" qui identifie la gestion urbaine comme un aspect fondamental du développement et établit divers principes pour le maintien des habitats urbains.
En janvier 2002, la Commission des Nations Unies sur les établissements humains est devenue une agence faîtière appelée Programme des Nations Unies pour les établissements humains ou ONU-Habitat, membre du Groupe des Nations Unies pour le développement.
Les politiques de la Banque ont eu tendance à se concentrer sur le renforcement des marchés immobiliers par le crédit et l'assistance technique.
Les villes occupent une place prépondérante dans la culture occidentale traditionnelle, apparaissant dans la Bible sous des formes à la fois mauvaises et saintes, symbolisées par Babylone et Jérusalem.
Les villes peuvent être perçues en termes d'extrêmes ou d'opposés : à la fois libératrices et oppressives, riches et pauvres, organisées et chaotiques.
Cette idéologie politique et d'autres influencent fortement les récits et les thèmes du discours sur les villes.
La littérature classique et médiévale comprend un genre de descriptions qui traitent des caractéristiques et de l'histoire de la ville.
D'autres premières représentations cinématographiques des villes au XXe siècle les décrivaient généralement comme des espaces technologiquement efficaces avec des systèmes de transport automobile fonctionnant sans heurts.
Un pays est une entité territoriale ou une entité politique distincte (c'est-à-dire une nation).
Il n'est pas intrinsèquement souverain.
Le plus grand pays du monde par zone géographique est la Russie, tandis que le plus peuplé est la Chine, suivi de l'Inde, des États-Unis, de l'Indonésie, du Pakistan et du Brésil.
Dans de nombreux pays européens, les mots sont utilisés pour les subdivisions du territoire national, comme dans les Bundesländer allemands, ainsi qu'un terme moins formel pour un État souverain.
Il n'y a pas d'accord universel sur le nombre de "pays" dans le monde car un certain nombre d'États se sont disputés le statut de souveraineté.
Le degré d'autonomie des pays non souverains varie considérablement.
Le rapport classe le développement des pays en fonction du revenu national brut (RNB) par habitant.
Le rapport 2019 ne reconnaît que les pays développés d'Amérique du Nord, d'Europe et d'Asie et du Pacifique.
La Banque mondiale définit ses régions comme étant l'Asie de l'Est et le Pacifique, l'Europe et l'Asie centrale, l'Amérique latine et les Caraïbes, le Moyen-Orient et l'Afrique du Nord, l'Amérique du Nord, l'Asie du Sud et l'Afrique subsaharienne.
L'exploration est l'acte de rechercher dans le but de découvrir des informations ou des ressources, en particulier dans le contexte de la géographie ou de l'espace, plutôt que la recherche et le développement qui ne sont généralement pas centrés sur les sciences de la terre ou l'astronomie.
Seul celui réalisé par l'empereur Néron semblait être un préparatif à la conquête de l'Éthiopie ou de la Nubie : en 62 après JC, deux légionnaires explorèrent les sources du Nil.
Les Romains ont également organisé plusieurs explorations en Europe du Nord et ont exploré jusqu'en Chine en Asie.
100 AD-166 AD Début des relations romano-chinoises.
L'invention clé de leur exploration était la pirogue à balancier, qui offrait une plate-forme rapide et stable pour transporter des marchandises et des personnes.
Des études de 2011 à Wairau Bar en Nouvelle-Zélande montrent une forte probabilité qu'une origine soit l'île de Ruahine dans les îles de la Société.
Il existe des similitudes culturelles et linguistiques entre les habitants des îles Cook et les Maoris de Nouvelle-Zélande.
De 1328 à 1333, il navigua le long de la mer de Chine méridionale et visita de nombreux endroits en Asie du Sud-Est et atteignit l'Asie du Sud, atterrissant au Sri Lanka et en Inde, et il se rendit même en Australie.
Le Portugal et l'Espagne ont dominé les premières étapes de l'exploration, tandis que d'autres nations européennes ont suivi, comme l'Angleterre, les Pays-Bas et la France.
Les conditions extrêmes en haute mer nécessitent des méthodes et des technologies élaborées pour les supporter.
Une subdivision administrative, au contraire, est comprise comme une division d'un État proprement dit.
Les territoires dépendants qui subsistent actuellement dans le monde conservent aujourd'hui généralement un très haut degré d'autonomie politique.
Le statut des Îles Cook est considéré comme équivalent à l'indépendance aux fins du droit international, et le pays exerce une pleine souveraineté sur ses affaires intérieures et extérieures.
Aux termes de l'accord de libre association, cependant, la Nouvelle-Zélande conserve une certaine responsabilité pour les relations extérieures et la défense de Niue.
Cette liste est généralement limitée aux entités qui sont soit soumises à un traité international sur leur statut, soit inhabitées, soit jouissent d'un niveau d'autonomie unique et sont largement autonomes dans des domaines autres que les affaires internationales.
Ce sont des juridictions administrées de manière indépendante, bien que le gouvernement britannique soit seul responsable de la défense et de la représentation internationale et ait la responsabilité ultime d'assurer un bon gouvernement.
Aucune dépendance de la Couronne n'a de représentation au Parlement britannique.
La Nouvelle-Zélande et ses dépendances partagent le même gouverneur général et constituent un royaume monarchique.
Le Pacte mutuellement négocié pour établir un Commonwealth des îles Mariannes du Nord (CNMI) en union politique avec les États-Unis a été approuvé en 1976.
C'est une source constante d'ambiguïté et de confusion lorsqu'on essaie de définir, de comprendre et d'expliquer la relation politique de Porto Rico avec les États-Unis.
Cependant, le statut de ses "pays constitutifs" dans les Caraïbes (Aruba, Curaçao et Sint Maarten) peut être considéré comme apparenté à des dépendances ou à des "États non indépendants associés".
Les frontières sont des limites géographiques, imposées soit par des caractéristiques géographiques telles que les océans, soit par des groupements arbitraires d'entités politiques telles que des gouvernements, des États souverains, des États fédérés et d'autres entités infranationales.
La plupart des frontières extérieures sont partiellement ou entièrement contrôlées et ne peuvent être franchies légalement qu'aux points de contrôle frontaliers désignés et les zones frontalières peuvent être contrôlées.
La plupart des pays ont une certaine forme de contrôle aux frontières pour réglementer ou limiter les mouvements de personnes, d'animaux et de marchandises à l'intérieur et à l'extérieur du pays.
Pour séjourner ou travailler à l'intérieur des frontières d'un pays, les étrangers (personnes étrangères) peuvent avoir besoin de documents ou de permis d'immigration spéciaux ; mais la possession de tels documents ne garantit pas que la personne soit autorisée à franchir la frontière.
La plupart des pays interdisent le transport de drogues illégales ou d'animaux en voie de disparition à travers leurs frontières.
Dans les endroits où la contrebande, la migration et l'infiltration sont un problème, de nombreux pays fortifient les frontières avec des clôtures et des barrières et instituent des procédures formelles de contrôle aux frontières.
Ceci est courant dans les pays de l'espace Schengen européen et sur les sections rurales de la frontière canado-américaine.
Rivières : certaines frontières politiques ont été formalisées le long des frontières naturelles formées par les rivières.
Dans la Bible hébraïque, Moïse a défini le milieu du fleuve Arnon comme la frontière entre Moab et les tribus israélites s'installant à l'est du Jourdain.
Les exemples sont le lac Tanganyika, avec la République démocratique du Congo et la Zambie sur sa rive ouest et la Tanzanie et le Burundi à l'est ; et les Grands Lacs qui forment une partie importante de la frontière entre le Canada et les États-Unis.
Chaînes de montagnes : De nombreux pays ont leurs frontières politiques définies le long des chaînes de montagnes, souvent le long d'une ligne de partage des eaux.
Un exemple est la forêt défensive créée par la dynastie chinoise Song au XIe siècle.
Par exemple, la frontière entre l'Allemagne de l'Est et l'Allemagne de l'Ouest n'est plus une frontière internationale, mais elle est toujours visible en raison de repères historiques sur le paysage, et c'est toujours une division culturelle et économique en Allemagne.
Les frontières maritimes existent dans le contexte des eaux territoriales, des zones contiguës et des zones économiques exclusives ; cependant, la terminologie n'englobe pas les frontières des lacs ou des rivières, qui sont considérées dans le contexte des frontières terrestres.
L'espace aérien s'étend à 12 milles marins de la côte d'un pays et il est responsable de la protection de son propre espace aérien, à moins qu'il ne soit sous la protection de l'OTAN en temps de paix.
Cependant, il existe un accord général sur l'espace aérien vertical se terminant au point de la ligne Kármán.
Les réglementations frontalières générales sont imposées par les gouvernements nationaux et locaux et peuvent varier en fonction de la nation et des conditions politiques ou économiques actuelles.
Travailler au-delà des frontières – Exploiter le potentiel des activités transfrontalières pour améliorer la sécurité des moyens de subsistance dans les zones arides de la Corne de l'Afrique .
Le trafic économique humain à travers les frontières (en dehors des enlèvements) peut impliquer des déplacements massifs entre les lieux de travail et les établissements résidentiels.
Il peut permettre et arrêter les mouvements, à travers et le long des frontières.
De nombreuses régions transfrontalières encouragent également la communication et le dialogue interculturels ainsi que les stratégies de développement économique transfrontalier.
Depuis sa conception au milieu des années 1980, cette pratique artistique a contribué au développement des questions entourant la patrie, les frontières, la surveillance, l'identité, la race, l'ethnicité et l'origine(s) nationale(s).
Les frontières peuvent inclure, mais sans s'y limiter, la langue, la culture, la classe sociale et économique, la religion et l'identité nationale.
Ces artistes sont souvent eux-mêmes des « frontaliers ».
En général, une zone rurale ou une campagne est une zone géographique située en dehors des villes et des villes.
Les zones rurales typiques ont une faible densité de population et de petites agglomérations.
Les régions à prédominance urbaine ont moins de 15 % de leur population vivant dans une communauté rurale.
Les régions rurales du Nord sont des divisions de recensement à prédominance rurale qui se trouvent entièrement ou principalement au-dessus des parallèles suivants dans chaque province : Terre-Neuve-et-Labrador, 50e; Québec 54e; Ontario, 54e ; Manitoba, 53e; Saskatchewan, Alberta et Colombie-Britannique, 54e.
Le US Census Bureau, le Service de recherche économique de l'USDA et le Bureau de la gestion et du budget (OMB) se sont réunis pour aider à définir les zones rurales.
La loi agricole de 2002 (PL 107-171, Sec.
Selon le manuel, Definitions of Rural: A Handbook for Health Policy Makers and Researchers, "Les résidents des comtés métropolitains sont généralement considérés comme ayant un accès facile aux services de santé relativement concentrés des zones centrales du comté.
Cela est devenu la définition Goldsmith Modification de rural. "
Le gouvernement du président Emmanuel Macron a lancé en 2019 un plan d'action en faveur des zones rurales nommé "Agenda Rural".
En Écosse, une définition différente du rural est utilisée.
RBI définit les zones rurales comme les zones de moins de 49 000 habitants (villes de niveau -3 à niveau 6).
Les zones rurales du Pakistan proches des villes sont considérées comme des zones suburbaines ou des banlieues.
Les banlieues peuvent avoir leur propre juridiction politique ou juridique, en particulier aux États-Unis, mais ce n'est pas toujours le cas, en particulier au Royaume-Uni où la plupart des banlieues sont situées dans les limites administratives des villes.
Dans d'autres, comme le Maroc, la France et une grande partie des États-Unis, de nombreuses banlieues restent des municipalités distinctes ou sont gouvernées localement dans le cadre d'une zone métropolitaine plus vaste telle qu'un comté, un district ou un arrondissement.
Les termes banlieue intérieure et banlieue extérieure sont utilisés pour différencier les zones à forte densité à proximité du centre-ville (qui ne seraient pas appelées «banlieues» dans la plupart des autres pays) et les banlieues à faible densité à la périphérie de la zone urbaine.
En Nouvelle-Zélande, la plupart des banlieues ne sont pas légalement définies, ce qui peut prêter à confusion quant à l'endroit où elles peuvent commencer et se terminer.
Le mot suburbani a été employé pour la première fois par l'homme d'État romain Cicéron en référence aux grandes villas et propriétés construites par les riches patriciens de Rome à la périphérie de la ville.
Au milieu du XIXe siècle, les premières grandes zones suburbaines surgissaient autour de Londres alors que la ville (alors la plus grande du monde) devenait de plus en plus surpeuplée et insalubre.
La ligne atteint Harrow en 1880.
Le service marketing du Met a inventé le terme «Metro-land» en 1915 lorsque le Guide de la ligne d'extension est devenu le guide Metro-land, au prix de 1d.
C'était en partie une réponse au manque choquant de condition physique de nombreuses recrues pendant la Première Guerre mondiale, attribué aux mauvaises conditions de vie; une croyance résumée dans une affiche de logement de l'époque "vous ne pouvez pas vous attendre à faire sortir une population A1 des maisons C3" - faisant référence aux classifications d'aptitude militaire de l'époque.
Le rapport a également légiféré sur les normes minimales requises pour la poursuite de la construction suburbaine; cela comprenait une réglementation sur la densité maximale des logements et leur disposition et il faisait même des recommandations sur le nombre idéal de chambres et d'autres pièces par maison.
En l'espace d'une décennie seulement, la taille des banlieues s'est considérablement accrue.
Levittown s'est développé comme un prototype majeur de logements produits en série.
L'achat de différents biens et services dans un emplacement central sans avoir à se déplacer dans plusieurs endroits a contribué à maintenir les centres commerciaux dans ces banlieues nouvellement conçues qui connaissaient un boom démographique.
Le Highway Act de 1956 a aidé à financer la construction de 64 000 kilomètres à travers le pays en disposant de 26 milliards de dollars à utiliser, ce qui a permis de relier facilement beaucoup plus à ces centres commerciaux.
Certaines banlieues s'étaient développées autour des grandes villes où il y avait un transport ferroviaire vers les emplois du centre-ville.
Le produit a été un grand boom immobilier.
Avec 16 millions d'anciens combattants éligibles, l'opportunité d'acheter une maison était soudainement à portée de main.
Les promoteurs ont acheté des terrains vides juste à l'extérieur de la ville, installé des maisons sur une poignée de modèles et fourni des rues et des services publics, ou les autorités publiques locales se sont précipitées pour construire des écoles.
Les anciens combattants pourraient en obtenir un avec une mise de fonds beaucoup plus faible.
La croissance des banlieues a été facilitée par le développement des lois de zonage, la redlining et de nombreuses innovations dans les transports.
Les Afro-Américains et les autres personnes de couleur sont restés largement concentrés dans les noyaux en décomposition de la pauvreté urbaine.
Après la Seconde Guerre mondiale, la disponibilité des prêts FHA a stimulé un boom immobilier dans les banlieues américaines.
La croissance économique aux États-Unis a encouragé la suburbanisation des villes américaines qui a nécessité des investissements massifs pour les nouvelles infrastructures et les maisons.
Une stratégie alternative est la conception délibérée de "villes nouvelles" et la protection de ceintures vertes autour des villes.
Les subventions fédérales pour le développement des banlieues ont accéléré ce processus, tout comme la pratique de redlining par les banques et autres établissements de crédit.
Virginia Beach est maintenant la plus grande ville de toute la Virginie, ayant depuis longtemps dépassé la population de sa ville principale voisine, Norfolk.
Un plus grand pourcentage de blancs (à la fois non hispaniques et, dans certaines régions, hispaniques) et un pourcentage moindre de citoyens d'autres groupes ethniques que dans les zones urbaines.
Par rapport aux zones rurales, les banlieues ont généralement une plus grande densité de population, un niveau de vie plus élevé, des réseaux routiers plus complexes, plus de magasins et de restaurants franchisés, et moins de terres agricoles et d'animaux sauvages.
Cependant, de cette population métropolitaine, en 2001, près de la moitié vivait dans des quartiers à faible densité, avec seulement un sur cinq vivant dans un quartier «urbain» typique.
Partout au Canada, des plans complets sont en place pour freiner l'étalement.
La majorité de la croissance démographique récente dans les trois plus grandes régions métropolitaines du Canada (Grand Toronto, Grand Montréal et Grand Vancouver) s'est produite dans des municipalités non centrales.
Cela est dû à l'annexion et à la grande empreinte géographique à l'intérieur des frontières de la ville.
Lors du recensement de 2016, la ville de Calgary comptait 1 239 220 habitants, tandis que la région métropolitaine de Calgary comptait 1 392 609 habitants, ce qui indique que la grande majorité des habitants de la RMR de Calgary vivaient dans les limites de la ville.
Au Royaume-Uni, le gouvernement cherche à imposer des densités minimales sur les programmes de logement nouvellement approuvés dans certaines parties du sud-est de l'Angleterre.
Les banlieues se trouvent à Guadalajara, Mexico, Monterrey et la plupart des grandes villes.
À mesure que la croissance des banlieues de la classe moyenne et de la classe supérieure augmentait, les zones de squatters de classe inférieure se sont multipliées, notamment les «villes perdues» au Mexique, les campamentos au Chili, les barriadas au Pérou, les villa miserias en Argentine, les asentamientos au Guatemala et les favelas de Brésil.
Dans un cas illustratif de l'Afrique du Sud, des logements RDP ont été construits.
Dans certaines zones telles que Klang, Subang Jaya et Petaling Jaya, les banlieues forment le cœur de ces lieux.
Dans le système suburbain, la plupart des déplacements d'une composante à une autre composante nécessitent que les voitures empruntent une route collectrice, quelle que soit la distance.
Si un accident de la circulation se produit sur une route collectrice, ou si la construction de la route inhibe la circulation, l'ensemble du système routier peut être rendu inutilisable jusqu'à ce que le blocage soit dégagé.
Cela encourage les déplacements en voiture même sur des distances aussi faibles que plusieurs centaines de mètres ou de mètres (qui peuvent atteindre plusieurs miles ou kilomètres en raison du réseau routier).
Pris ensemble, ces deux groupes de contribuables représentent une source largement inexploitée de revenus potentiels que les villes peuvent commencer à cibler plus agressivement, en particulier si elles sont en difficulté.
Des chansons françaises comme La Zone de Fréhel (1933), Aux quatre coins de la banlieue de Damia (1936), Ma banlieue de Reda Caire (1937) ou Banlieue de Robert Lamoureux (1953), évoquent explicitement la banlieue parisienne depuis les années 1930 .
Le cinéma français s'intéresse cependant très tôt aux mutations urbaines des banlieues, avec des films comme Mon oncle de Jacques Tati (1958), L'Amour existe de Maurice Pialat (1961) ou Deux ou trois choses que je sais d'elle de Jean-Luc Godard (1967).
La chanson de 1962 " Little Boxes " de Malvina Reynolds dénonce le développement de la banlieue et ses valeurs bourgeoises et conformistes perçues, tandis que la chanson de 1982 Subdivisions du groupe canadien Rush discute également de la banlieue, tout comme Rockin 'the Suburbs de Ben Folds.
Over the Hedge est une bande dessinée syndiquée écrite et dessinée par Michael Fry et T. Lewis.
Des séries télévisées britanniques telles que The Good Life, Butterflies et The Fall and Rise of Reginald Perrin ont dépeint la banlieue comme bien entretenue mais implacablement ennuyeuse, et ses habitants comme trop conformistes ou enclins à devenir fous.
Un village est un établissement humain ou une communauté regroupée, plus grand qu'un hameau mais plus petit qu'une ville (bien que le mot soit souvent utilisé pour décrire à la fois des hameaux et des petites villes), avec une population allant généralement de quelques centaines à quelques milliers.
Cela a également permis la spécialisation de la main-d'œuvre et de l'artisanat, et le développement de nombreux métiers.
La taille de ces villages varie considérablement.
Desa sont généralement situés dans les zones rurales tandis que les kelurahan sont généralement des subdivisions urbaines.
Un desa ou kelurahan est la subdivision d'un kecamatan (sous-district), à son tour la subdivision d'un kabupaten (district) ou kota (ville).
En Malaisie, un kampung est déterminé comme une localité de 10 000 habitants ou moins.
Tous les musulmans du village malais ou indonésien veulent qu'on prie pour eux et qu'ils reçoivent les bénédictions d'Allah dans l'au-delà.
La partie continentale de Singapour avait autrefois de nombreux villages de kampung, mais les développements modernes et les travaux d'urbanisation rapides les ont vus rasés au bulldozer ; Kampong Lorong Buangkok est le dernier village survivant de la partie continentale du pays.
Le village vietnamien est le symbole typique de la production agricole asiatique.
En Slovénie, le mot selo est utilisé pour les très petits villages (moins de 100 personnes) et dans les dialectes ; le mot slovène vas est utilisé dans toute la Slovénie.
Il pourrait être relatif à un sanskrit comme le mot afghan deh et le mot indonésien desa.
Environ 46% de toutes les personnes émigrées ont changé leur résidence d'une ville à l'autre.
L'unité administrative la plus basse de l'Empire russe, un volost, ou son successeur soviétique ou russe moderne, un selsoviet, avait généralement son siège dans un selo et englobait quelques villages voisins.
Alors que les paysans du centre de la Russie vivaient dans un village autour du manoir du seigneur, une famille cosaque vivait souvent dans sa propre ferme, appelée khutor.
Il existe cependant un autre type de colonie plus petit qui est désigné en ukrainien sous le nom de selysche ( селище ).
Ils représentent un type de petite localité rurale qui aurait pu être autrefois un khutir, une colonie de pêcheurs ou une datcha.
Cependant, l'ambiguïté est souvent évitée en ce qui concerne les établissements urbanisés en se référant à eux en utilisant l'abréviation à trois lettres smt à la place.
Ils sont devenus très populaires lors de la réforme Stolypine au début du XXe siècle.
Les plus grands villages peuvent également être appelés Flecken ou Markt selon la région.
Par exemple, dans des régions telles que les Lincolnshire Wolds , les villages se trouvent souvent le long de la ligne de source à mi-chemin des collines et sont à l'origine des colonies de ligne de source, avec les systèmes originaux de champs ouverts autour du village.
Certains villages ont disparu (par exemple, des villages médiévaux désertés), laissant parfois derrière eux une église ou un manoir et parfois que des bosses dans les champs.
D'autres villages ont grandi et fusionné et forment souvent des centres au sein de la masse générale de la banlieue, comme Hampstead, Londres et Didsbury à Manchester.
Vu comme étant loin de l'agitation de la vie moderne, il est représenté comme calme et harmonieux, bien qu'un peu introverti.
Ceux-ci (comme Murton, comté de Durham) sont issus de hameaux lorsque le naufrage d'une mine au début du XXe siècle a entraîné une croissance rapide de leur population et les propriétaires de la mine ont construit de nouveaux logements, magasins, pubs et églises.
Maltby a été construit sous les auspices de la Sheepbridge Coal and Iron Company et comprenait de vastes espaces ouverts et des jardins.
Le village typique avait un pub ou une auberge, des boutiques et un forgeron.
Cependant, certaines paroisses civiles n'ont pas de paroisse, de ville ou de conseil municipal fonctionnel ni d'assemblée paroissiale fonctionnelle.
En Écosse, l'équivalent est également un conseil communautaire, cependant, bien qu'il s'agisse d'organes statutaires, ils n'ont aucun pouvoir exécutif.
Le district de Danniyeh se compose de trente-six petits villages, dont Almrah, Kfirchlan, Kfirhbab, Hakel al Azimah, Siir, Bakhoun, Miryata, Assoun, Sfiiri, Kharnoub, Katteen, Kfirhabou, Zghartegrein, Ein Qibil.
Dinniyeh possède un excellent environnement écologique rempli de forêts, de vergers et de bosquets.
Les villages du sud de la Syrie (Hauran, Jabal al-Druze), du nord-est (l'île syrienne) et du bassin de l'Oronte dépendent principalement de l'agriculture, principalement des céréales, des légumes et des fruits.
Les villes méditerranéennes de Syrie, telles que Tartous et Lattaquié, ont des types de villages similaires.
Toute urbanisation est un "pueblo" à moins qu'elle ne soit élevée par décret à la catégorie supérieure.
Cependant, c'est une généralité; dans de nombreux États, il existe des villages qui sont d'un ordre de grandeur plus grands que les plus petites villes de l'État.
Dans certains cas, le village peut coïncider avec la ville ou le canton, auquel cas les deux peuvent avoir un gouvernement consolidé.
Hempstead, le plus grand village, compte 55 000 habitants ; ce qui le rend plus peuplé que certaines villes de l'État.
Le village d'Arlington Heights, dans l'Illinois, comptait 75 101 habitants au recensement de 2010.
Les villages peuvent incorporer des terres dans plusieurs cantons et même plusieurs comtés.
Le plus grand village est Menomonee Falls, qui compte plus de 32 000 habitants.
Dans le Maryland, une localité désignée "Village de ..." peut être soit une ville incorporée, soit un district fiscal spécial.
A cette époque, les dirigeants traditionnels avaient le pouvoir absolu dans leurs régions administratives.
Chaque village haoussa était dirigé par Magaji (chef de village) qui était responsable devant son Hakimi (maire) au niveau de la ville.
Ils ont des maisons en terre avec des toits de chaume mais, comme dans la plupart des villages du Nord, les toits en zinc deviennent monnaie courante.
D'autres ont la chance d'avoir des puits à distance de marche.
Un atlas est une collection de cartes ; il s'agit généralement d'un ensemble de cartes de la Terre ou d'une région de la Terre.
Ce titre fournit la définition de Mercator du mot comme une description de la création et de la forme de l'univers entier, pas simplement comme une collection de cartes.
Un atlas de bureau est semblable à un livre de référence.
En cartographie, une ligne de contour (souvent simplement appelée "contour") relie des points d'élévation (hauteur) égale au-dessus d'un niveau donné, tel que le niveau moyen de la mer.
Le gradient de la fonction est toujours perpendiculaire aux courbes de niveau.
Les lignes de contour sont courbes, droites ou un mélange des deux lignes sur une carte décrivant l'intersection d'une surface réelle ou hypothétique avec un ou plusieurs plans horizontaux.
En 1701, Edmond Halley a utilisé de telles lignes (isogones) sur une carte de variation bamnétique.
En 1791, une carte de France de JL Dupain-Triel utilisait des courbes de niveau tous les 20 mètres, des hachures, des hauteurs ponctuelles et une coupe verticale.
Les isobathes n'étaient pas systématiquement utilisées sur les cartes marines jusqu'à celles de la Russie à partir de 1834 et celles de la Grande-Bretagne à partir de 1838.
Jusqu'en 1944, John K. Wright préférait encore l'isogramme, mais il n'a jamais été largement utilisé.
Malgré les tentatives de sélection d'une norme unique, toutes ces alternatives ont survécu jusqu'à présent.
Les stations météorologiques sont rarement positionnées exactement à une courbe de niveau (lorsqu'elles le sont, cela indique une mesure précisément égale à la valeur de la courbe de niveau).
En météorologie, les pressions barométriques indiquées sont réduites au niveau de la mer, et non aux pressions de surface aux emplacements de la carte.
Les isallobars sont des lignes joignant des points de changement de pression égal pendant un intervalle de temps spécifique.
Les gradients isallobariques sont des composants importants du vent car ils augmentent ou diminuent le vent géostrophique.
Une isotherme à 0 °C est appelée niveau de congélation.
A partir de ces contours, une idée du terrain général peut être déterminée.
En cartographie, l'intervalle de contour est la différence d'élévation entre les lignes de contour adjacentes.
La fusion de deux lignes de contour ou plus indique une falaise.
Habituellement, les intervalles de contour sont cohérents sur toute une carte, mais il existe des exceptions.
Que le franchissement d'une ligne équipotentielle représente une montée ou une descente, le potentiel est déduit des étiquettes sur les charges.
Les précipitations acides sont indiquées sur les cartes avec des isoplats.
Les lignes de contour sont également utilisées pour afficher des informations non géographiques en économie.
De telles isolignes sont utiles pour représenter plus de deux dimensions (ou quantités) sur des graphiques bidimensionnels.
Dans l'interprétation des ibames radar, un isodop est une ligne de vitesse Doppler égale, et un isoécho est une ligne de réflectivité radar égale.
La couleur de ligne est le choix de n'importe quel nombre de pigments qui conviennent à l'affichage.
Le type de ligne indique si la ligne de contour de base est pleine, en pointillés, en pointillés ou brisée dans un autre motif pour créer l'effet souhaité.
Le marquage numérique est la manière de désigner les valeurs arithmétiques des courbes de niveau.
Si les lignes de contour ne sont pas étiquetées numériquement et que les lignes adjacentes ont le même style (avec le même poids, la même couleur et le même type), la direction du dégradé ne peut pas être déterminée à partir des seules lignes de contour.
Une carte de contour correctement étiquetée aide le lecteur à interpréter rapidement la forme du terrain.
Ensuite, les coordonnées des autres lieux sont mesurées à partir du point de contrôle le plus proche par arpentage.
Ce phénomène est appelé décalage de référence.
Des entreprises plus ambitieuses telles que l'arc géodésique de Struve à travers l'Europe de l'Est (1816-1855) et le Great Trigonometrical Survey of India (1802-1871) ont pris beaucoup plus de temps, mais ont abouti à des estimations plus précises de la forme de l'ellipsoïde terrestre.
Une définition approximative du niveau de la mer est le datum WGS 84, un ellipsoïde, alors qu'une définition plus précise est Earth Gravitational Model 2008 (EGM2008), utilisant au moins 2 159 harmoniques sphériques.
Lorsqu'il est utilisé sans qualification, le terme latitude fait référence à la latitude géodésique.
Le décalage de référence entre deux références particulières peut varier d'un endroit à un autre dans un pays ou une région, et peut aller de zéro à des centaines de mètres (ou plusieurs kilomètres pour certaines îles éloignées).
Par exemple, à Sydney, il existe une différence de 200 mètres (700 pieds) entre les coordonnées GPS configurées dans GDA (basées sur la norme mondiale WGS 84) et AGD (utilisées pour la plupart des cartes locales), ce qui est une erreur inacceptable pour certaines applications, telles que comme arpentage ou emplacement de site pour la plongée sous-marine.
Étant donné que les référentiels peuvent avoir différents rayons et différents points centraux, un point spécifique sur Terre peut avoir des coordonnées sensiblement différentes en fonction du référentiel utilisé pour effectuer la mesure.
Les référentiels de référence les plus couramment utilisés en Amérique du Nord sont NAD27, NAD83 et WGS 84.
Cette donnée, désignée NAD 83 ... est basée sur l'ajustement de 250 000 points dont 600 stations satellites Doppler qui contraignent le système à une origine géocentrique."
C'est le référentiel utilisé par le Département américain de la Défense (DoD) et défini par la National Geospatial-Intelligence Agency (NGA) (anciennement Defense Mapping Agency, puis National Ibamery and Mapping Agency).
Il a été utilisé comme cadre de référence pour la diffusion des éphémérides GPS (orbites) à partir du 23 janvier 1987.
Il est devenu le cadre de référence pour les orbites de diffusion le 28 juin 1994.
WGS 84 (G873) a été adopté comme cadre de référence pour les orbites de diffusion le 29 janvier 1997.
WGS 84 est le système de référence standard par défaut pour les coordonnées stockées dans les unités GPS récréatives et commerciales.
Par exemple, la différence longitudinale entre un point sur l'équateur en Ouganda, sur la plaque africaine, et un point sur l'équateur en Équateur, sur la plaque sud-américaine, augmente d'environ 0,0014 seconde d'arc par an.
La plupart des cartographies, comme dans un seul pays, ne couvrent pas les plaques.
Ptolémée lui a attribué l'adoption complète de la longitude et de la latitude, plutôt que de mesurer la latitude en termes de longueur du jour d'été.
La cartographie mathématique a repris en Europe suite à la récupération par Maximus Planudes du texte de Ptolémée un peu avant 1300 ; le texte a été traduit en latin à Florence par Jacobus Angelus vers 1407.
Ils choisissent ensuite la cartographie la plus appropriée du système de coordonnées sphériques sur cet ellipsoïde, appelé système de référence terrestre ou datum géodésique.
φ , ou phi) d'un point sur la surface de la Terre est l'angle entre le plan équatorial et la ligne droite qui passe par ce point et par (ou à proximité) le centre de la Terre.
Tous les méridiens sont des moitiés de grandes ellipses (souvent appelées grands cercles), qui convergent aux pôles Nord et Sud.
Le méridien antipodal de Greenwich est à la fois 180°W et 180°E. Cela ne doit pas être confondu avec la ligne de date internationale , qui en diverge à plusieurs endroits pour des raisons politiques et de commodité, y compris entre l'extrême est de la Russie et l'extrême ouest des îles Aléoutiennes.
Les coordonnées sur une carte sont généralement en termes de décalages nord N et est E par rapport à une origine spécifiée.
En géographie, la latitude est une coordonnée géographique qui spécifie la position nord-sud d'un point sur la surface de la Terre.
La latitude est utilisée avec la longitude pour spécifier l'emplacement précis des éléments à la surface de la Terre.
La deuxième étape consiste à approximer le géoïde par une surface de référence mathématiquement plus simple.
Les lignes de latitude et de longitude constantes constituent ensemble un graticule sur la surface de référence.
Puisqu'il existe de nombreux ellipsoïdes de référence différents, la latitude précise d'une entité sur la surface n'est pas unique : ceci est souligné dans la norme ISO qui stipule que « sans la spécification complète du système de référence de coordonnées, les coordonnées (c'est-à-dire la latitude et la longitude) sont au mieux ambigus et au pire dénués de sens ».
Le plan passant par le centre de la Terre et perpendiculaire à l'axe de rotation coupe la surface en un grand cercle appelé l'équateur.
La variation temporelle est discutée plus en détail dans l'article sur l'inclinaison axiale.
La situation est inversée au solstice de juin, lorsque le soleil est au-dessus du tropique du cancer.
La latitude étant définie par rapport à un ellipsoïde, la position d'un point donné est différente sur chaque ellipsoïde : on ne peut pas préciser exactement la latitude et la longitude d'un élément géographique sans préciser l'ellipsoïde utilisé.
La latitude géographique doit être utilisée avec précaution.
L'évaluation de l'intégrale de la distance méridienne est au cœur de nombreuses études en géodésie et en projection cartographique.
Il existe deux manières de procéder.
Lors de la conversion d'isométrique ou conforme en géodésique, deux itérations de Newton-Raphson donnent une précision double.
Les différences indiquées sur le graphique sont en minutes d'arc.
La transformation entre les coordonnées géodésiques et cartésiennes peut être trouvée dans Conversion de coordonnées géographiques.
En général, la verticale vraie en un point de la surface ne coïncide exactement ni avec la normale à l'ellipsoïde de référence ni avec la normale au géoïde.
La longitude est une coordonnée géographique qui spécifie la position est-ouest d'un point sur la surface de la Terre ou la surface d'un corps céleste.
Le premier méridien, qui passe près de l'Observatoire royal de Greenwich, en Angleterre, est défini comme 0° de longitude par convention.
L'heure locale (par exemple à partir de la position du soleil) varie avec la longitude, un écart de 15° de longitude correspondant à un décalage d'une heure de l'heure locale.
Le principe est simple, mais en pratique, trouver une méthode fiable pour déterminer la longitude a pris des siècles et a nécessité l'effort de certains des plus grands esprits scientifiques.
Son premier méridien passait par Alexandrie.
En 1910, le Journal a publié un article d'Ulysses G. Weatherly (1865-1940) qui appelait à la suprématie blanche et à la ségrégation des races pour protéger la pureté raciale.
Dans son travail, il a soutenu que la classe sociale, le colonialisme et le capitalisme façonnaient les idées sur la race et les catégories raciales.
En 1978, William Julius Wilson (1935–) a fait valoir que la race et les systèmes de classification raciale perdaient en importance et qu'au lieu de cela, la classe sociale décrivait plus précisément ce que les sociologues avaient auparavant compris comme la race.
Eduardo Bonilla-Silva, professeur de sociologie à l'Université Duke, remarque : « Je soutiens que le racisme est, plus que toute autre chose, une question de pouvoir de groupe ; il s'agit d'un groupe racial dominant (les Blancs) qui s'efforce de maintenir ses avantages systémiques et des minorités qui luttent renverser le statu quo racial.
Dans les milieux cliniques, la race a parfois été prise en compte dans le diagnostic et le traitement des conditions médicales.
Il y a un débat actif parmi les chercheurs biomédicaux sur la signification et l'importance de la race dans leurs recherches.
Les membres de ce dernier camp fondent souvent leurs arguments sur le potentiel de création d'une médecine personnalisée basée sur le génome.
Ils soutiennent que le fait de trop insister sur les contributions génétiques aux disparités en matière de santé comporte divers risques, tels que le renforcement des stéréotypes, la promotion du racisme ou l'ignorance de la contribution des facteurs non génétiques aux disparités en matière de santé.
IC" signifie "Identification Code" ; ces éléments sont également appelés classifications Phoenix.
Dans de nombreux pays, comme la France, il est légalement interdit à l'État de conserver des données basées sur la race, ce qui oblige souvent la police à émettre des avis de recherche au public qui incluent des étiquettes comme "teint de peau foncée", etc.
Beaucoup considèrent le profilage racial de facto comme un exemple de racisme institutionnel dans l'application de la loi.
L'incarcération de masse est aussi "le réseau plus large de lois, de règles, de politiques et de coutumes qui contrôlent les criminels étiquetés à l'intérieur et à l'extérieur de la prison".
De nombreux résultats de recherche semblent convenir que l'impact de la race de la victime dans la décision d'arrestation de VPI pourrait éventuellement inclure un préjugé racial en faveur des victimes blanches.
Certaines études ont rapporté que les races peuvent être identifiées avec un haut degré de précision en utilisant certaines méthodes, comme celle développée par Giles et Elliot.
L'étude a conclu que "la répartition de la diversité génétique dans la couleur de la peau est atypique et ne peut pas être utilisée à des fins de classification".
L'anthropologie culturelle est une branche de l'anthropologie axée sur l'étude de la variation culturelle chez les humains.
En abordant cette question, les ethnologues du XIXe siècle se sont divisés en deux écoles de pensée.
Certains de ceux qui prônaient «l'invention indépendante», comme Lewis Henry Morgan, supposaient en outre que les similitudes signifiaient que différents groupes avaient traversé les mêmes étapes d'évolution culturelle (voir aussi l'évolutionnisme social classique).
Morgan, comme d'autres évolutionnistes sociaux du XIXe siècle, croyait qu'il y avait une progression plus ou moins ordonnée du primitif au civilisé.
Bien que les ethnologues du XIXe siècle considéraient la «diffusion» et «l'invention indépendante» comme des théories mutuellement exclusives et concurrentes, la plupart des ethnographes sont rapidement parvenus à un consensus sur le fait que les deux processus se produisent et que les deux peuvent vraisemblablement expliquer les similitudes interculturelles.
Boas a formulé l'idée pour la première fois en 1887: "... la civilisation n'est pas quelque chose d'absolu, mais ... est relative, et ... nos idées et conceptions ne sont vraies que dans la mesure où notre civilisation va."
Le relativisme culturel implique des revendications épistémologiques et méthodologiques spécifiques.
Le relativisme culturel était en partie une réponse à l'ethnocentrisme occidental.
Cette compréhension de la culture confronte les anthropologues à deux problèmes : premièrement, comment échapper aux liens inconscients de sa propre culture, qui biaisent inévitablement nos perceptions et nos réactions au monde, et deuxièmement, comment donner un sens à une culture inconnue.
L'une de ces méthodes est celle de l'ethnographie : fondamentalement, ils préconisaient de vivre avec des personnes d'une autre culture pendant une période prolongée, afin qu'elles puissent apprendre la langue locale et être inculturées, au moins partiellement, dans cette culture.
Son approche était empirique, sceptique quant aux généralisations excessives et évitait les tentatives d'établir des lois universelles.
Il croyait que chaque culture devait être étudiée dans sa particularité et soutenait que les généralisations interculturelles, comme celles faites dans les sciences naturelles, n'étaient pas possibles.
Sa première génération d'étudiants comprenait Alfred Kroeber, Robert Lowie, Edward Sapir et Ruth Benedict, qui ont chacun produit des études richement détaillées sur les cultures indigènes nord-américaines.
La publication du manuel Anthropology d'Alfred Kroeber (1923) a marqué un tournant dans l'anthropologie américaine.
Influencés par des psychologues psychanalytiques tels que Sigmund Freud et Carl Jung, ces auteurs ont cherché à comprendre la manière dont les personnalités individuelles étaient façonnées par les forces culturelles et sociales plus larges dans lesquelles elles ont grandi.
L'anthropologie économique, influencée par Karl Polanyi et pratiquée par Marshall Sahlins et George Dalton, a défié l'économie néoclassique standard de prendre en compte les facteurs culturels et sociaux et a utilisé l'analyse marxienne dans l'étude anthropologique.
Conformément à l'époque, une grande partie de l'anthropologie s'est politisée à travers la guerre d'indépendance algérienne et l'opposition à la guerre du Vietnam ; Le marxisme est devenu une approche théorique de plus en plus populaire dans la discipline.
Dans les années 1980, des livres comme Anthropology and the Colonial Encounter se sont penchés sur les liens de l'anthropologie avec l'inégalité coloniale, tandis que l'immense popularité de théoriciens comme Antonio Gramsci et Michel Foucault a mis en lumière les questions de pouvoir et d'hégémonie.
Ces interprétations doivent ensuite être renvoyées à leurs auteurs, et leur adéquation en tant que traduction affinée de manière répétée, un processus appelé le cercle herméneutique.
L'analyse culturelle de David Schnieder sur la parenté américaine s'est avérée tout aussi influente.
La méthode trouve son origine dans la recherche sur le terrain d'anthropologues sociaux, en particulier Bronislaw Malinowski en Grande-Bretagne, les étudiants de Franz Boas aux États-Unis, et dans la recherche urbaine ultérieure de la Chicago School of Sociology.
Walnut Creek, Californie : AltaMira Press.
Pour établir des liens qui mèneront éventuellement à une meilleure compréhension du contexte culturel d'une situation, un anthropologue doit être ouvert à faire partie du groupe et désireux de développer des relations significatives avec ses membres.
Avant que l'observation participante puisse commencer, un anthropologue doit choisir à la fois un lieu et un objet d'étude.
Cela permet à l'anthropologue de mieux s'implanter dans la communauté.
La majorité de l'observation des participants est basée sur la conversation.
Dans certains cas, les ethnographes se tournent également vers l'observation structurée, dans laquelle les observations d'un anthropologue sont dirigées par un ensemble spécifique de questions auxquelles il essaie de répondre.
Cela aide à normaliser la méthode d'étude lorsque les données ethnographiques sont comparées entre plusieurs groupes ou sont nécessaires pour atteindre un objectif spécifique, comme la recherche pour une décision politique gouvernementale.
Qui est l'ethnographe a beaucoup à voir avec ce qu'il ou elle finira par écrire sur une culture, car chaque chercheur est influencé par sa propre perspective.
Cependant, ces approches n'ont généralement pas été couronnées de succès et les ethnographes modernes choisissent souvent d'inclure leurs expériences personnelles et leurs éventuels préjugés dans leurs écrits.
Une ethnographie est un écrit sur un peuple, à un endroit et à une époque particuliers.
Une ethnographie typique comprendra également des informations sur la géographie physique, le climat et l'habitat.
Les étudiants de Boas tels qu'Alfred L. Kroeber, Ruth Benedict et Margaret Mead se sont inspirés de sa conception de la culture et du relativisme culturel pour développer l'anthropologie culturelle aux États-Unis.
Aujourd'hui, les anthropologues socioculturels s'occupent de tous ces éléments.
Les «anthropologues culturels» américains se sont concentrés sur la manière dont les gens exprimaient leur vision d'eux-mêmes et de leur monde, en particulier sous des formes symboliques, telles que l'art et les mythes.
La monogamie, par exemple, est souvent présentée comme un trait humain universel, mais une étude comparative montre que ce n'est pas le cas.
Grâce à cette méthodologie, une meilleure compréhension peut être obtenue lors de l'examen de l'impact des systèmes mondiaux sur les communautés locales et mondiales.
Par exemple, une ethnographie multi-sites peut suivre une «chose», telle qu'une marchandise particulière, telle qu'elle est transportée à travers les réseaux du capitalisme mondial.
Un exemple d'ethnographie multi-sites est le travail de Nancy Scheper-Hughes sur le marché noir international du commerce d'organes humains.
La recherche dans les études de parenté traverse souvent différents sous-domaines anthropologiques, notamment l'anthropologie médicale, féministe et publique.
C'est la matrice dans laquelle les enfants humains naissent dans la grande majorité des cas, et leurs premiers mots sont souvent des termes de parenté.
Il existe des différences marquées entre les communautés en termes de pratiques et de valeurs matrimoniales, laissant une grande place au travail de terrain anthropologique.
La pratique conjugale que l'on retrouve dans la plupart des cultures, cependant, est la monogamie, où une femme est mariée à un homme.
Il existe des différences fondamentales similaires en ce qui concerne l'acte de procréation.
Le changement remonte aux années 1960, avec la réévaluation des principes de base de la parenté proposée par Edmund Leach, Rodney Neeham, David Scbamider et d'autres.
Ce changement a été renforcé par l'émergence du féminisme de la deuxième vague au début des années 1970, qui a introduit les idées d'oppression conjugale, d'autonomie sexuelle et de subordination domestique.
À cette époque, il y avait l'arrivée du «féminisme du tiers monde», un mouvement qui soutenait que les études de parenté ne pouvaient pas examiner les relations entre les sexes des pays en développement de manière isolée et devaient également respecter les nuances raciales et économiques.
En Jamaïque, le mariage en tant qu'institution se substitue souvent à une série de partenaires, car les femmes pauvres ne peuvent pas compter sur des contributions financières régulières dans un climat d'instabilité économique.
Avec cette technologie, des questions de parenté ont émergé sur la différence entre la parenté biologique et génétique, car les mères porteuses gestationnelles peuvent fournir un environnement biologique à l'embryon tandis que les liens génétiques restent avec un tiers.
Il y a également eu des problèmes de tourisme reproductif et de marchandisation corporelle, car les individus recherchent la sécurité économique par le biais de la stimulation hormonale et de la récolte des œufs, qui sont des procédures potentiellement nocives.
Une critique est que, à ses débuts, le cadre des études sur la parenté était beaucoup trop structuré et stéréotypé, reposant sur un langage dense et des règles strictes.
Une grande partie de ce développement peut être attribuée à l'augmentation du nombre d'anthropologues travaillant en dehors du milieu universitaire et à l'importance croissante de la mondialisation dans les institutions et le domaine de l'anthropologie.
Les deux types d'institutions définis dans le domaine de l'anthropologie sont les institutions totales et les institutions sociales.
L'anthropologie des institutions peut analyser les syndicats, les entreprises allant des petites entreprises aux sociétés, le gouvernement, les organisations médicales, l'éducation, les prisons et les institutions financières.
Les anthropologues institutionnels peuvent étudier la relation entre les organisations ou entre une organisation et d'autres parties de la société.
Plus précisément, les anthropologues peuvent analyser des événements spécifiques au sein d'une institution, effectuer des enquêtes sémiotiques ou analyser les mécanismes par lesquels les connaissances et la culture sont organisées et dispersées.
Cette nouvelle ère impliquerait de nombreux nouveaux développements technologiques, tels que l'enregistrement mécanique.
Current Anthropology 43 (Supplément):S5-17.Schieffelin, Bambi B. 2006.
Woolard, dans son aperçu du "changement de code", ou la pratique systématique d'alternance de variétés linguistiques au sein d'une conversation ou même d'un seul énoncé, trouve que la question sous-jacente que les anthropologues posent à la pratique - Pourquoi font-ils cela ? - reflète une idéologie linguistique dominante .
D'autres linguistes ont mené des recherches dans les domaines du contact linguistique, de la mise en danger des langues et de «l'anglais en tant que langue mondiale».
Le travail de Joel Kuipers développe ce thème face à l'île de Sumba, en Indonésie.
Il estime, en effet, que l'idée de centre exemplaire est l'une des trois découvertes les plus importantes de l'anthropologie linguistique.
Par conséquent, après quelques générations, ces langues peuvent ne plus être parlées.
Pour suivre les meilleures pratiques de documentation, ces enregistrements doivent être clairement annotés et conservés en lieu sûr dans une archive quelconque.
La revitalisation de la langue est la pratique consistant à ramener une langue dans l'usage courant.
Le cours vise à éduquer les étudiants autochtones et non autochtones sur la langue et la culture Lenape.
Encourager ceux qui connaissent déjà la langue à l'utiliser, augmenter les domaines d'utilisation et augmenter le prestige global de la langue sont tous des éléments de la récupération.
L'anthropologie sociale est l'étude des modèles de comportement dans les sociétés et les cultures humaines.
Des anthropologues britanniques et américains, dont Gillian Tett et Karen Ho, qui ont étudié Wall Street, ont fourni une explication alternative à la crise financière de 2007-2010 aux explications techniques ancrées dans la théorie économique et politique.
Ce développement a été renforcé par l'introduction par Franz Boas du relativisme culturel arguant que les cultures sont basées sur des idées différentes sur le monde et ne peuvent donc être correctement comprises qu'en fonction de leurs propres normes et valeurs.
En 1906, le pygmée congolais Ota Benga a été placé par l'anthropologue américaine Madison Grant dans une cage du zoo du Bronx, étiqueté "le chaînon manquant" entre un orang-outan et la "race blanche" - Grant, un eugéniste renommé, était également l'auteur de The Passage de la Grande Race (1916).
L'anthropologie s'est de plus en plus distinguée de l'histoire naturelle et à la fin du XIXe siècle, la discipline a commencé à se cristalliser dans sa forme moderne - en 1935, par exemple, il était possible pour TK Penniman d'écrire une histoire de la discipline intitulée A Hundred Years of Anthropology .
Les sociétés non européennes étaient ainsi vues comme des « fossiles vivants » évolutifs qui pouvaient être étudiés pour comprendre le passé européen.
Cependant, comme le note Stocking, Tylor s'est principalement préoccupé de décrire et de cartographier la distribution d'éléments particuliers de la culture, plutôt que de la fonction plus large, et il semblait généralement assumer une idée victorienne du progrès plutôt que l'idée d'un système non directionnel et multilinéaire. changement culturel proposé par les anthropologues ultérieurs.
Ses études comparatives, les plus influentes dans les nombreuses éditions de The Golden Bough, ont analysé les similitudes dans les croyances religieuses et le symbolisme à l'échelle mondiale.
Les découvertes de l'expédition ont établi de nouvelles normes pour la description ethnographique.
Parmi les autres fondateurs intellectuels figurent WHR Rivers et AC Haddon, dont l'orientation reflétait les parapsychologies contemporaines de Wilhelm Wundt et Adolf Bastian, et Sir EB Tylor, qui a défini l'anthropologie comme une science positiviste à la suite d'Auguste Comte.
AR Radcliffe-Brown a également publié un ouvrage fondateur en 1922.
Ce fut particulièrement le cas de Radcliffe-Brown, qui diffusa son programme d'« anthropologie sociale » en enseignant dans des universités de l'Empire britannique et du Commonwealth.
Il croyait que les termes autochtones utilisés dans les données ethnographiques devraient être traduits en termes juridiques anglo-américains pour le bénéfice du lecteur.
Les départements d'anthropologie sociale de différentes universités ont eu tendance à se concentrer sur des aspects disparates du domaine.
Un peuple est une pluralité de personnes considérées comme un tout.
Quatre États – le Massachusetts, la Virginie, la Pennsylvanie et le Kentucky – se désignent eux-mêmes comme le Commonwealth dans les légendes de cas et la procédure judiciaire.
Dans certaines parties du monde, l'ethnologie s'est développée selon des voies indépendantes d'investigation et de doctrine pédagogique, l'anthropologie culturelle devenant dominante, notamment aux États-Unis, et l'anthropologie sociale en Grande-Bretagne.
L'exploration de l'Amérique au XVe siècle par les explorateurs européens a joué un rôle important dans la formulation de nouvelles notions de l'Occident (le monde occidental), telles que la notion de «l'Autre».
Les progrès de l'ethnologie, par exemple avec l'anthropologie structurale de Claude Lévi-Strauss, ont conduit à critiquer les conceptions d'un progrès linéaire, ou la pseudo-opposition entre « sociétés à histoire » et « sociétés sans histoire », jugées trop dépendantes d'un vue de l'histoire comme constituée par la croissance accumulative.
Cependant, les revendications d'un tel universalisme culturel ont été critiquées par divers penseurs sociaux des XIXe et XXe siècles, dont Marx, Nietzsche, Foucault, Derrida, Althusser et Deleuze.
Un groupe ethnique ou une ethnie est un groupement de personnes qui s'identifient les unes aux autres sur la base d'attributs communs qui les distinguent des autres groupes, tels qu'un ensemble commun de traditions, d'ascendance, de langue, d'histoire, de société, de culture, de nation, de religion ou de traitement social dans leur zone de résidence.
L'appartenance à un groupe ethnique a tendance à être définie par un patrimoine culturel partagé, une ascendance, un mythe d'origine, une histoire, une patrie, une langue ou un dialecte, des systèmes symboliques tels que la religion, la mythologie et les rituels, la cuisine, le style vestimentaire, l'art ou l'apparence physique.
Par voie de changement de langue, d'acculturation, d'adoption et de conversion religieuse, des individus ou des groupes peuvent au fil du temps passer d'un groupe ethnique à un autre.
Que ce soit par division ou fusion, la formation d'une identité ethnique distincte est appelée ethnogenèse.
Au début de l'anglais moderne et jusqu'au milieu du XIXe siècle, ethnique était utilisé pour signifier païen ou païen (au sens de « nations » disparates qui ne participaient pas encore à l'oikumene chrétien), comme la Septante utilisait ta etbam (« les nations ") pour traduire l'hébreu goyim "les nations, non-hébreux, non-juifs".
Au 19ème siècle, le terme est venu à être utilisé dans le sens de "particulier à une race, un peuple ou une nation", dans un retour au sens grec d'origine.
ethnique, a. et n.") Selon le contexte, le terme nationalité peut être utilisé soit comme synonyme d'appartenance ethnique, soit comme synonyme de citoyenneté (dans un État souverain).
La question de savoir si l'ethnicité peut être qualifiée d'universel culturel dépend dans une certaine mesure de la définition exacte utilisée.
Selon Thomas Hylland Eriksen, l'étude de l'ethnicité était jusqu'à récemment dominée par deux débats distincts.
L'approche instrumentaliste, d'autre part, traite l'ethnicité principalement comme un élément ad hoc d'une stratégie politique, utilisée comme une ressource pour les groupes d'intérêt pour atteindre des objectifs secondaires tels que, par exemple, une augmentation de la richesse, du pouvoir ou du statut.
Les constructivistes considèrent les identités nationales et ethniques comme le produit de forces historiques, souvent récentes, même lorsque les identités sont présentées comme anciennes.
Cela s'inscrit dans le contexte des débats sur le multiculturalisme dans des pays comme les États-Unis et le Canada, qui comptent d'importantes populations d'immigrants de nombreuses cultures différentes, et sur le post-colonialisme dans les Caraïbes et l'Asie du Sud.
Troisièmement, la formation de groupes résultait de la volonté de monopoliser le pouvoir et le statut.
Barth est allé plus loin que Weber en insistant sur la nature construite de l'ethnicité.
Il voulait se séparer des notions anthropologiques des cultures en tant qu'entités délimitées et de l'ethnicité en tant que liens primordialistes, en les remplaçant par une focalisation sur l'interface entre les groupes. "
Il est d'accord avec l'observation de Joan Vincent selon laquelle (dans la paraphrase de Cohen) « l'ethnicité... peut être réduite ou élargie en termes de frontières en fonction des besoins spécifiques de la mobilisation politique.
Les groupes ethniques en sont venus à être définis comme des entités sociales plutôt que biologiques.
Des exemples de diverses approches sont le primordialisme, l'essentialisme, le pérennialisme, le constructivisme, le modernisme et l'instrumentalisme.
Le «primordialisme essentialiste» soutient en outre que l'ethnicité est un fait a priori de l'existence humaine, que l'ethnicité précède toute interaction sociale humaine et qu'elle en est inchangée.
Le « primordialisme de la parenté » soutient que les communautés ethniques sont des extensions d'unités de parenté, étant essentiellement dérivées de liens de parenté ou de clan où les choix de signes culturels (langue, religion, traditions) sont faits exactement pour montrer cette affinité biologique.
Le «primordialisme de Geertz», notamment adopté par l'anthropologue Clifford Geertz, soutient que les humains en général attribuent un pouvoir écrasant aux «données» humaines primordiales telles que les liens du sang, la langue, le territoire et les différences culturelles.
Smith (1999) distingue deux variantes : le « pérennialisme continu », qui prétend que des nations particulières existent depuis de très longues périodes, et le « pérennialisme récurrent », qui se concentre sur l'émergence, la dissolution et la réapparition des nations comme un aspect récurrent de l'histoire humaine.
Ce point de vue soutient que le concept d'ethnicité est un outil utilisé par les groupes politiques pour manipuler des ressources telles que la richesse, le pouvoir, le territoire ou le statut dans l'intérêt de leurs groupes particuliers.
Le « pérennialisme instrumentaliste », tout en considérant l'ethnicité principalement comme un outil polyvalent qui identifie les différents groupes ethniques et les limites à travers le temps, explique l'ethnicité comme un mécanisme de stratification sociale, ce qui signifie que l'ethnicité est la base d'une hiérarchisation des individus.
Selon Donald Noel, la stratification ethnique n'émergera que lorsque des groupes ethniques spécifiques seront mis en contact les uns avec les autres, et seulement lorsque ces groupes seront caractérisés par un degré élevé d'ethnocentrisme, de compétition et de pouvoir différentiel.
En continuant avec la théorie de Noel, un certain degré de pouvoir différentiel doit être présent pour l'émergence de la stratification ethnique.
Les différents groupes ethniques doivent être en compétition pour un objectif commun, comme le pouvoir ou l'influence, ou un intérêt matériel, comme la richesse ou le territoire.
Il soutient que les groupes ethniques ne sont que des produits de l'interaction sociale humaine, maintenus uniquement dans la mesure où ils sont maintenus en tant que constructions sociales valides dans les sociétés.
Ils soutiennent qu'avant cela, l'homogénéité ethnique n'était pas considérée comme un facteur idéal ou nécessaire pour forger des sociétés à grande échelle.
Les membres d'un groupe ethnique, dans l'ensemble, revendiquent des continuités culturelles dans le temps, bien que les historiens et les anthropologues culturels aient documenté que bon nombre des valeurs, pratiques et normes qui impliquent une continuité avec le passé sont d'invention relativement récente.
Elle est basée sur la notion de « culture ».
Ce point de vue est apparu comme un moyen de justifier l'asservissement des Afro-Américains et le génocide des Amérindiens dans une société officiellement fondée sur la liberté pour tous.
Bon nombre des scientifiques les plus éminents de l'époque ont repris l'idée de la différence raciale et ont découvert que les Européens blancs étaient supérieurs.
Au lieu d'attribuer le statut marginalisé des personnes de couleur aux États-Unis à leur infériorité biologique inhérente, il l'a attribué à leur incapacité à s'assimiler à la culture américaine.
Ils soutiennent dans Racial Formation in the United States que la théorie de l'ethnicité était exclusivement basée sur les schémas d'immigration de la population blanche et tenait compte des expériences uniques des non-Blancs aux États-Unis.
L'assimilation éliminant les qualités particulières d'une culture autochtone dans le but de se fondre dans une culture d'accueil n'a pas fonctionné pour certains groupes en réponse au racisme et à la discrimination, alors qu'elle l'a été pour d'autres.
Ils ont abouti à la montée des "États-nations" dans lesquels les frontières présumées de la nation coïncidaient (ou idéalement coïncidaient) avec les frontières des États.
Les États-nations, cependant, incluent invariablement des populations qui ont été exclues de la vie nationale pour une raison ou une autre.
Les États multiethniques peuvent être le résultat de deux événements opposés, soit la création récente de frontières étatiques en contradiction avec les territoires tribaux traditionnels, soit l'immigration récente de minorités ethniques dans un ancien État-nation.
Des États tels que le Royaume-Uni, la France et la Suisse comprenaient des groupes ethniques distincts depuis leur formation et ont également connu une immigration importante, aboutissant à ce qu'on a appelé des sociétés «multiculturelles», en particulier dans les grandes villes.
Bien que ces catégories soient généralement considérées comme appartenant à la sphère publique et politique, elles sont largement défendues dans la sphère privée et familiale.
Avant Weber (1864-1920), la race et l'ethnicité étaient principalement considérées comme deux aspects d'une même chose.
Selon ce point de vue, l'État ne devrait pas reconnaître l'identité ethnique, nationale ou raciale, mais plutôt faire respecter l'égalité politique et juridique de tous les individus.
Le XIXe siècle a vu le développement de l'idéologie politique du nationalisme ethnique, lorsque le concept de race a été lié au nationalisme, d'abord par des théoriciens allemands dont Johann Gottfried von Herder.
Chacun a promu l'idée panethnique selon laquelle ces gouvernements n'acquéraient que des terres qui avaient toujours été habitées par des Allemands de souche.
La colonisation de l'Asie a pris fin en grande partie au XXe siècle, avec des efforts nationaux pour l'indépendance et l'autodétermination à travers le continent.
Un certain nombre de pays européens, dont la France et la Suisse, ne collectent pas d'informations sur l'origine ethnique de leur population résidente.
Lors de la colonisation européenne, les Européens sont arrivés en Amérique du Nord.
L'ethnographie numérique offre beaucoup plus d'opportunités de regarder différentes cultures et sociétés.
L'ethnographie relationnelle articule l'étude des champs plutôt que des lieux ou des processus plutôt que des personnes traitées.
L'objectif est de collecter des données de manière à ce que le chercheur impose un minimum de biais personnels dans les données.
Les entretiens sont souvent enregistrés puis transcrits, ce qui permet à l'entretien de se dérouler sans altération de la prise de notes, mais avec toutes les informations disponibles ultérieurement pour une analyse complète.
Malgré ces tentatives de réflexivité, aucun chercheur ne peut être totalement impartial.
Ces informateurs sont généralement invités à identifier d'autres informateurs qui représentent la communauté, souvent en utilisant un échantillonnage en boule de neige ou en chaîne.
2010) examinent les présupposés ontologiques et épistémologiques qui sous-tendent l'ethnographie.
Les chercheurs en théorie critique abordent «les questions de pouvoir dans les relations chercheur-recherche et les liens entre savoir et pouvoir».
Un ibame peut être contenu dans le monde physique à travers la perspective d'un individu particulier, principalement basée sur les expériences passées de cet individu.
L'idée d'un ibame repose sur l'ibamination et a été vue comme étant utilisée par les enfants de manière très spontanée et naturelle.
Les anthropologues culturels et sociaux accordent aujourd'hui une grande valeur à la recherche ethnographique.
Les ethnographies sont aussi parfois appelées « études de cas ».
Le travail de terrain consiste généralement à passer un an ou plus dans une autre société, à vivre avec la population locale et à découvrir leurs modes de vie.
Les expériences de Benedict avec le pueblo Zuni du sud-ouest doivent être considérées comme la base de son travail de terrain formatif.
Une ethnographie typique tente d'être holistique et suit généralement un plan pour inclure une brève histoire de la culture en question, une analyse de la géographie physique ou du terrain habité par les personnes à l'étude, y compris le climat, et incluant souvent ce que les anthropologues biologiques appellent l'habitat.
La parenté et la structure sociale (y compris la classification par âge, les groupes de pairs, le sexe, les associations bénévoles, les clans, les moitiés, etc., s'ils existent) sont généralement incluses.
Les rites, rituels et autres preuves de la religion suscitent depuis longtemps un intérêt et sont parfois au cœur des ethnographies, en particulier lorsqu'ils sont menés en public où les anthropologues en visite peuvent les voir.
Par exemple, si au sein d'un groupe de personnes, le clin d'œil était un geste de communication, il cherchait d'abord à déterminer quel genre de choses un clin d'œil pouvait signifier (cela pouvait signifier plusieurs choses).
Geertz, tout en suivant quelque chose d'un schéma ethnographique traditionnel, est sorti de ce schéma pour parler de « toiles » au lieu de « contours » de la culture.
La culture de l'écriture a contribué à apporter des changements à la fois à l'anthropologie et à l'ethnographie souvent décrites comme étant de nature « postmoderne », « réflexive », « littéraire », « déconstructive » ou « poststructurale », dans la mesure où le texte a aidé à mettre en évidence les divers aspects épistémiques et des difficultés politiques que de nombreux praticiens considéraient comme affligeant les représentations et les pratiques ethnographiques.
En ce qui concerne ce dernier point, Writing Culture est devenu un point central pour examiner comment les ethnographes pouvaient décrire différentes cultures et sociétés sans nier la subjectivité des individus et des groupes étudiés tout en le faisant simultanément sans revendiquer une connaissance absolue et une autorité objective.
Comme le but de l'ethnographie est de décrire et d'interpréter les modèles partagés et appris de valeurs, de comportements, de croyances et de langage d'un groupe de partage de culture, Harris, (1968), ainsi qu'Agar (1980) notent que l'ethnographie est à la fois un processus et un résultat de la recherche.
La sociologue Sam Ladner soutient dans son livre que comprendre les consommateurs et leurs désirs nécessite un changement de "point de vue", que seule l'ethnographie fournit.
En évaluant l'expérience de l'utilisateur dans un cadre « naturel », l'ethnologie donne un aperçu des applications pratiques d'un produit ou d'un service.
La conférence Ethnographic Praxis in Industry (EPIC) en est la preuve.
La monographie de Jaber F. Gubrium et James A. Holstein (1997), The New Language of Qualitative Method, traite des formes d'ethnographie en termes de leur « discours méthodologique ».
Essentiellement, Fine soutient que les chercheurs ne sont généralement pas aussi éthiques qu'ils le prétendent ou le supposent - et que "chaque travail comprend des façons de faire les choses qu'il serait inapproprié pour les autres de connaître".
Il soutient que les « illusions » sont essentielles pour maintenir une réputation professionnelle et éviter des conséquences potentiellement plus caustiques.
Le code d'éthique note que les anthropologues font partie d'un réseau scientifique et politique plus large, ainsi que de l'environnement humain et naturel, qui doit être rapporté avec respect.
Les chercheurs prennent des quasi-fictions et les transforment en affirmations factuelles.
En réalité, un ethnographe manquera toujours un aspect à cause de son manque d'omniscience.
Les peuples autochtones, également appelés premiers peuples, peuples autochtones, peuples autochtones ou peuples autochtones, sont des groupes ethniques culturellement distincts qui sont originaires d'un lieu qui a été colonisé et colonisé par un autre groupe ethnique.
Les peuples sont généralement qualifiés d'« autochtones » lorsqu'ils conservent des traditions ou d'autres aspects d'une culture ancienne associée à une région donnée.
Les peuples autochtones continuent de faire face à des menaces pour leur souveraineté, leur bien-être économique, leurs langues, leurs modes de connaissance et leur accès aux ressources dont dépendent leurs cultures.
Les estimations de la population mondiale totale des peuples autochtones varient généralement entre 250 et 600 millions.
En référence à un groupe de personnes, le terme autochtone a d'abord été utilisé par les Européens qui l'ont utilisé pour différencier les peuples autochtones des Amériques des Africains réduits en esclavage.
Dans les années 1970, le terme a été utilisé comme un moyen de relier les expériences, les problèmes et les luttes des groupes de personnes colonisées à travers les frontières internationales.
Cette situation peut persister même dans le cas où la population autochtone est plus nombreuse que les autres habitants de la région ou de l'État; la notion déterminante ici est celle de la séparation des processus de décision et de réglementation qui ont une certaine influence, au moins titulaire, sur certains aspects de leurs droits communautaires et fonciers.
Un rapport des Nations Unies de 2009 publié par le Secrétariat de l'Instance permanente sur les questions autochtones a déclaré : Pendant des siècles, depuis l'époque de leur colonisation, conquête ou occupation, les peuples autochtones ont documenté des histoires de résistance, d'interface ou de coopération avec les États, démontrant ainsi leur conviction et la détermination à survivre avec leurs identités souveraines distinctes.
Ces personnes étaient considérées par les écrivains anciens soit comme les ancêtres des Grecs, soit comme un groupe antérieur de personnes qui habitaient la Grèce avant les Grecs.
Les croisades (1096-1271) étaient fondées sur cette ambition d'une guerre sainte contre ceux que l'Église considérait comme des infidèles.
Cependant, le concile a soutenu que les conquêtes pouvaient se produire «légalement» si les non-chrétiens refusaient de se conformer à la christianisation et à la loi naturelle européenne.
Aux XIVe et XVe siècles, les peuples autochtones de ce que l'on appelle maintenant les îles Canaries, connus sous le nom de Guanches (qui vivaient sur les îles depuis l'ère BCE) sont devenus le sujet de l'attention des colonisateurs.
En 1402, les Espagnols ont commencé leurs efforts pour envahir et coloniser les îles.
Les envahisseurs ont apporté destruction et maladies au peuple Guanche, dont l'identité et la culture ont ainsi disparu.
Comme l'ont déclaré Robert J. Miller, Jacinta Ruru, Larissa Behrendt et Tracey Lindberg, la doctrine s'est développée au fil du temps "pour justifier la domination de peuples non chrétiens et non européens et la confiscation de leurs terres et de leurs droits".
Le roi d'Espagne Ferdinand et la reine Isabelle ont engagé Christophe Colomb, qui a été envoyé en 1492, pour coloniser et apporter de nouvelles terres sous la couronne espagnole.
Alexandre a accordé à l'Espagne toutes les terres qu'elle découvrait tant qu'elles n'avaient pas été «déjà possédées par un propriétaire chrétien».
De nombreux conquistadors craignaient apparemment que, s'ils en avaient la possibilité, les peuples autochtones acceptent le christianisme, qui ne permettrait légalement pas l'invasion de leurs terres et le vol de leurs biens.
Étant des pays catholiques en 1493, l'Angleterre ainsi que la France ont travaillé à «réinterpréter» la doctrine de la découverte pour servir leurs propres intérêts coloniaux.
Les revendications territoriales ont été faites par le biais de «rituels de découverte» symboliques qui ont été exécutés pour illustrer la revendication légale de la nation colonisatrice sur la terre.
En 1774, le capitaine James Cook a tenté d'invalider les revendications territoriales espagnoles sur Tahiti en supprimant leurs marques de possession, puis en procédant à la mise en place de marques de possession anglaises.
Ce concept a formalisé l'idée que les terres qui n'étaient pas utilisées d'une manière approuvée par les systèmes juridiques européens étaient ouvertes à la colonisation européenne.
Au fur et à mesure que les « règles » de la colonisation sont devenues une doctrine juridique convenue entre les puissances coloniales européennes, les méthodes de revendication des terres autochtones ont continué à se développer rapidement.
Des estimations précises de la population totale des peuples autochtones du monde sont très difficiles à compiler, compte tenu des difficultés d'identification et des écarts et insuffisances des données de recensement disponibles.
Cela comprend au moins 5 000 peuples distincts dans plus de 72 pays.
Certaines ont également été assimilées par d'autres populations ou ont subi bien d'autres transformations.
Les groupes ethniques très divers et nombreux qui composent la plupart des États africains indépendants modernes contiennent en leur sein divers peuples dont la situation, les cultures et les modes de vie d'éleveurs ou de chasseurs-cueilleurs sont généralement marginalisés et mis à l'écart des structures politiques et économiques dominantes de la nation.
Les impacts de la colonisation européenne historique et en cours des Amériques sur les communautés autochtones ont été en général assez graves, de nombreuses autorités estimant des plages de déclin démographique important principalement en raison de la maladie, du vol de terres et de la violence.
Dans les États du sud d'Oaxaca (65,73%) et du Yucatán (65,40%), la majorité de la population est autochtone, comme indiqué en 2015.
Les descripteurs « Indien » et « Esquimau » sont tombés en désuétude au Canada.
Le plus notable a été le changement d'Affaires autochtones et Développement du Nord Canada (AADNC) en Affaires autochtones et du Nord Canada (AANC) en 2015, qui s'est ensuite scindé en Services aux Autochtones Canada et Relations Couronne-Autochtones et Développement du Nord Canada en 2017.
Entre 1871 et 1921, les peuples des Premières nations ont signé 11 traités numérotés dans une grande partie de ce qui est maintenant connu sous le nom de Canada, sauf dans certaines parties de la Colombie-Britannique.
Le territoire autonome du Groenland au sein du Royaume du Danemark abrite également une population autochtone et majoritaire reconnue d'Inuits (environ 85 %) qui se sont installés dans la région au XIIIe siècle, déplaçant les peuples autochtones dorsétiens et les norrois groenlandais.
Dans les pays hispanophones ou lusophones, on trouve l'utilisation de termes tels que índios, pueblos indígenas, amerindios, povos nativos, povos indígenas et, au Pérou, Comunidades Nativas (Communautés indigènes), en particulier parmi les sociétés amazoniennes comme les Urarina et les Matsés.
Les peuples autochtones se trouvent sur l'ensemble du territoire brésilien, bien que la majorité d'entre eux vivent dans des réserves indiennes du nord et du centre-ouest du pays.
Il y a actuellement plus d'Arméniens vivant en dehors de leur patrie ancestrale à cause du génocide arménien de 1915.
L'argument est entré dans le conflit israélo-palestinien dans les années 1990, les Palestiniens revendiquant le statut d'indigène en tant que population préexistante déplacée par la colonisation juive et constituant actuellement une minorité dans l'État d'Israël.
En Russie, la définition des "peuples autochtones" est contestée en se référant largement à un certain nombre de personnes (moins de 50 000 personnes) et en négligeant l'auto-identification, l'origine des populations autochtones qui habitaient le pays ou la région lors de l'invasion, de la colonisation ou de l'établissement de l'État. frontières, institutions sociales, économiques et culturelles distinctives.
Les Tibétains sont indigènes au Tibet.
À Hong Kong, les habitants autochtones des Nouveaux Territoires sont définis dans la Déclaration commune sino-britannique comme des personnes descendant par la lignée masculine d'une personne qui était en 1898, avant la Convention pour l'extension du territoire de Hong Kong.
Les Cham sont le peuple indigène de l'ancien État de Champa qui a été conquis par le Vietnam lors des guerres cham-vietnamiennes pendant le Nam tiến.
Les Khmers Krom sont les peuples autochtones du delta du Mékong et de Saigon qui ont été acquis par le Vietnam du roi cambodgien Chey Chettha II en échange d'une princesse vietnamienne.
Ce problème est partagé par de nombreux autres pays de la région de l'ANASE.
Les peuples autochtones de Mindanao sont les peuples Lumad et les Moro (Tausug, Maguindanao Maranao et autres) qui vivent également dans l'archipel de Sulu.
Ces groupes sont souvent considérés ensemble comme des Australiens autochtones.
Au cours du XXe siècle, plusieurs de ces anciennes colonies ont obtenu leur indépendance et des États-nations se sont formés sous contrôle local.
Les restes d'au moins 25 humains miniatures, qui vivaient il y a entre 1 000 et 3 000 ans, ont récemment été découverts sur les îles de Palau en Micronésie.
Selon le recensement de 2013, les Maoris néo-zélandais représentent 14,9% de la population de la Nouvelle-Zélande, avec moins de la moitié (46,5%) de tous les résidents maoris s'identifiant uniquement comme Maoris.
De nombreux dirigeants nationaux maoris ont signé un traité avec les Britanniques, le Traité de Waitangi (1840), considéré dans certains cercles comme formant l'entité géopolitique moderne qu'est la Nouvelle-Zélande.
Ces questions comprennent la préservation culturelle et linguistique, les droits fonciers, la propriété et l'exploitation des ressources naturelles, la détermination et l'autonomie politiques, la dégradation et l'incursion de l'environnement, la pauvreté, la santé et la discrimination.
La situation peut être encore plus confuse lorsqu'il y a une histoire compliquée ou contestée de la migration et de la population d'une région donnée, ce qui peut donner lieu à des différends sur la primauté et la propriété de la terre et des ressources.
Malgré la diversité des peuples autochtones, on peut noter qu'ils partagent des problèmes et des problèmes communs face à la société dominante ou envahissante.
Les exceptions notables sont les peuples Sakha et Komi (deux peuples autochtones du nord de la Russie), qui contrôlent désormais leurs propres républiques autonomes au sein de l'État russe, et les Inuits du Canada, qui forment la majorité du territoire du Nunavut (créé en 1999).
Ce rejet a fini par reconnaître qu'il existait un système de droit préexistant pratiqué par le peuple Meriam.
Consulté le 11 octobre 2011.
Les hindous et les chams ont tous deux subi des persécutions religieuses et ethniques et des restrictions à leur foi sous le gouvernement vietnamien actuel, l'État vietnamien confisquant les biens des cham et interdisant aux cham d'observer leurs croyances religieuses.
En 2012, la police vietnamienne du village de Chau Giang a fait irruption dans une mosquée cham, a volé le générateur électrique et a également violé des filles cham.
En 2012, l'Indonésie a déclaré que "le gouvernement indonésien soutient la promotion et la protection des peuples autochtones dans le monde entier ... L'Indonésie, cependant, ne reconnaît pas l'application du concept de peuples autochtones ... dans le pays".
Les Vietnamiens étaient à l'origine centrés autour du delta du fleuve Rouge mais se sont engagés dans la conquête et ont saisi de nouvelles terres telles que Champa, le delta du Mékong (du Cambodge) et les hauts plateaux du centre pendant Nam Tien.
L'énorme ampleur des colons vietnamiens Kinh inondant les hauts plateaux du centre a considérablement modifié la démographie de la région.
Et pas d'élimination d'une culture par une autre.
Les peuples autochtones ont été qualifiés de primitifs, de sauvages ou de non civilisés.
Certains philosophes, comme Thomas Hobbes (1588-1679), considéraient les indigènes comme de simples "sauvages".
Extrait des archives Internet le 13 décembre 2013.
La Déclaration des Nations Unies sur les droits des peuples autochtones, adoptée par l'Assemblée générale en 2007, a établi le droit des peuples autochtones à l'autodétermination, impliquant plusieurs droits concernant la gestion des ressources naturelles.
Le forage pétrolier pourrait détruire des milliers d'années de culture pour les Gwich'in.
Des projets de développement tels que la construction de barrages, de pipelines et l'extraction de ressources ont déplacé un grand nombre de peuples autochtones, souvent sans compensation.
Ces femmes deviennent également économiquement dépendantes des hommes lorsqu'elles perdent leurs moyens de subsistance.
Par exemple, le peuple Munduruku de la forêt amazonienne s'oppose à la construction du barrage de Tapajós avec l'aide de Greenpeace.
Deux scénarios principaux sont proposés, une expansion précoce vers l'Afrique centrale et une origine unique de la dispersion rayonnant à partir de là, ou une séparation précoce en une vague de dispersion vers l'est et vers le sud, avec une vague se déplaçant à travers le bassin du Congo vers l'Afrique de l'Est, et un autre se déplaçant vers le sud le long de la côte africaine et du système du fleuve Congo vers l'Angola.
La terminologie du bétail en usage parmi les relativement peu nombreux groupes d'éleveurs bantous modernes suggère que l'acquisition de bétail pourrait provenir de voisins de langue soudanienne centrale, kuliak et couchitique.
Non loin de la rivière Mutirikiwi, les rois Monomatapa ont construit le complexe du Grand Zimbabwe, une civilisation ancestrale du peuple Kalanga.
La culture swahili qui a émergé de ces échanges témoigne de nombreuses influences arabes et islamiques inconnues dans la culture bantoue traditionnelle, tout comme les nombreux membres afro-arabes du peuple bantou swahili.
Après la Seconde Guerre mondiale, les gouvernements du Parti national ont officiellement adopté cet usage, tandis que le mouvement nationaliste africain croissant et ses alliés libéraux se sont plutôt tournés vers le terme «africain», de sorte que «bantou» s'est identifié aux politiques d'apartheid.
Encore une fois, l'association avec l'apartheid a discrédité le terme, et le gouvernement sud-africain est passé au terme politiquement attrayant mais historiquement trompeur de «patries ethniques».
En swati, le radical est -ntfu et le nom est buntfu.
Tous les Basques ne sont pas bascophones.
basque moderne esan) et le suffixe -(k)ara ("façon (de faire quelque chose)").
Il enregistre le nom de la langue basque comme enusquera.
Bien qu'ils soient génétiquement distinctifs à certains égards en raison de l'isolement, les Basques sont encore très typiquement européens en termes de séquences d'ADN-Y et d'ADNmt, et en termes de certains autres loci génétiques.
Cependant, des études sur les haplogroupes Y-ADN ont révélé que sur leurs lignées masculines directes, la grande majorité des Basques modernes ont une ascendance commune avec d'autres Européens de l'Ouest, à savoir une prédominance marquée de l'haplogroupe indo-européen R1b-DF27 (70%).
Malgré sa fréquence élevée chez les Basques, la diversité interne Y-STR de R1b-DF27 y est plus faible et se traduit par des estimations d'âge plus récentes ", ce qui implique qu'il a été apporté dans la région d'ailleurs.
La collection d'haplogroupes d'ADNmt et d'ADN-Y échantillonnés là-bas différait considérablement par rapport à leurs fréquences modernes.
Au contraire, il y a environ 4500 ans, presque tout l'héritage Y-ADN du mélange ibérique de chasseurs-cueilleurs mésolithiques et d'agriculteurs néolithiques a été remplacé par la lignée R1b des éleveurs indo-européens de la steppe, et la spécificité génétique basque est le résultat de siècles de faible la taille de la population, la dérive génétique et l'endogamie.
Mattias Jakobsson de l'Université d'Uppsala en Suède a analysé le matériel génétique de huit squelettes humains de l'âge de pierre trouvés dans la caverne d'El Portalón à Atapuerca, dans le nord de l'Espagne.
Les résultats ont été publiés dans Actes de l'Académie nationale des sciences des États-Unis.
Ce groupe mélangé s'est également avéré être ancestral d'autres peuples ibériques modernes, mais alors que les Basques sont restés relativement isolés pendant des millénaires après cette époque, les migrations ultérieures vers la péninsule ibérique ont conduit à un mélange distinct et supplémentaire dans tous les autres groupes ibériques.
Il y a suffisamment de preuves pour étayer l'hypothèse qu'à cette époque et plus tard, ils parlaient d'anciennes variétés de la langue basque (voir : langue aquitaine).
Le royaume de Pampelune , un royaume basque central, plus tard connu sous le nom de Navarre, a subi un processus de féodalisation et a été soumis à l'influence de ses voisins aragonais, castillans et français beaucoup plus grands.
Affaibli par la guerre civile navarraise, la majeure partie du royaume finit par tomber devant l'assaut des armées espagnoles (1512-1524).
Néanmoins, les Basques ont bénéficié d'une grande autonomie gouvernementale jusqu'à la Révolution française (1790) et les guerres carlistes (1839, 1876), lorsque les Basques ont soutenu l'héritier présomptif Carlos V et ses descendants.
La communauté autonome (concept établi dans la Constitution espagnole de 1978) connue sous le nom d'Euskal Autonomia Erkidegoa ou EAE en basque et sous le nom de Comunidad Autónoma Vasca ou CAV en espagnol (en anglais : Basque Autonomous Community ou BAC), est composée des trois provinces d'Álava, Biscaye et Gipuzkoa.
Il est parfois simplement appelé "le Pays basque" (ou Euskadi) par des écrivains et des organismes publics ne considérant que ces trois provinces occidentales, mais aussi parfois simplement comme une abréviation pratique lorsque cela ne prête pas à confusion dans le contexte.
En particulier, dans l'usage courant, le terme français Pays Basque (" Pays Basque "), en l'absence de qualification supplémentaire, se réfère soit à l'ensemble du Pays Basque (" Euskal Herria " en basque), soit assez souvent au nord (ou " Français ") Pays basque plus précisément.
Notez que dans des contextes historiques, la Navarre peut désigner une zone plus large et que l'actuelle province basque du nord de la Basse Navarre peut également être appelée (partie de) Nafarroa, tandis que le terme "Haute Navarre" (Nafarroa Garaia en basque, Alta Navarra en espagnol) est également rencontré comme une manière de se référer au territoire de la communauté autonome actuelle.
La connaissance de l'espagnol est obligatoire en vertu de la constitution espagnole (article no.
La connaissance du basque, après avoir décliné pendant de nombreuses années sous la dictature de Franco en raison de la persécution officielle, est à nouveau en hausse en raison de politiques favorables aux langues officielles et du soutien populaire.
Seul l'espagnol est une langue officielle de Navarre, et la langue basque n'est co-officielle que dans la région nord de la province, où se concentrent la plupart des Navarrais bascophones.
Une grande partie de cette population vit dans ou à proximité de la ceinture urbaine Bayonne-Anglet-Biarritz (BAB) sur la côte (en basque, ce sont Baiona, Angelu et Miarritze).
Des millions de descendants basques (voir basque américain et basque canadien) vivent en Amérique du Nord (États-Unis; Canada, principalement dans les provinces de Terre-Neuve et du Québec), en Amérique latine (dans les 23 pays), en Afrique du Sud et en Australie.
Les estimations varient entre 2,5 et 5 millions de descendants basques vivent au Chili ; les Basques ont eu une influence majeure sinon la plus forte dans le développement culturel et économique du pays.
Il se composait principalement de la zone qui est aujourd'hui les États de Chihuahua et de Durango.
Au Guatemala, la plupart des Basques sont concentrés dans le département de Sacatepequez, Antigua Guatemala, Jalapa depuis six générations maintenant, tandis que certains ont migré vers Guatemala City.
Bambuco, une musique folklorique colombienne, a des racines basques.
Elko, Nevada, parraine un festival basque annuel qui célèbre la danse, la cuisine et les cultures des peuples basques de nationalités espagnole, française et mexicaine qui sont arrivés au Nevada depuis la fin du XIXe siècle.
Certains des plus grands ranchs d'Amérique du Nord, qui ont été fondés sous ces concessions de terres coloniales, se trouvent dans cette région.
Il y a une histoire de la culture basque à Chino, en Californie.
Ils sont pour la plupart des descendants de colons d'Espagne et du Mexique.
Ce sentiment d'identité basque lié à la langue locale n'existe pas seulement isolément.
Comme dans de nombreux États européens, une identité régionale, qu'elle soit d'origine linguistique ou autre, n'est pas mutuellement exclusive avec l'identité nationale plus large.
J'ai des amis qui sont impliqués dans le côté politique des choses mais ce n'est pas pour moi.
Il y a extrêmement peu de bascophones monolingues : pratiquement tous les bascophones sont bilingues des deux côtés de la frontière.
La langue basque est considérée comme une langue génétique isolée contrairement à d'autres langues européennes, qui appartiennent presque toutes à la grande famille des langues indo-européennes.
La maison dans ce contexte est synonyme de racines familiales.
Comme dans d'autres cultures, le sort des autres membres de la famille dépendait des actifs d'une famille: les familles basques riches avaient tendance à subvenir aux besoins de tous les enfants d'une manière ou d'une autre, tandis que les familles moins aisées n'avaient peut-être qu'un seul actif à fournir à un enfant.
Surtout après l'avènement de l'industrialisation, ce système a entraîné l'émigration de nombreux Basques ruraux vers l'Espagne, la France ou les Amériques.
Certains chercheurs et commentateurs ont tenté de concilier ces points en supposant que la parenté patrilinéaire représente une innovation.
Ils sont sortis du régime franquiste avec une langue et une culture revitalisées.
La région a été une source de missionnaires comme Francis Xavier et Michel Garicoïts.
Lasuén a succédé au franciscain Padre Junípero Serra et a fondé 9 des 21 missions californiennes existantes le long de la côte.
Au moment où Henri III de Navarre se convertit au catholicisme pour devenir roi de France, le protestantisme a pratiquement disparu de la communauté basque.
De nos jours, selon un seul sondage d'opinion, seulement un peu plus de 50% des Basques professent une sorte de croyance en Dieu, tandis que les autres sont agnostiques ou athées.
Selon l'un, le christianisme serait arrivé au Pays basque aux IVe et Ve siècles mais selon l'autre, il n'aurait eu lieu qu'aux XIIe et XIIIe siècles.
En ce sens, le christianisme est arrivé « tôt ».
Selon une tradition, elle voyageait tous les sept ans entre une grotte du mont Anboto et une autre sur une autre montagne (les histoires varient) ; le temps était humide quand elle était à Anboto, sec quand elle était à Aloña, ou Supelegor, ou Gorbea.
On dit que lorsqu'ils se rassemblaient dans les hautes grottes des pics sacrés, ils engendraient les tempêtes.
Les légendes parlent aussi de génies nombreux et abondants, comme les jentilak (équivalents des géants), les lamiak (équivalents des nymphes), les mairuak (constructeurs des cromlechs ou cercles de pierres, littéralement Maures), les iratxoak (lutins), les sorginak (sorcières, prêtresse de Mari ), et ainsi de suite.
Il y a un filou nommé San Martin Txiki ("St Martin le moindre").
Les jentilak ("Géants"), quant à eux, sont un peuple légendaire qui explique la disparition d'un peuple de culture de l'âge de pierre qui vivait dans les hautes terres et qui n'avait aucune connaissance du fer.
Pendant plus d'un siècle, les universitaires ont largement débattu du statut élevé des femmes basques dans les codes juridiques, ainsi que de leurs positions en tant que juges, héritières et arbitres à travers les temps pré-romains, médiévaux et modernes.
La Navarre a un statut d'autonomie distinct, un arrangement controversé conçu lors de la transition espagnole vers la démocratie (l'Amejoramiento, une «mise à niveau» de son statut antérieur pendant la dictature).
Les questions d'appartenance et d'identité politique, linguistique et culturelle sont très complexes en Navarre.
La majorité des écoles sous la juridiction du système éducatif basque utilisent le basque comme principal moyen d'enseignement.
En revanche, le désir d'une plus grande autonomie ou indépendance est particulièrement courant chez les nationalistes basques de gauche.
Ils se considèrent comme culturellement et surtout linguistiquement distincts de leurs voisins environnants.
Miguel de Unamuno était un romancier et philosophe réputé de la fin du XIXe et du XXe siècle.
Il a également fondé l'Association syndicale chilienne pour promouvoir un mouvement syndical basé sur les enseignements sociaux de l'Église catholique.
La présence historique des San au Botswana est particulièrement évidente dans la région des collines de Tsodilo, au nord du Botswana.
Des années 1950 aux années 1990, les communautés San sont passées à l'agriculture en raison des programmes de modernisation mandatés par le gouvernement.
Certains groupes San sont l'un des 14 "groupes de population ancestrale" connus; c'est-à-dire "des groupes de populations ayant une ascendance génétique commune, qui partagent une ethnie et des similitudes à la fois dans leur culture et dans les propriétés de leurs langues".
Les représentants des peuples San en 2003 ont déclaré leur préférence pour l'utilisation de tels noms de groupes individuels lorsque cela était possible par rapport à l'utilisation du terme collectif San.
J'ai continué à utiliser Bushman, et j'ai été publiquement corrigé plusieurs fois par les justes.
Au lieu de cela, le représentant du Conseil San a été catégorique sur le fait qu'aucun mal ou préjudice n'a été causé à eux ou à la communauté San avec la manière dont (Die Burger) a publié le mot 'boesman'."
La parenté San est comparable à la parenté Eskimo, avec le même ensemble de termes que dans les cultures européennes, mais utilise également une règle de nom et une règle d'âge.
Les enfants n'ont pas d'obligations sociales à part jouer, et les loisirs sont très importants pour les San de tous âges.
Ils prennent d'importantes décisions familiales et collectives et revendiquent la propriété des points d'eau et des zones d'alimentation.
Les sécheresses peuvent durer plusieurs mois et les points d'eau peuvent s'assécher.
Dans ce trou est insérée une longue tige d'herbe creuse.
Le début du printemps est la saison la plus dure : une période chaude et sèche après l'hiver frais et sec.
Les femmes ramassent des fruits, des baies, des tubercules, des oignons de brousse et d'autres matières végétales pour la consommation de la bande.
Selon l'endroit, les San consomment de 18 à 104 espèces, dont des sauterelles, des coléoptères, des chenilles, des papillons de nuit, des papillons et des termites.
Ces haplogroupes sont des sous-groupes spécifiques des haplogroupes A et B, les deux premières branches de l'arbre du chromosome Y humain.
L'haplogroupe mitochondrial le plus divergent (le plus ancien), L0d, a été identifié à ses fréquences les plus élevées dans les groupes San d'Afrique australe.
Les San ont été particulièrement touchés par l'empiètement des peuples majoritaires et des agriculteurs non autochtones sur les terres traditionnellement occupées par les San.
La perte de terres est un facteur majeur des problèmes auxquels sont confrontés les peuples autochtones du Botswana, notamment l'expulsion des San de la réserve de gibier du Kalahari central.
Cela accorderait des redevances aux San pour les avantages de leurs connaissances indigènes.
Van der Post a grandi en Afrique du Sud et a eu une fascination respectueuse pour les cultures indigènes africaines.
Poussé par une fascination de toujours pour cette "tribu disparue", Van der Post a publié un livre de 1958 sur cette expédition, intitulé The Lost World of the Kalahari.
Son premier film The Hunters, sorti en 1957, montre une chasse à la girafe.
Sa sœur Elizabeth Marshall Thomas a écrit plusieurs livres et de nombreux articles sur les San, basés en partie sur ses expériences de vie avec ces personnes lorsque leur culture était encore intacte.
Cela a été revu par Lawrence Van Gelder pour le New York Times , qui a déclaré que le film "constitue un acte de préservation et un requiem".
La série The Life of Mammals (2003) de la BBC comprend des séquences vidéo d'un San indigène du désert du Kalahari entreprenant une chasse persistante d'un koudou dans des conditions désertiques difficiles.
En raison de leurs similitudes, les œuvres San peuvent illustrer les raisons des anciennes peintures rupestres.
Le film a été réalisé par Jamie Uys, qui est revenu aux San une décennie plus tard avec The Gods Must Be Crazy, qui s'est avéré être un succès international.
The Covenant (1980) de James A. Michener est une œuvre de fiction historique centrée sur l'Afrique du Sud.
Le roman Mating de Norman Rush de 1991 présente un campement de Basarwa près de la ville (ibaminary) du Botswana où se déroule l'action principale.
En 2007, David Gilman publie The Devil's Breath.
Le fiancé du protagoniste de The No.
Les peuples germaniques étaient un groupe historique de personnes vivant en Europe centrale et en Scandinavie.
Dans les discussions sur la période romaine, les peuples germaniques sont parfois appelés Germani ou anciens Allemands, bien que de nombreux chercheurs considèrent le deuxième terme comme problématique, car il suggère une identité avec les Allemands modernes.
En revanche, les auteurs romains ont décrit pour la première fois les peuples germaniques près du Rhin au moment où l'Empire romain a établi sa domination dans cette région.
Les efforts romains pour intégrer la vaste zone entre le Rhin et l'Elbe ont pris fin vers 16 CE, à la suite de la défaite romaine majeure à la bataille de la forêt de Teutoburg en 9 CE.
Au IIIe siècle, les Goths germanophones dominent la steppe pontique, en dehors de la Germanie, et lancent une série d'expéditions maritimes dans les Balkans et l'Anatolie jusqu'à Chypre.
L'archéologie montre plutôt une société et une économie complexes dans toute la Germanie.
Traditionnellement, les peuples germaniques ont été considérés comme possédant un droit dominé par les concepts de querelle et de compensation du sang.
Les anciens peuples germanophones partageaient probablement une tradition poétique commune, des vers allitératifs, et plus tard, les peuples germaniques partageaient également des légendes originaires de la période de migration.
Même la langue dont il dérive est un sujet de dispute, avec des propositions d'origine germanique, celtique, latine et illyrienne.
Indépendamment de sa langue d'origine, le nom a été transmis aux Romains via des locuteurs celtiques.
À la fin de l'Antiquité, seuls les peuples riverains du Rhin, en particulier les Francs, et parfois les Alamans, étaient appelés Germani par les écrivains latins ou grecs.
Alors que les auteurs romains n'excluaient pas systématiquement les personnes de langue celtique ou ne traitaient pas les peuples germaniques comme le nom d'un peuple, cette nouvelle définition, en utilisant la langue germanique comme critère principal, comprenait les Germani comme un peuple ou une nation avec une identité de groupe stable. lié au langage.
Certains chercheurs qui étudient le haut Moyen Âge insistent désormais sur la question de savoir si les peuples germaniques se considéraient comme une unité ethnique, tandis que d'autres soulignent l'existence des langues germaniques comme un fait historique pouvant être utilisé pour identifier les peuples germaniques, qu'ils aient vu ou non eux-mêmes comme "germaniques".
Pour de telles raisons, Goffart soutient que le terme germanique devrait être entièrement évité en faveur de «barbare», sauf au sens linguistique, et des historiens tels que Walter Pohl ont également appelé à ce que le terme soit évité ou utilisé avec une explication minutieuse.
Dans le récit de César, la caractéristique déterminante la plus claire du peuple Germani était qu'il vivait à l'est du Rhin, en face de la Gaule du côté ouest, une observation qu'il a faite avec des digressions historiques dans ses écrits.
Tacite était parfois incertain si un peuple était germanique ou non, exprimant son incertitude sur les Bastarnae, qui, selon lui, ressemblaient à des Sarmates mais parlaient comme les Germani, sur les Osi et les Cotini, et sur les Aesti, qui étaient comme Suebi mais parlaient. une langue différente.
Le Haut Danube servait de frontière sud.
On ne sait pas si ces Germani parlaient une langue germanique, et ils étaient peut-être des locuteurs celtiques à la place.
Tacite continue de mentionner les tribus germaniques de la rive ouest du Rhin au début de l'Empire, telles que les Tungri, les Némètes, les Ubii et les Batavi.
Inspirés par cela, ces trois groupes sont aussi parfois utilisés dans la terminologie linguistique moderne plus ancienne, tentant de décrire les divisions des langues germaniques ultérieures.)
Les Herminones ou Hermiones à l'intérieur, comprenaient les Suevi, les Hermunduri, les Chatti, les Cherusci selon Pline.
D'autre part, Tacite a écrit dans le même passage que certains pensent qu'il existe d'autres groupes qui sont tout aussi anciens que ces trois, dont "les Marsi, Gambrivii, Suevi, Vandilii".
Strabon, qui s'est concentré principalement sur les Germains entre l'Elbe et le Rhin, et ne mentionne pas les fils de Mannus, a également séparé les noms des Germains qui ne sont pas Suèves, en deux autres groupes, impliquant de même trois divisions principales : "petites tribus allemandes, comme les Cherusci, Chatti, Gamabrivi, Chattuarii, et à côté de l'océan les Sicambri, Chaubi, Bructeri, Cimbri, Cauci, Caulci, Campsiani".
Au cours de la période linguistique pré-germanique (2500–500 avant notre ère), la proto-langue a presque certainement été influencée par des substrats linguistiques encore perceptibles dans la phonologie et le lexique germaniques.
Il y a aussi beaucoup d'influence dans le vocabulaire des langues celtiques, mais la plupart de cela semble être beaucoup plus tardif, la plupart des emprunts se produisant avant ou pendant le changement de son décrit par la loi de Grimm.
Bien que le proto-germanique soit reconstruit sans dialectes via la méthode comparative, il est presque certain qu'il n'a jamais été une proto-langue uniforme.
Les premières inscriptions runiques attestées ( peigne Vimose , fer de lance Øvre Stabu ), initialement concentrées dans le Danemark moderne et écrites avec le système Elder Futhark , sont datées de la seconde moitié du IIe siècle de notre ère.
Cependant, la fusion des voyelles proto-germaniques non accentuées, attestée dans les inscriptions runiques des 4e et 5e siècles de notre ère, suggère également que le norrois primitif n'aurait pas pu être un prédécesseur direct des dialectes germaniques occidentaux.
À la fin du IIIe siècle de notre ère, des divergences linguistiques comme la perte germanique occidentale de la consonne finale -z s'étaient déjà produites dans le continuum dialectal «résiduel» du Nord-Ouest.
L'inclusion des langues bourguignonne et vandalique dans le groupe germanique oriental, bien que plausible, est encore incertaine en raison de leur faible attestation.
Une société est un groupe d'individus impliqués dans une interaction sociale persistante, ou un grand groupe social partageant le même territoire spatial ou social, généralement soumis à la même autorité politique et aux mêmes attentes culturelles dominantes.
Les sociétés construisent des modèles de comportement en considérant certaines actions ou certains discours comme acceptables ou inacceptables.
Dans la mesure où elle est collaborative, une société peut permettre à ses membres de bénéficier d'une manière qui serait autrement difficile sur une base individuelle ; les avantages individuels et sociaux (communs) peuvent ainsi être distingués ou, dans de nombreux cas, se chevaucher.
C'était à son tour du mot latin societas , qui à son tour était dérivé du nom socius ("camarade, ami, allié"; forme adjectivale socialis ) utilisé pour décrire un lien ou une interaction entre des parties amicales, ou du moins civiles.
Dans les années 1630, il était utilisé en référence aux "personnes liées par le voisinage et les relations sexuelles conscientes de vivre ensemble dans une communauté ordonnée".
Ces structures peuvent avoir divers degrés de pouvoir politique, selon les environnements culturels, géographiques et historiques auxquels ces sociétés doivent faire face.
Sociétés tribales dans lesquelles il existe quelques exemples limités de rang social et de prestige.
Cette évolution culturelle a un effet profond sur les modèles de communauté.
Les villes se sont transformées en cités-États et en États-nations.
À l'inverse, les membres d'une société peuvent également éviter ou bouc émissaire tout membre de la société qui viole ses normes.
Certaines sociétés accordent un statut à un individu ou à un groupe de personnes lorsque cet individu ou ce groupe accomplit une action admirée ou souhaitée.
Bien que les humains aient établi de nombreux types de sociétés au cours de l'histoire, les anthropologues ont tendance à classer différentes sociétés en fonction du degré auquel différents groupes au sein d'une société ont un accès inégal à des avantages tels que les ressources, le prestige ou le pouvoir.
Cependant, certaines sociétés de chasse et de cueillette dans des zones aux ressources abondantes (comme les gens de tlingit) vivaient en groupes plus importants et formaient des structures sociales hiérarchisées complexes telles que la chefferie.
Les statuts au sein de la tribu sont relativement égaux et les décisions sont prises par accord général.
Il n'y a pas de fonctions politiques contenant un pouvoir réel, et un chef n'est qu'une personne influente, une sorte de conseiller ; par conséquent, les consolidations tribales pour l'action collective ne sont pas gouvernementales.
Parce que leur approvisionnement alimentaire est beaucoup plus fiable, les sociétés pastorales peuvent soutenir des populations plus importantes.
Par exemple, certaines personnes deviennent des artisans, fabriquant des outils, des armes et des bijoux, entre autres objets de valeur.
Ces familles acquièrent souvent du pouvoir grâce à leur richesse accrue.
La végétation sauvage est coupée et brûlée, et les cendres sont utilisées comme engrais.
Ils peuvent retourner sur leur terre d'origine plusieurs années plus tard et recommencer le processus.
La taille de la population d'un village dépend de la quantité de terres disponibles pour l'agriculture ; ainsi, les villages peuvent aller d'aussi peu que 30 personnes à 2000.
Les sociologues utilisent l'expression révolution agricole pour désigner les changements technologiques qui se sont produits il y a 8 500 ans et qui ont conduit à cultiver des cultures et à élever des animaux de ferme.
Des degrés plus élevés de stratification sociale sont apparus dans les sociétés agraires.
Cependant, à mesure que les magasins d'alimentation s'amélioraient et que les femmes assumaient un rôle moins important dans la fourniture de nourriture à la famille, elles devenaient de plus en plus subordonnées aux hommes.
Un système de dirigeants au statut social élevé est également apparu.
L'exploration des Amériques par l'Europe a servi d'impulsion au développement du capitalisme.
Cela a produit d'autres augmentations spectaculaires de l'efficacité.
Cet excédent plus important a rendu encore plus prononcés tous les changements évoqués plus tôt dans la révolution de la domestication.
Cependant, l'inégalité est devenue encore plus grande qu'auparavant.
Géographiquement, il couvre au moins les pays d'Europe occidentale, d'Amérique du Nord, d'Australie et de Nouvelle-Zélande.
L'un des domaines d'intérêt de l'Union européenne est la société de l'information.
Certaines associations universitaires, professionnelles et scientifiques se décrivent comme des sociétés (par exemple, l'American Mathematical Society, l'American Society of Civil Engineers ou la Royal Society).
Une communauté est une unité sociale (un groupe d'êtres vivants) avec des points communs tels que les normes, la religion, les valeurs, les coutumes ou l'identité.
En ce sens, il est synonyme du concept d'ancienne colonie - qu'il s'agisse d'un hameau, d'un village, d'une ville ou d'une ville.
La plupart des reconstructions de communautés sociales par les archéologues reposent sur le principe que l'interaction sociale dans le passé était conditionnée par la distance physique.
Aucun groupe n'est exclusivement l'un ou l'autre.
La socialisation est principalement influencée par la famille, à travers laquelle les enfants apprennent d'abord les normes communautaires.
Les praticiens du développement communautaire doivent comprendre à la fois comment travailler avec les individus et comment influer sur les positions des communautés dans le contexte d'institutions sociales plus larges.
À l'intersection entre le développement communautaire et le développement communautaire se trouvent un certain nombre de programmes et d'organismes dotés d'outils de développement communautaire.
Vide : va au-delà des tentatives de réparation, de guérison et de conversion de l'étape du chaos, lorsque toutes les personnes deviennent capables de reconnaître leurs propres blessures et brisures, communes aux êtres humains.
Les trois types de base d'organisation communautaire sont l'organisation de base, la formation de coalitions et « l'organisation communautaire basée sur l'institution » (également appelée « organisation communautaire élargie », dont un exemple est l'organisation communautaire basée sur la foi ou la communauté basée sur la congrégation). Organisation).
Consulté le : 22 juin 2008.
L'organisation communautaire peut se concentrer sur plus que la simple résolution de problèmes spécifiques.
Ces groupes facilitent et encouragent la prise de décision par consensus en mettant l'accent sur la santé générale de la communauté plutôt que sur un groupe d'intérêt spécifique.
Communautés basées sur l'identité : gamme de la clique locale, de la sous-culture, du groupe ethnique, de la civilisation religieuse, multiculturelle ou pluraliste, ou des cultures communautaires mondiales d'aujourd'hui.
Les relations entre les membres d'une communauté virtuelle ont tendance à se concentrer sur l'échange d'informations sur des sujets spécifiques.
Les érudits en sciences humaines sont des « érudits en sciences humaines » ou humanistes.
Les sciences humaines étudient généralement les traditions locales, à travers leur histoire, leur littérature, leur musique et leurs arts, en mettant l'accent sur la compréhension d'individus, d'événements ou d'époques particuliers.
L'anthropologie (comme certains domaines de l'histoire) ne rentre pas facilement dans l'une de ces catégories, et différentes branches de l'anthropologie puisent dans un ou plusieurs de ces domaines.
Le mot anthropos ( άνθρωπος ) vient du mot grec pour « être humain » ou « personne ».
Cela signifie que, bien que les anthropologues se spécialisent généralement dans un seul sous-domaine, ils gardent toujours à l'esprit les aspects biologiques, linguistiques, historiques et culturels de tout problème.
La quête de l'holisme conduit la plupart des anthropologues à étudier un peuple en détail, en utilisant des données biogénétiques, archéologiques et linguistiques parallèlement à l'observation directe des coutumes contemporaines.
L'archéologie peut être considérée à la fois comme une science sociale et comme une branche des sciences humaines.
Une bonne partie de la philosophie des XXe et XXIe siècles a été consacrée à l'analyse du langage et à la question de savoir si, comme le prétendait Wittgenstein, bon nombre de nos confusions philosophiques dérivent du vocabulaire que nous utilisons ; la théorie littéraire a exploré les caractéristiques rhétoriques, associatives et ordonnées du langage ; et les linguistes historiques ont étudié le développement des langues à travers le temps.
Il a été défini comme un "système de règles", comme un "concept interprétatif" pour réaliser la justice, comme une "autorité" pour arbitrer les intérêts des gens, et même comme "l'ordre d'un souverain, soutenu par la menace d'une sanction" .
Les lois sont politiques, parce que les politiciens les créent.
Comme l'a noté Immanuel Kant, "la philosophie de la Grèce antique était divisée en trois sciences : la physique, l'éthique et la logique".)
Le shintoïsme, le taoïsme et les autres religions folkloriques ou naturelles n'ont pas de codes éthiques.
Les systèmes de croyance impliquent un modèle logique que les religions n'affichent pas en raison de leurs contradictions internes, de leur manque de preuves et de leurs mensonges. .
Ils sont nécessaires pour comprendre la situation humaine.
Les religions non fondatrices sont l'hindouisme, le shinto et les religions indigènes ou folkloriques.
Lorsque les religions traditionnelles échouent à répondre aux nouvelles préoccupations, de nouvelles religions émergent.
Les arts de la scène sont également soutenus par des travailleurs dans des domaines connexes, tels que l'écriture de chansons et la scénographie.
C'est ce qu'on appelle l'art de la performance.
La danse est également utilisée pour décrire les méthodes de communication non verbale (voir langage corporel) entre humains ou animaux (danse des abeilles, danse d'accouplement) et le mouvement d'objets inanimés (les feuilles dansent dans le vent).
Dans l'art byzantin et gothique du Moyen Âge, la prédominance de l'Église insistait sur l'expression de vérités bibliques et non matérielles.
Une caractéristique de ce style est que la couleur locale est souvent définie par un contour (un équivalent contemporain est le dessin animé).
Cela implique généralement de faire des marques sur une surface en appliquant une pression à partir d'un outil ou en déplaçant un outil sur une surface.
Cependant, lorsqu'il est utilisé dans un sens artistique, cela signifie l'utilisation de cette activité en combinaison avec le dessin, la composition et d'autres considérations esthétiques afin de manifester l'intention expressive et conceptuelle du praticien.
Le noir est associé au deuil en Occident, mais ailleurs le blanc peut l'être.
Le mot "rouge", par exemple, peut couvrir une large gamme de variations sur le rouge pur du spectre.
Cela a commencé avec le cubisme et n'est pas la peinture au sens strict.
Par conséquent, beaucoup passent les premières années après l'obtention de leur diplôme à décider quoi faire ensuite, ce qui se traduit par des revenus inférieurs au début de leur carrière; pendant ce temps, les diplômés des programmes axés sur la carrière entrent plus rapidement sur le marché du travail.
Cependant, les preuves empiriques montrent également que les diplômés en sciences humaines gagnent toujours des revenus nettement plus élevés que les travailleurs sans formation postsecondaire et ont des niveaux de satisfaction au travail comparables à ceux de leurs pairs d'autres domaines.
Cependant, en pourcentage du type de diplômes délivrés, les sciences humaines semblent décliner.
Le financement fédéral représente une fraction beaucoup plus petite du financement des sciences humaines que d'autres domaines tels que les STEM ou la médecine.
Cette compréhension, selon eux, relie les personnes partageant les mêmes idées et issues de milieux culturels similaires et offre un sentiment de continuité culturelle avec le passé philosophique.
Outre son application sociétale, l'ibamination narrative est un outil important dans la (re)production du sens compris dans l'histoire, la culture et la littérature.
Le poststructuralisme a problématisé une approche de l'étude humaniste basée sur des questions de sens, d'intentionnalité et de paternité.
De plus, la pensée critique, bien qu'elle soit sans doute le résultat d'une formation humaniste, peut être acquise dans d'autres contextes.
Un tel plaisir contraste avec la privatisation croissante des loisirs et la gratification instantanée caractéristiques de la culture occidentale ; elle répond ainsi aux exigences de Jürgen Habermas de mépris du statut social et de problématisation rationnelle de domaines auparavant incontestés nécessaires à une entreprise qui se déroule dans la sphère publique bourgeoise.
Malgré de nombreux arguments basés sur les sciences humaines contre les sciences humaines, certains au sein des sciences exactes ont appelé à leur retour.
C'est bien de connaître l'histoire de la philosophie.
La communication (du latin communicare, qui signifie "partager" ou "être en relation avec") est "une réponse apparente aux divisions douloureuses entre soi et l'autre, privé et public, et pensée intérieure et parole extérieure".
Composition du message (élaboration interne ou technique supplémentaire sur ce qu'il faut exprimer exactement).
Les sources de bruit telles que les forces naturelles et, dans certains cas, l'activité humaine (à la fois intentionnelle et accidentelle) commencent à influencer la qualité des signaux se propageant de l'émetteur à un ou plusieurs récepteurs.
Interprétation et sens du message d'origine présumé.
Des exemples d'intention sont des mouvements volontaires, intentionnels comme serrer la main ou un clin d'œil, ainsi que des mouvements involontaires, comme la transpiration.
De même, les textes écrits incluent des éléments non verbaux tels que le style d'écriture, la disposition spatiale des mots et l'utilisation d'émoticônes pour transmettre des émotions.
Certaines des fonctions de la communication non verbale chez l'homme sont de compléter et d'illustrer, de renforcer et d'accentuer, de remplacer et de substituer, de contrôler et de réguler et de contredire le message dénotatif.
Pour avoir une communication totale, tous les canaux non verbaux tels que le corps, le visage, la voix, l'apparence, le toucher, la distance, le timing et d'autres forces environnementales doivent être engagés lors de l'interaction en face à face.
"Les comportements non verbaux peuvent former un système de langage universel."
L'apprentissage des langues se produit normalement de manière plus intensive pendant l'enfance humaine.
Les langages construits tels que l'espéranto, les langages de programmation et divers formalismes mathématiques ne sont pas nécessairement limités aux propriétés partagées par les langages humains.
Les propriétés du langage sont régies par des règles.
Contrairement à la croyance populaire, les langues des signes du monde (par exemple, la langue des signes américaine) sont considérées comme une communication verbale parce que leur vocabulaire des signes, leur grammaire et d'autres structures linguistiques respectent toutes les classifications nécessaires en tant que langues parlées.
La communication est donc un processus par lequel le sens est attribué et transmis dans le but de créer une compréhension partagée.
Un canal auquel les signaux sont adaptés pour la transmission.
Une destination, où le message arrive.
Aucune indemnité à des fins différentes.
Aucune tolérance pour les contextes situationnels.
Ces actes peuvent prendre plusieurs formes, dans l'un des divers modes de communication.
Syntactique (propriétés formelles des signes et symboles).
À la lumière de ces faiblesses, Barnlund (2008) a proposé un modèle transactionnel de communication.
Cette deuxième attitude de communication, appelée modèle constitutif ou vision constructionniste, se concentre sur la manière dont un individu communique comme facteur déterminant de la manière dont le message sera interprété.
Les filtres personnels de l'expéditeur et les filtres personnels du destinataire peuvent varier en fonction des traditions régionales, des cultures ou du sexe ; qui peuvent altérer la signification voulue du contenu du message.
Bien que quelque chose comme des livres de codes soit impliqué par le modèle, ils ne sont représentés nulle part dans le modèle, ce qui crée de nombreuses difficultés conceptuelles.
Les entreprises disposant de ressources limitées peuvent choisir de ne s'engager que dans quelques-unes de ces activités, tandis que les grandes organisations peuvent utiliser un éventail complet de communications.
L'environnement de l'information est l'ensemble des individus, des organisations et des systèmes qui collectent, traitent, diffusent ou agissent sur l'information.
Dans la communication interpersonnelle verbale, il existe deux types de messages envoyés : un message de contenu et un message relationnel.
Il s'agit de l'étude de la façon dont les individus expliquent ce qui cause différents événements et comportements.
Une communication ouverte et honnête crée une atmosphère qui permet aux membres de la famille d'exprimer leurs différences ainsi que l'amour et l'admiration les uns pour les autres.
Les chercheurs élaborent des théories pour comprendre les comportements de communication.
Cela inclut également un manque d'expression d'une communication "appropriée aux connaissances", qui se produit lorsqu'une personne utilise des termes juridiques ambigus ou complexes, un jargon médical ou des descriptions d'une situation ou d'un environnement qui n'est pas compris par le destinataire.
De même, un équipement médiocre ou obsolète, en particulier l'incapacité de la direction à introduire de nouvelles technologies, peut également causer des problèmes.
Les exemples peuvent inclure une structure organisationnelle qui n'est pas claire et rend donc difficile de savoir avec qui communiquer.
Il est préférable d'éviter ces mots en utilisant des alternatives chaque fois que possible.
Cependant, la recherche en communication a montré que la confusion peut conférer une légitimité à la recherche lorsque la persuasion échoue.
C'est lorsque l'expéditeur exprime une pensée ou un mot mais que le destinataire lui donne un sens différent.
Cela a, à son tour, conduit à un changement notable dans la façon dont les jeunes générations communiquent et perçoivent leur propre efficacité à communiquer et à se connecter avec les autres.
Peur d'être critiqué - C'est un facteur majeur qui empêche une bonne communication.
Cela renforcera non seulement votre confiance en vous, mais améliorera également votre langue et votre vocabulaire.
Certaines attitudes peuvent aussi rendre la communication difficile.
L'acte de désambiguïsation concerne la tentative de réduire le bruit et les mauvaises interprétations, lorsque la valeur sémantique ou la signification d'un signe peut être sujette au bruit, ou en présence de plusieurs significations, ce qui rend difficile la création de sens.
Par exemple : les mots, les couleurs et les symboles ont des significations différentes selon les cultures.
Comprendre les aspects culturels de la communication fait référence à la connaissance de différentes cultures afin de communiquer efficacement avec des personnes interculturelles.
Il comprend également des sons de gorge et tous ceux-ci sont fortement influencés par les différences culturelles à travers les frontières.
Ce concept diffère d'une culture à l'autre car l'espace autorisé varie selon les pays.
Certains problèmes expliquant ce concept sont les pauses, les silences et le délai de réponse lors d'une interaction.
Dans différents pays, les mêmes gestes et postures sont utilisés pour véhiculer des messages différents.
Les racines des plantes communiquent avec les bactéries, les champignons et les insectes du rhizome dans le sol.
En parallèle ils produisent d'autres volatils pour attirer les parasites qui attaquent ces herbivores.
Les produits biochimiques déclenchent une réaction spécifique de l'organisme fongique, tandis que si les mêmes molécules chimiques ne font pas partie des messages biotiques, elles ne déclenchent pas la réaction de l'organisme fongique.
Grâce à la détection du quorum, les bactéries peuvent détecter la densité des cellules et réguler l'expression des gènes en conséquence.
L'information, au sens général, est une donnée traitée, organisée et structurée.
Les informations sont associées aux données.
L'information peut être transmise dans le temps, via le stockage de données, et dans l'espace, via la communication et les télécommunications.
Les informations peuvent être codées sous diverses formes pour la transmission et l'interprétation (par exemple, les informations peuvent être codées dans une séquence de signes ou transmises via un signal).
L'incertitude est inversement proportionnelle à la probabilité d'occurrence.
De plus, le latin lui-même contenait déjà le mot īnfōrmātiō signifiant concept ou idée, mais la mesure dans laquelle cela a pu influencer le développement du mot information en anglais n'est pas claire.
En grec moderne, le mot Πληροφορία est encore utilisé quotidiennement et a le même sens que le mot information en anglais.
Le domaine a été fondamentalement établi par les travaux de Harry Nyquist et Ralph Hartley dans les années 1920, et de Claude Shannon dans les années 1940.
L'entropie quantifie la quantité d'incertitude impliquée dans la valeur d'une variable aléatoire ou le résultat d'un processus aléatoire.
Les sous-domaines importants de la théorie de l'information comprennent le codage de source, la théorie de la complexité algorithmique, la théorie de l'information algorithmique et la sécurité de la théorie de l'information.
Dans son livre Sensory Ecology, le biophysicien David B. Dusenbery a appelé ces entrées causales.
En pratique, l'information est généralement portée par des stimuli faibles qui doivent être détectés par des systèmes sensoriels spécialisés et amplifiés par des apports d'énergie avant qu'ils ne puissent être fonctionnels pour l'organisme ou le système.
La séquence de nucléotides est un modèle qui influence la formation et le développement d'un organisme sans aucun besoin d'un esprit conscient.
En d'autres termes, on peut dire que l'information dans ce sens est quelque chose de potentiellement perçu comme une représentation, bien qu'elle ne soit pas créée ou présentée à cette fin.
Que la réponse fournisse des connaissances dépend de la personne informée.
C'est l'équivalent informationnel de près de 61 CD-ROM par personne en 2007.
Une saine gestion des documents garantit que l'intégrité des documents est préservée aussi longtemps qu'ils sont nécessaires.
Beynon-Davies explique le concept à multiples facettes de l'information en termes de signes et de systèmes signal-signal.
La pragmatique s'intéresse au but de la communication.
En d'autres termes, la pragmatique relie le langage à l'action.
La sémantique est l'étude de la signification des signes - l'association entre les signes et le comportement.
La syntaxe en tant que domaine étudie la forme de communication en termes de logique et de grammaire des systèmes de signes.
Il introduit le concept de coût de l'information lexicographique et fait référence à l'effort qu'un utilisateur d'un dictionnaire doit faire pour d'abord trouver, puis comprendre des données afin qu'elles puissent générer des informations.
Dans une situation de communication, les intentions sont exprimées à travers des messages qui comprennent des ensembles de signes interdépendants tirés d'un langage mutuellement compris par les agents impliqués dans la communication.
La visualisation de l'information (abrégé en InfoVis) dépend du calcul et de la représentation numérique des données, et assiste les utilisateurs dans la reconnaissance des formes et la détection des anomalies.
Le terme est généralement employé en sociologie et dans les autres sciences sociales ainsi qu'en philosophie et en bioéthique.
Dans les sociétés en développement, il peut être principalement basé sur la parenté et les valeurs partagées, tandis que les sociétés plus développées accumulent diverses théories sur ce qui contribue à un sentiment de solidarité, ou plutôt de cohésion sociale.
Durkheim a introduit les termes de solidarité mécanique et organique dans le cadre de sa théorie du développement des sociétés dans La division du travail dans la société (1893).
Dictionnaire Collins de sociologie, p405-6.
Définition : c'est la cohésion sociale fondée sur la dépendance que les individus ont les uns des autres dans les sociétés plus avancées.
Les premiers philosophes de l'Antiquité tels que Socrate et Aristote discutent de la solidarité comme cadre de l'éthique de la vertu, car pour vivre une bonne vie, il faut accomplir des actions et se comporter de manière solidaire avec la communauté.
La pratique moderne de la bioéthique est fortement influencée par le concept d'impératif catégorique d'Emmanuel Kant.
Les études à l'étranger étaient pratiquement inexistantes.
Les premiers sont devenus les défenseurs des études régionales, les seconds les partisans de la théorie de la modernisation.
De 1953 à 1966, il a versé 270 millions de dollars à 34 universités pour des études régionales et linguistiques.
D'autres programmes importants et importants ont suivi celui de Ford.
D'autres ont cependant insisté sur le fait qu'une fois établies sur les campus universitaires, les études régionales ont commencé à englober un programme intellectuel beaucoup plus large et plus profond que celui prévu par les agences gouvernementales, donc non centré sur l'Amérique.
D'autres domaines de recherche interdisciplinaires tels que les études sur les femmes, les études de genre, les études sur le handicap, les études LGBT et les études ethniques (y compris les études afro-américaines, les études asiatiques américaines, les études latino-américaines, les études chicanos et les études amérindiennes) ne font pas partie des études régionales mais sont parfois inclus. en discussion avec lui.
La démographie (du préfixe demo- du grec ancien δῆμος (dēmos) signifiant 'le peuple', et -graphie de γράφω (graphō) signifiant 'écriture, description ou mesure') est l'étude statistique des populations, en particulier des êtres humains.
Les données démographiques des patients constituent le cœur des données de tout établissement médical, telles que les informations de contact des patients et d'urgence et les données des dossiers médicaux des patients.
Le terme démographie fait référence à l'étude globale de la population.
Au Moyen Âge, les penseurs chrétiens ont consacré beaucoup de temps à réfuter les idées classiques sur la démographie.
L'une des premières études démographiques de la période moderne était Natural and Political Observations Made on the Bills of Mortality (1662) de John Graunt, qui contient une forme primitive de table de mortalité.
Son travail a influencé Thomas Robert Malthus, qui, écrivant à la fin du XVIIIe siècle, craignait que, si elle n'était pas maîtrisée, la croissance démographique ait tendance à dépasser la croissance de la production alimentaire, entraînant une famine et une pauvreté toujours croissantes (voir Catastrophe malthusienne).
Le recensement est l'autre méthode directe courante de collecte de données démographiques.
Des analyses sont effectuées après un recensement pour estimer l'ampleur du surdénombrement ou du sous-dénombrement.
D'autres méthodes indirectes de la démographie contemporaine consistent à interroger les gens sur les frères et sœurs, les parents et les enfants.
Ils comprennent des modèles de mortalité (y compris la table de mortalité, les modèles de Gompertz, les modèles à risques, les modèles à risques proportionnels de Cox, les tables de mortalité à décroissance multiple, les logits relationnels de Brass), la fécondité (modèle de Hernes, les modèles de Coale-Trussell, les ratios de progression de la parité), le mariage (Singulate Moyenne au mariage, modèle de Page), invalidité (méthode de Sullivan, tables de mortalité multi-états), projections démographiques (modèle de Lee-Carter, matrice de Leslie) et dynamique démographique (Keyfitz).
Les taux de fécondité par âge, le nombre annuel de naissances vivantes pour 1 000 femmes dans des groupes d'âge particuliers (généralement 15-19 ans, 20-24 ans, etc.)
L'espérance de vie (ou l'espérance de vie), le nombre d'années qu'un individu à un âge donné peut espérer vivre aux niveaux de mortalité actuels.
Une population stationnaire, à la fois stable et de taille constante (la différence entre le taux brut de natalité et le taux brut de mortalité est nulle).
Notez que le taux brut de mortalité tel que défini ci-dessus et appliqué à l'ensemble de la population peut donner une impression trompeuse.
Les personnes qui changent d'auto-étiquette ethnique ou dont la classification ethnique dans les statistiques gouvernementales changent avec le temps peuvent être considérées comme migrant ou passant d'une sous-catégorie de population à une autre.
La figure de cette section montre les dernières projections (2004) de la population mondiale de l'ONU jusqu'en 2150 (rouge = élevé, orange = moyen, vert = faible).
La mortalité est l'étude des causes, des conséquences et de la mesure des processus affectant la mort des membres de la population.
Les chercheurs en migration ne désignent pas les mouvements comme des « migrations » à moins qu'ils ne soient quelque peu permanents.
La démographie est aujourd'hui largement enseignée dans de nombreuses universités à travers le monde, attirant des étudiants ayant une formation initiale en sciences sociales, statistiques ou études de santé.
À cet égard, on peut voir les sciences de l'information comme une réponse au déterminisme technologique, la croyance que la technologie "se développe selon ses propres lois, qu'elle réalise son propre potentiel, limitée uniquement par les ressources matérielles disponibles et la créativité de ses développeurs.
Elle concerne l'ensemble des connaissances relatives à l'origine, la collecte, l'organisation, le stockage, la récupération, l'interprétation, la transmission, la transformation et l'utilisation de l'information.
Cela est particulièrement vrai lorsqu'il est lié au concept développé par AI Mikhailov et d'autres auteurs soviétiques au milieu des années 1960.
Des définitions dépendant de la nature des outils utilisés pour obtenir des informations significatives à partir de données émergent dans les programmes universitaires d'informatique.
Il peut être utilisé pour raisonner sur les entités au sein de ce domaine et peut être utilisé pour décrire le domaine.
Traditionnellement, ils travaillaient avec des documents imprimés, mais ces compétences sont de plus en plus utilisées avec des documents électroniques, visuels, audio et numériques.
Sur le plan institutionnel, les sciences de l'information ont émergé au XIXe siècle avec de nombreuses autres disciplines des sciences sociales.
En 1731, Benjamin Franklin a créé la Library Company of Philadelphia, la première bibliothèque appartenant à un groupe de citoyens publics, qui s'est rapidement étendue au-delà du domaine des livres et est devenue un centre d'expérimentation scientifique, et qui a accueilli des expositions publiques d'expériences scientifiques.
En 1801, Joseph Marie Jacquard invente un système de cartes perforées pour contrôler les opérations du métier à tisser en France.
En 1843, Richard Hoe développa la presse rotative et, en 1844, Samuel Morse envoya le premier message télégraphique public.
En 1860, un congrès a eu lieu à Karlsruhe Technische Hochschule pour discuter de la faisabilité d'établir une nomenclature systématique et rationnelle pour la chimie.
L'année suivante, la Royal Society a commencé la publication de son catalogue d'articles à Londres.
De nombreux historiens des sciences de l'information citent Paul Otlet et Henri La Fontaine comme les pères de la science de l'information avec la fondation de l'Institut international de bibliographie (IIB) en 1895.
Les documentalistes ont mis l'accent sur l'intégration utilitaire de la technologie et de la technique vers des objectifs sociaux spécifiques.
Otlet et Lafontaine ont créé de nombreuses organisations vouées à la normalisation, à la bibliographie, aux associations internationales et, par conséquent, à la coopération internationale.
Cette collection impliquait des feuilles de papier et des cartes standardisées classées dans des armoires conçues sur mesure selon un index hiérarchique (qui rassemblait des informations dans le monde entier à partir de diverses sources) et un service commercial de recherche d'informations (qui répondait aux demandes écrites en copiant les informations pertinentes à partir de fiches).
De plus, les frontières traditionnelles entre les disciplines ont commencé à s'estomper et de nombreux chercheurs en sciences de l'information se sont joints à d'autres programmes.
Les années 1980 ont également vu l'émergence de nombreux groupes d'intérêts spéciaux pour répondre aux changements.
Zhang, B., Semenov, A., Vos, M. et Veijlainen, J. (2014).
Le partage via les médias sociaux est devenu si influent que les éditeurs doivent "jouer gentiment" s'ils souhaitent réussir.
C'est pour cette raison que ces réseaux ont été réalisés pour le potentiel qu'ils offrent. "
Qu'en est-il de l'attribution de privilèges et de la restriction de l'accès aux utilisateurs non autorisés ?
Il s'agit d'une discipline émergente et d'une communauté de pratique axée sur le rapprochement des principes de conception et d'architecture dans le paysage numérique.
Les systèmes automatisés de recherche d'informations sont utilisés pour réduire ce que l'on a appelé la "surcharge d'informations".
Un processus de récupération d'informations commence lorsqu'un utilisateur entre une requête dans le système.
Au lieu de cela, plusieurs objets peuvent correspondre à la requête, peut-être avec différents degrés de pertinence.
Selon l'application, les objets de données peuvent être, par exemple, des documents texte, des ibames, de l'audio, des cartes mentales ou des vidéos.
La recherche d'informations est liée à la recherche d'informations (IR), mais différente de celle-ci.
La logique est utilisée pour fournir une sémantique formelle sur la façon dont les fonctions de raisonnement doivent être appliquées aux symboles dans le système KR.
C'était aussi une croyance commune que les catastrophes naturelles telles que la famine et les inondations étaient des rétributions divines portant des signes du mécontentement du Ciel envers le dirigeant, de sorte qu'il y aurait souvent des révoltes à la suite de catastrophes majeures car les gens voyaient ces calamités comme des signes que le Mandat du Ciel avait été retiré.
Le concept est à certains égards similaire au concept européen du droit divin des rois; cependant, contrairement au concept européen, il ne confère pas un droit inconditionnel de gouverner.
Le Mandat du Ciel était souvent invoqué par les philosophes et les érudits en Chine comme un moyen de limiter l'abus de pouvoir par le dirigeant, dans un système qui avait peu d'autres contrôles.
Notamment, la dynastie a duré un temps considérable au cours duquel 31 rois ont régné sur une longue période de 17 générations.
Au fil du temps, cependant, les abus des dirigeants envers les autres classes sociales ont conduit à des troubles sociaux et à l'instabilité.
Ils ont créé le Mandat du Ciel pour expliquer leur droit d'assumer le pouvoir et ont présumé que la seule façon de tenir le mandat était de bien gouverner aux yeux du Ciel.
Cependant, afin d'apaiser certains citoyens, ils ont permis à certains bénéficiaires Shang de continuer à gouverner leurs petits royaumes conformément aux règles et règlements de Zhou.
Ils excellaient également dans la construction navale, ce qui, couplé à leur découverte de la navigation céleste, faisait d'eux d'excellents marins.
La plupart de ces ouvrages sont des commentaires sur les progrès et le mouvement politique de la dynastie.
Leurs œuvres ont principalement souligné l'importance de la classe dirigeante, le respect et leur relation avec la classe inférieure.
Au sein de ces districts se trouvaient des administrateurs qui étaient nommés par le gouvernement, en retour, ils devaient maintenir leur allégeance au principal gouvernement interne.
Enfin, lorsque le pouvoir de la dynastie Zhou a diminué, il a été anéanti par l'État de Qin, qui croyait que les Zhou étaient devenus faibles et leur règne injuste.
Au cours de cette réforme, des changements administratifs ont été apportés et un système de légalisme a été développé qui stipulait que la loi est suprême sur chaque individu, y compris les dirigeants.
L'établissement de la dynastie Han a marqué une grande période de l'histoire de la Chine marquée par des changements importants dans la structure politique du pays.
Un objectif majeur était d'établir la justification du transfert du Mandat du Ciel à travers ces cinq dynasties et donc à la dynastie Song.
Ils détenaient également considérablement plus de territoire que n'importe lequel des autres États chinois qui avaient existé de manière continue dans le sud.
Le comportement brutal de Zhu Wen et des Liang ultérieurs était une source d'embarras considérable, et il y avait donc des pressions pour les exclure du mandat.
Cependant, Kublai Khan était le seul dirigeant indifférent lorsqu'il a revendiqué le Mandat du Ciel sur la dynastie Yuan car il avait une armée importante et faisait partie du peuple Khitan, comme beaucoup d'autres du même milieu car ils n'avaient pas les mêmes traditions. et la culture comme leurs adversaires chinois.
Ce n'était que de la politique du début à la fin et une tentative de l'empereur de maintenir un acte favorable envers le Ciel.
Le droit de rébellion n'est inscrit dans aucune loi officielle.
Puisque le gagnant est celui qui détermine qui a obtenu le Mandat du Ciel et qui l'a perdu, certains érudits chinois le considèrent comme une sorte de justice du vainqueur, mieux caractérisée dans le dicton populaire chinois "Le gagnant devient roi, le perdant devient hors-la-loi" (chinois : "成者爲王，敗者爲寇" ).
On dit également que le royaume de Silla a adopté le Mandat du Ciel, mais les premiers enregistrements datent de la dynastie Joseon, qui a fait du Mandat du Ciel une idéologie d'État durable.
Les dynasties vietnamiennes plus tardives et plus centralisées ont adopté le confucianisme comme idéologie d'État, ce qui a conduit à la création d'un système tributaire vietnamien en Asie du Sud-Est qui a été calqué sur le système sinocentrique chinois en Asie de l'Est.
Plus tard, ce besoin a été évité parce que la Maison impériale du Japon prétendait descendre dans une lignée ininterrompue de la déesse japonaise du soleil, Amaterasu.
Même après la restauration Meiji en 1868, lorsque l'empereur fut replacé au centre de la bureaucratie politique, le trône lui-même avait très peu de pouvoir vis-à-vis de l'oligarchie Meiji.
Les études médiatiques sont une discipline et un domaine d'étude qui traitent du contenu, de l'histoire et des effets de divers médias; en particulier, les médias de masse.
Les études sur les médias en Australie ont d'abord été développées en tant que domaine d'étude dans les universités victoriennes au début des années 1960 et dans les écoles secondaires au milieu des années 1960.
Dans les écoles secondaires, un premier cours d'études cinématographiques a commencé à être enseigné dans le cadre du programme d'études secondaires du premier cycle de l'époque victorienne au milieu des années 1960.
Il est depuis devenu, et continue d'être, une composante importante du VCE.
Les études sur les médias ne semblent pas être enseignées dans l'État de la Nouvelle-Galles du Sud au niveau secondaire.
Harold Innis et Marshall McLuhan sont des universitaires canadiens célèbres pour leurs contributions aux domaines de l'écologie des médias et de l'économie politique au XXe siècle.
L'Université Carleton et l'Université de Western Ontario, 1945 et 1946 prospectivement, ont créé des programmes ou des écoles spécifiques au journalisme.
Aujourd'hui, la plupart des universités offrent des diplômes de premier cycle en études des médias et de la communication, et de nombreux chercheurs canadiens contribuent activement au domaine, parmi lesquels : Brian Massumi (philosophie, études culturelles), Kim Sawchuk (études culturelles, féministes, études sur le vieillissement), Carrie Rentschler ( théorie féministe), et François Cooren (communication organisationnelle).
Un médium est tout ce qui médiatise notre interaction avec le monde ou d'autres humains.
McLuhan dit que la "technique de fragmentation qui est l'essence de la technologie des machines" a façonné la restructuration du travail humain et de l'association et que "l'essence de la technologie de l'automatisation est à l'opposé".
La caractéristique de tous les médias signifie que le « contenu » de tout média est toujours un autre média.
Si la lumière électrique est utilisée pour le football du vendredi soir ou pour éclairer votre bureau, vous pourriez dire que le contenu de la lumière électrique est ces activités.
Ce n'est que lorsque la lumière électrique est utilisée pour épeler un nom de marque qu'elle est reconnue comme médium.
L'effet du médium est renforcé parce qu'il reçoit un autre « contenu » médiatique.
Les médias chauds ont une faible participation et les médias froids une forte participation.
Communication University of China, anciennement connu sous le nom de Beijing Broadcasting Institute, qui remonte à 1954.
L'analyse de Bourdieu est que la télévision offre beaucoup moins d'autonomie, ou de liberté, qu'on ne le pense.
Dans le domaine des études cinématographiques, encore une fois, Francfort et Berlin ont dominé le développement de nouvelles perspectives sur les médias ibame en mouvement.
L'une des premières publications dans cette nouvelle direction est un volume édité par Helmut Kreuzer, Literature Studies - Media Studies (Literaturwissenschaft - Medienwissenschaft), qui résume les présentations données au Düsseldorfer Germanistentag 1976.
L'Institut allemand pour la politique des médias et de la communication, fondé en 2005 par le spécialiste des médias Lutz Hachmeister, est l'une des rares institutions de recherche indépendantes qui se consacre aux questions entourant les politiques des médias et de la communication.
Medienwissenschaften est actuellement l'un des programmes d'études les plus populaires dans les universités allemandes, de nombreux candidats supposant à tort que l'étudier conduira automatiquement à une carrière à la télévision ou dans d'autres médias.
Il offre un programme intégré de cinq ans et un programme de deux ans en médias électroniques.
Alors que les sciences de la communication se concentrent sur la façon dont les gens communiquent, qu'ils soient médiatisés ou non, les études sur les médias ont tendance à réduire la communication à une communication médiatisée.
Les sciences de la communication (ou un dérivé de celles-ci) peuvent être étudiées à l'Université Erasmus de Rotterdam, à l'Université Radboud, à l'Université de Tilburg, à l'Université d'Amsterdam, à l'Université de Groningue, à l'Université de Twente, à l'Académie Roosevelt, à l'Université d'Utrecht, à l'Université VU d'Amsterdam et à l'Université et centre de recherche de Wageningen. .
L'Université du Pendjab Lahore est le département le plus ancien.
Les études sur les médias sont maintenant enseignées dans tout le Royaume-Uni.
Cependant, l'objectif de ces programmes exclut parfois certains médias - films, édition de livres, jeux vidéo, etc.
C'est en partie grâce à l'acquisition du professeur Siva Vaidhyanathan, historienne de la culture et spécialiste des médias, ainsi qu'à la conférence inaugurale sur la politique et l'éthique des médias de Verklin, financée par le PDG de Canoe Ventures et ancien élève de l'UVA, David Verklin.
Une majeure en études médiatiques à Radford signifie toujours quelqu'un qui se concentre sur le journalisme, la radiodiffusion, la publicité ou la production Web.)
Bergson oppose une société ouverte à ce qu'il appelle une société fermée, un système fermé de droit, de morale ou de religion.
Soros, George, "L'âge de la faillibilité", Affaires publiques (2006).
Le totalitarisme a forcé le savoir à devenir politique, ce qui a rendu la pensée critique impossible et a conduit à la destruction du savoir dans les pays totalitaires.
Dans la société fermée, les prétentions à certaines connaissances et à la vérité ultime mènent à la tentative d'imposition d'une version de la réalité.
Parce que la perception de la réalité par l'électorat peut facilement être manipulée, le discours politique démocratique ne conduit pas nécessairement à une meilleure compréhension de la réalité.
Cependant, Popper n'identifiait la société ouverte ni à la démocratie, ni au capitalisme, ni à une économie de laissez-faire, mais plutôt à un état d'esprit critique de la part de l'individu, face à la pensée collective de quelque nature que ce soit.
Les ordres professionnels sont des entités juridiques chargées de servir l'intérêt public en réglementant l'exercice d'une profession.
Par exemple, aucun travailleur en Ontario ne peut exercer un métier à accréditation obligatoire sans être membre de l'Ordre des métiers de l'Ontario.
Pour Weber, la sociologie est l'étude de la société et des comportements et doit donc s'intéresser au cœur de l'interaction.
Le terme est plus pratique et englobant que les "phénomènes sociaux" de Florian Znaniecki, puisque l'individu réalisant une action sociale n'est pas passif, mais plutôt actif et réactif.
Ceci est également considéré comme un moyen alternatif lorsque les conséquences secondaires ont pris fin.
Si l'étudiant choisit de ne pas réussir à l'université, il sait qu'il sera difficile d'entrer à la faculté de droit et d'atteindre finalement l'objectif d'être avocat.
La relation de valeur est divisée en sous-groupes de commandes et de demandes.
Ces exigences ont posé plusieurs problèmes même le formalisme juridique a été mis à l'épreuve.
Dans la mesure où de nombreuses entreprises religieuses se font concurrence, elles auront tendance à se spécialiser et à répondre aux besoins particuliers de certains segments de consommateurs religieux.
Il est bien connu que les églises strictes sont fortes et en croissance dans les États-Unis contemporains, tandis que les églises libérales sont en déclin.
Action affective (également connue sous le nom d'actions émotionnelles): actions qui sont prises en raison de «ses émotions, pour exprimer des sentiments personnels».
Dans la réaction incontrôlée, il n'y a pas de retenue et il y a un manque de discrétion.
Lorsque les aspirations ne sont pas satisfaites, il y a des troubles internes.
Un exemple courant est celui des hypothèses de choix comportemental et rationnel.
Ces six concepts ont été identifiés par Aristote et font encore l'objet de plusieurs entretiens.
Les théories micrologiques de l'économie considèrent les actes d'un groupe d'individus.
Ce faisant, cela rend les fournisseurs compétitifs et crée donc de l'ordre dans l'économie.
La théorie du choix rationnel bien que de plus en plus colonisée par les économistes, elle diffère des conceptions microéconomiques.
Actions traditionnelles : actions qui sont réalisées en raison de la tradition, car elles sont toujours réalisées d'une manière particulière pour certaines situations.
Une coutume est une pratique qui repose sur la familiarité.
Une habitude est une série d'étapes apprises progressivement et parfois sans conscience.
L'idée du soi miroir de Cooley est que notre sens de soi se développe lorsque nous observons et réfléchissons sur les autres et sur ce qu'ils peuvent penser de nos actions.
Le capital social est "les réseaux de relations entre les personnes qui vivent et travaillent dans une société particulière, permettant à cette société de fonctionner efficacement".
Dans la première moitié du XIXe siècle, de Tocqueville avait des observations sur la vie américaine qui semblaient esquisser et définir le capital social.
La communauté dans son ensemble bénéficiera de la coopération de toutes ses parties, tandis que l'individu trouvera dans ses associations les avantages de l'aide, de la sympathie et de la camaraderie de ses voisins.
Selon les mots de Stein (1960 : 1) : « Le prix à payer pour maintenir une société qui encourage la différenciation culturelle et l'expérimentation est sans aucun doute l'acceptation d'une certaine désorganisation tant au niveau individuel que social.
Toutes ces réflexions ont contribué de manière remarquable au développement du concept de capital social dans les décennies suivantes.
Robert D. Putnam (1993) a suggéré que le capital social faciliterait la coopération et les relations de soutien mutuel dans les communautés et les nations et serait donc un moyen précieux de lutter contre de nombreux troubles sociaux inhérents aux sociétés modernes, par exemple le crime.
Le concept de capital social de Nan Lin a une approche plus individualiste : « Investissement dans les relations sociales avec rendements attendus sur le marché ».
Le terme capital est utilisé par analogie avec d'autres formes de capital économique, car on prétend que le capital social a des avantages similaires (bien que moins mesurables).
Robison, Schmid et Siles (2002) ont examiné diverses définitions du capital social et ont conclu que bon nombre d'entre elles ne satisfaisaient pas à l'exigence formelle d'une définition.
Ils proposent que le capital social soit défini comme la sympathie : l'objet de la sympathie d'autrui a un capital social ; ceux qui ont de la sympathie pour les autres fournissent un capital social.
Le capital social se distingue également de la théorie économique du capitalisme social.
Cela "crée de la valeur pour les personnes connectées, mais aussi pour les spectateurs".
Selon Robert D. Putnam, le capital social fait référence aux "liens entre les individus - les réseaux sociaux et les normes de réciprocité et de fiabilité qui en découlent".
Cela se voit dans des niveaux inférieurs de confiance dans le gouvernement et des niveaux inférieurs de participation civique.
Putnam suggère également que l'une des causes profondes du déclin du capital social est l'entrée des femmes sur le marché du travail, ce qui pourrait être en corrélation avec les contraintes de temps qui inhibent la participation des organisations civiques comme les associations parents-enseignants.
Fukuyama suggère que si le capital social est bénéfique pour le développement, il impose également un coût aux non-membres du groupe avec des conséquences imprévues pour le bien-être général.
Cette dimension se concentre sur les avantages tirés de la configuration du réseau d'un acteur, qu'il soit individuel ou collectif.
Cela se caractérise le mieux par la confiance des autres, leur coopération et l'identification d'un individu au sein d'un réseau.
Les recherches de Sheri Berman et Dylan Riley, ainsi que des économistes Shanker Satyanath, Nico Voigtländer et Hans-Joachim Voth, ont lié les associations civiques à la montée des mouvements fascistes.
Les conséquences négatives du capital social sont plus souvent associées au lien vis-à-vis du bridging.
Le capital social de liaison et de liaison peut fonctionner ensemble de manière productive s'il est équilibré, ou ils peuvent fonctionner l'un contre l'autre.
Le renforcement des liens insulaires peut entraîner divers effets tels que la marginalisation ethnique ou l'isolement social.
Les Allemands se sont jetés dans leurs clubs, associations bénévoles et organisations professionnelles par frustration face aux échecs du gouvernement national et des partis politiques, contribuant ainsi à saper la République de Weimar et à faciliter la montée au pouvoir d'Hitler."
Ils étaient très introvertis dans la République de Weimar.
Robert Putnam, dans ses travaux ultérieurs, suggère également que le capital social et la croissance associée de la confiance du public sont inhibés par l'immigration et la diversité raciale croissante dans les communautés.
Le manque d'homogénéité a conduit les gens à se retirer même de leurs groupes et relations les plus proches, créant une société atomisée par opposition à une communauté cohésive.
Le capital humain, une ressource privée, pourrait être accessible grâce à ce que la génération précédente a accumulé grâce au capital social.
Même si Coleman ne s'adresse jamais vraiment à Pierre Bourdieu dans sa discussion, cela coïncide avec l'argument de Bourdieu exposé dans Reproduction in Education, Society and Culture.
Ainsi, c'est la plate-forme sociale, elle-même, qui équipe l'individu de la réalité sociale à laquelle il s'habitue.
Pour illustrer cela, supposons qu'un individu souhaite améliorer sa place dans la société.
La société civile est-elle une théorie adéquate ?
Des exemples typiques sont que les gangs criminels créent un capital social de liaison, tandis que les chorales et les clubs de bowling (d'où le titre, comme Putnam a déploré leur déclin) créent un capital social de liaison.
Aldrich applique également les idées de capital social aux principes fondamentaux de la reprise après sinistre et discute des facteurs qui facilitent ou entravent la reprise, tels que l'étendue du dabame, la densité de population, la qualité du gouvernement et l'aide.
Les personnes qui vivent leur vie de cette façon estiment que ce sont des normes de la société et sont capables de vivre leur vie sans se soucier de leur crédit, de leurs enfants et de recevoir la charité si nécessaire.
Toutes les formes de « capital » étaient, pour Marx, possédées uniquement par des capitalistes et il soulignait la base du travail dans la société capitaliste, en tant que classe constituée d'individus obligés de vendre leur force de travail, faute de capital suffisant, dans tous les sens du terme. mot, faire autrement.
Portes mentionne le don d'une bourse à un membre du même groupe ethnique à titre d'exemple.
Des sous-échelles de liaison et de transition sont proposées, qui ont été adoptées par plus de 300 articles scientifiques.
Cependant, il n'existe pas de méthode quantitative unique pour déterminer le niveau de cohésion, mais plutôt un ensemble de modèles de réseaux sociaux que les chercheurs ont utilisés au fil des décennies pour opérationnaliser le capital social.
Les groupes à forte adhésion (tels que les partis politiques) contribuent davantage au montant du capital que les groupes à faible adhésion, bien que de nombreux groupes à faible adhésion (tels que les communautés) s'additionnent encore pour être importants.
La façon dont un groupe se rapporte au reste de la société affecte également le capital social, mais d'une manière différente.
Reconnaissant que l'on peut ne pas être en mesure d'influencer la sympathie des autres, les personnes cherchant à appartenir peuvent agir pour accroître leur propre sympathie pour les autres et les organisations ou institutions qu'elles représentent.
Selon des auteurs tels que Walzer (1992), Alessandrini (2002), Newtown, Stolle & Rochon, Foley & Edwards (1997) et Walters, c'est à travers la société civile, ou plus exactement le troisième secteur, que les individus peuvent établir et entretenir des réseaux relationnels.
Non seulement la société civile a été documentée pour produire des sources de capital social, selon Lyons' Third Sector (2001), mais le capital social n'apparaît sous aucune forme ni dans les facteurs qui permettent ni dans ceux qui stimulent la croissance du tiers secteur.
L'objectif est de réintégrer ceux qui sont marginalisés par les récompenses du système économique dans « la communauté ».
Alessandrini est d'accord, affirmant qu'"en Australie en particulier, le néolibéralisme a été refondu en tant que rationalisme économique et identifié par plusieurs théoriciens et commentateurs comme un danger pour la société dans son ensemble en raison de l'utilisation qu'ils font du capital social".
En développement international, Ben Fine (2001) et John Harriss (2001) ont vivement critiqué l'adoption inappropriée du capital social comme prétendue panacée (promotion des organisations de la société civile et des ONG, par exemple, comme agents de développement) pour les inégalités générées par le développement économique néolibéral.
Cependant, des niveaux plus élevés de capital social ont conduit à un plus grand soutien à la démocratie.
Une évaluation minutieuse de ces facteurs fondamentaux suggère souvent que les femmes ne votent pas aux mêmes niveaux que les hommes.
Le capital social offre une multitude de ressources et de réseaux qui facilitent l'engagement politique.
Les femmes sont plus susceptibles de s'organiser de manière moins hiérarchique et de se concentrer sur la création d'un consensus.
Par exemple, une personne atteinte d'un cancer peut recevoir des informations, de l'argent ou un soutien moral dont elle a besoin pour supporter un traitement et se rétablir.
En outre, le capital social du quartier peut également contribuer à atténuer les inégalités en matière de santé chez les enfants et les adolescents.
Les relations et les réseaux entretenus par une population de minorité ethnique dans une zone géographique où un pourcentage élevé de résidents appartiennent au même groupe ethnique peuvent conduire à de meilleurs résultats de santé que ce à quoi on pourrait s'attendre en fonction d'autres caractéristiques individuelles et de quartier.
Par exemple, les résultats d'une enquête menée auprès d'étudiants de 13 à 18 ans en Suède ont montré qu'un faible capital social et une faible confiance sociale sont associés à des taux plus élevés de symptômes psychosomatiques, de douleurs musculo-squelettiques et de dépression.
Dans une étude, les utilisations informationnelles d'Internet étaient positivement corrélées à la production de capital social d'un individu, et les utilisations socio-récréatives étaient négativement corrélées (des niveaux plus élevés de ces utilisations étaient corrélés à des niveaux plus faibles de capital social).
Cela signifie que les individus peuvent se connecter de manière sélective avec d'autres en fonction d'intérêts et d'antécédents déterminés.
Cet argument se poursuit, bien que la prépondérance des preuves montre une association positive entre le capital social et Internet.
Des recherches récentes, menées en 2006, montrent également que les internautes ont souvent des réseaux plus étendus que ceux qui accèdent à Internet de manière irrégulière ou pas du tout.
D'autres recherches montrent que les jeunes utilisent Internet comme moyen de communication supplémentaire, plutôt que de laisser la communication Internet remplacer le contact en face à face.
Ils reprochent à Coleman, qui n'a utilisé que le nombre de parents présents dans la famille, d'avoir négligé l'effet invisible de dimensions plus discrètes telles que les beaux-parents et les différents types de familles monoparentales.
Morgan et Sorensen (1999) contestent directement Coleman pour son manque de mécanisme explicite pour expliquer pourquoi les élèves des écoles catholiques obtiennent de meilleurs résultats que les élèves des écoles publiques aux tests de rendement standardisés.
Il a été constaté que si le capital social peut avoir un effet positif sur le maintien d'une communauté fonctionnelle englobante dans les écoles appliquant les normes, il entraîne également la conséquence négative d'un contrôle excessif.
Ces écoles explorent un autre type de capital social, comme les informations sur les opportunités dans les réseaux sociaux étendus des parents et d'autres adultes.
La similitude de ces états est que les parents étaient davantage associés à l'éducation de leurs enfants.
Sans capital social dans le domaine de l'éducation, des enseignants et des parents qui assument une responsabilité dans l'apprentissage d'un élève, les impacts significatifs sur les apprentissages scolaires de leur enfant peuvent reposer sur ces facteurs.
Comme l'affirment Tedin et Weiher (2010), « l'un des facteurs les plus importants pour favoriser la réussite des élèves est l'implication active des parents dans l'éducation d'un enfant ».
Les réseaux de soutien, en tant que forme de capital social, sont nécessaires pour activer le capital culturel que possédaient les étudiants nouvellement arrivés.
La solidarité ethnique est particulièrement importante dans le contexte où les immigrants viennent d'arriver dans la société d'accueil.
Le soutien ethnique donne un élan à la réussite scolaire.
Son principal argument pour classer le capital social en tant que concept géographique est que les relations entre les personnes sont façonnées et façonnées par les zones dans lesquelles elles vivent.
Dans ses études, il ne s'intéresse pas aux acteurs individuels de ces structures, mais à la manière dont les structures et les liens sociaux qui en découlent se diffusent dans l'espace.
Un autre domaine où le capital social peut être considéré comme un domaine d'étude en géographie est l'analyse de la participation au bénévolat et son soutien aux différents gouvernements.
Il existe un lien important entre les loisirs et le capital social démocratique.
Dans une étude ultérieure, Kislev (2020) montre la relation entre le désir des relations amoureuses et le célibat.
Des résultats similaires ont été révélés dans une étude transversale menée par Sarker au Bangladesh.
Epo l'a fait en comparant les résultats en matière de bien-être des entrepreneurs qui avaient à la fois accès et pas d'accès.
La cohésion de groupe (également appelée cohésion de groupe et cohésion sociale) survient lorsque des liens lient les membres d'un groupe social les uns aux autres et au groupe dans son ensemble.
La cohésion peut être plus précisément définie comme la tendance d'un groupe à être uni tout en travaillant vers un but ou pour satisfaire les besoins émotionnels de ses membres.
Sa nature dynamique fait référence à la façon dont il change progressivement au fil du temps dans sa force et sa forme à partir du moment où un groupe est formé jusqu'au moment où un groupe est dissous.
Cette définition peut être généralisée à la plupart des groupes caractérisés par la définition de groupe discutée ci-dessus.
Dans une étude, ils ont demandé aux membres du groupe d'identifier tous leurs bons amis et ont calculé le rapport entre les choix de l'endogroupe et les choix de l'exogroupe.
La cohésion de groupe est similaire à un type d'attraction au niveau du groupe qui, selon Hogg, est connue sous le nom d'attraction sociale.
Lott et Lott (1965) qui se réfèrent à l'attirance interpersonnelle en tant que cohésion de groupe ont mené une revue approfondie de la littérature et ont constaté que les similitudes des individus en matière d'origine (par exemple, race, origine ethnique, profession, âge), d'attitudes, de valeurs et de traits de personnalité ont généralement des effets positifs. lien avec la cohésion du groupe.
De plus, des antécédents similaires rendent plus probable que les membres partagent des points de vue similaires sur diverses questions, notamment les objectifs du groupe, les méthodes de communication et le type de leadership souhaité.
Cela est souvent causé par le relâchement social, une théorie qui dit que les membres individuels d'un groupe feront en fait moins d'efforts, car ils croient que les autres membres compenseront le retard.
La plupart des méta-analyses (études qui ont synthétisé les résultats de nombreuses études) ont montré qu'il existe une relation entre cohésion et performance.
Lorsqu'elle est définie comme l'engagement dans la tâche, elle est également corrélée à la performance, bien qu'à un degré moindre que la cohésion comme attraction.
Cependant, certains groupes peuvent avoir une relation cohésion-performance plus forte que d'autres.
Il existe certaines preuves que la cohésion peut être plus fortement liée à la performance pour les groupes qui ont des rôles très interdépendants que pour les groupes dont les membres sont indépendants.
De plus, les groupes avec des objectifs de haute performance étaient extrêmement productifs.
Les membres des groupes cohésifs sont également plus optimistes et souffrent moins de problèmes sociaux que ceux des groupes non cohésifs.
Il a été constaté que les maçons et les charpentiers étaient plus satisfaits lorsqu'ils travaillaient en groupes cohérents.
Une étude a montré que la cohésion en tant qu'engagement dans la tâche peut améliorer la prise de décision de groupe lorsque le groupe est stressé, plus que lorsqu'il n'est pas stressé.
L'étude a révélé que les équipes à faible cohésion et à forte urgence obtenaient de moins bons résultats que les équipes à forte cohésion et à forte urgence.
La théorie de la pensée de groupe suggère que les pressions empêchent le groupe de réfléchir de manière critique aux décisions qu'il prend.
Une autre raison est que les gens apprécient le groupe et sont donc plus disposés à céder aux pressions de la conformité pour maintenir ou améliorer leurs relations.
Le degré d'appréciation des membres était supposé indiquer la cohésion du groupe.
Selon les rapports thématiques sur l'état des villes anglaises commandés par le gouvernement, il existe cinq dimensions différentes de la cohésion sociale : les conditions matérielles, les relations passives, les relations actives, la solidarité, l'inclusion et l'égalité.
Ces nécessités fondamentales de la vie sont les fondements d'un tissu social fort et d'importants indicateurs de progrès social.
La troisième dimension fait référence aux interactions positives, aux échanges et aux réseaux entre les individus et les communautés, ou "relations sociales actives".
Cela comprend également le sentiment d'appartenance des gens à une ville et la force des expériences, des identités et des valeurs partagées entre ceux qui viennent d'horizons différents.
Sur le plan sociétal, Albrekt Larsen définit la cohésion sociale « comme la croyance, chez les citoyens d'un État-nation donné, qu'ils partagent une communauté morale, qui leur permet de se faire confiance ».
La formation sociale est un concept marxiste (synonyme de « société ») faisant référence à l'articulation concrète et historique entre le mode de production capitaliste, le maintien des modes de production précapitalistes, et le contexte institutionnel de l'économie (désambiguïsation).
Dans les sciences sociales, la structure sociale est l'ensemble des arrangements sociaux dans la société qui sont à la fois issus et déterminants des actions des individus.
Il s'oppose au « système social », qui renvoie à la structure parentale dans laquelle s'inscrivent ces diverses structures.
Il détermine les normes et les modèles de relations entre les diverses institutions de la société.
Il est également important dans l'étude moderne des organisations, car la structure d'une organisation peut déterminer sa flexibilité, sa capacité à changer, etc.
A l'échelle méso, il s'agit de la structuration des réseaux sociaux entre individus ou organisations.
Par exemple, John Levi Martin a émis l'hypothèse que certaines structures à grande échelle sont les propriétés émergentes d'institutions culturelles à petite échelle (c'est-à-dire que la « structure » ressemble à celle utilisée par l'anthropologue Claude Lévi-Strauss).
Alexis de Tocqueville aurait été le premier à utiliser le terme « structure sociale ».
L'un des récits les plus anciens et les plus complets de la structure sociale a été fourni par Karl Marx, qui a lié la vie politique, culturelle et religieuse au mode de production (une structure économique sous-jacente).
Durkheim , s'appuyant sur les analogies entre les systèmes biologiques et sociaux popularisées par Herbert Spencer et d'autres, a introduit l'idée que diverses institutions et pratiques sociales jouaient un rôle pour assurer l'intégration fonctionnelle de la société par l'assimilation de diverses parties dans un ensemble unifié et autonome. reproduisant l'ensemble.
D'autres suivent Lévi-Strauss dans la recherche d'un ordre logique dans les structures culturelles.
Les tentatives les plus influentes de combiner le concept de structure sociale avec l'agentivité sont la théorie de la structuration d'Anthony Giddens et la théorie de la pratique de Pierre Bourdieu.
L'analyse de Giddens, à cet égard, est étroitement parallèle à la déconstruction par Jacques Derrida des binaires qui sous-tendent le raisonnement sociologique et anthropologique classique (notamment les tendances universalisantes du structuralisme de Lévi-Strauss).
Cela a été étudié par Jacob L. Moreno.
La sociobiologie est un domaine de la biologie qui vise à examiner et à expliquer le comportement social en termes d'évolution.
La sociobiologie étudie les comportements sociaux tels que les schémas d'accouplement, les combats territoriaux, la chasse en meute et la société de la ruche des insectes sociaux.
Il prédit que les animaux agiront d'une manière qui s'est avérée évolutive réussie au fil du temps.
Le comportement est donc vu comme un effort pour préserver ses gènes dans la population.
Altmann a développé sa propre marque de sociobiologie pour étudier le comportement social des macaques rhésus, en utilisant des statistiques, et a été embauché comme "sociobiologiste" au Yerkes Regional Primate Research Center en 1965.
Autrefois terme spécialisé, la "sociobiologie" est devenue largement connue en 1975 lorsque Wilson a publié son livre Sociobiology : The New Synthesis, qui a déclenché une intense controverse.
Cependant, l'influence de l'évolution sur le comportement intéresse les biologistes et les philosophes depuis peu de temps après la découverte de l'évolution elle-même.
Edward H. Hagen écrit dans The Handbook of Evolutionary Psychology que la sociobiologie est, malgré la controverse publique concernant les applications aux humains, "l'un des triomphes scientifiques du XXe siècle"\n"
Par conséquent, ces traits étaient probablement "adaptatifs" à l'environnement dans lequel l'espèce évoluait.
Ainsi, ils sont souvent intéressés par le comportement instinctif ou intuitif, et par l'explication des similitudes, plutôt que des différences, entre les cultures.
Cette protection parentale augmenterait en fréquence dans la population.
EO Wilson a soutenu que l'évolution peut aussi agir sur les groupes.
Si l'altruisme est génétiquement déterminé, alors les individus altruistes doivent reproduire leurs propres traits génétiques altruistes pour que l'altruisme survive, mais lorsque les altruistes prodiguent leurs ressources aux non-altruistes aux dépens des leurs, les altruistes ont tendance à disparaître et les autres à augmenter.
Au sein de la sociobiologie, un comportement social est d'abord expliqué comme une hypothèse sociobiologique en trouvant une stratégie évolutivement stable qui correspond au comportement observé.
L'altruisme entre insectes sociaux et compagnons de portée a été expliqué de cette manière.
En général, les femelles ayant plus d'opportunités de procréation peuvent accorder moins d'importance à la progéniture et peuvent également organiser des opportunités de procréation pour maximiser la nourriture et la protection contre les partenaires.
Les études sur la génétique du comportement humain ont généralement révélé que les traits comportementaux tels que la créativité, l'extraversion, l'agressivité et le QI ont une héritabilité élevée.
Ainsi, lorsque le FEV est génétiquement supprimé du génome de la souris, les souris mâles attaquent instantanément d'autres mâles, alors que leurs homologues de type sauvage mettent beaucoup plus de temps à initier un comportement violent.
Au cours d'une réunion de 1976 du groupe d'étude sociobiologique, comme le rapporte Ullica Segerstråle, Chomsky a plaidé pour l'importance d'une notion sociobiologiquement informée de la nature humaine.
Wilson a affirmé qu'il n'avait jamais voulu impliquer ce qui devrait être, seulement ce qui est le cas.
Les affaires sont l'activité qui consiste à gagner sa vie ou à gagner de l'argent en produisant ou en achetant et en vendant des produits (tels que des biens et des services).
Si l'entreprise contracte des dettes, les créanciers peuvent s'en prendre aux biens personnels du propriétaire.
Le terme est également souvent utilisé familièrement (mais pas par les avocats ou par les fonctionnaires) pour désigner une entreprise.
Une société privée à but lucratif appartient à ses actionnaires, qui élisent un conseil d'administration pour diriger la société et embaucher son personnel de direction.
Une coopérative diffère d'une société par le fait qu'elle a des membres, et non des actionnaires, et qu'ils partagent le pouvoir décisionnel.
Les sociétés à responsabilité limitée (LLC), les sociétés à responsabilité limitée et d'autres types spécifiques d'organisations commerciales protègent leurs propriétaires ou actionnaires contre la faillite d'entreprise en faisant des affaires sous une entité juridique distincte avec certaines protections juridiques.
Les membres garantissent le paiement de certains montants (généralement nominaux) si la société entre en liquidation insolvable, mais sinon, ils n'ont aucun droit économique par rapport à la société.
Ce type de société ne peut plus être formé au Royaume-Uni, bien que des dispositions existent toujours dans la loi pour qu'elles existent.
Notez que "Ltd après le nom de la société signifie société à responsabilité limitée, et PLC (société anonyme) indique que ses actions sont largement détenues".
Dans une société à responsabilité limitée, ce seront les garants.
Les sociétés privées n'ont pas d'actions cotées en bourse et contiennent souvent des restrictions sur les transferts d'actions.
Les sociétés de divertissement et les agences de médias de masse génèrent des bénéfices principalement à partir de la vente de propriété intellectuelle.
Ils comprennent des biens corporels tels que des voitures, des bus, des dispositifs médicaux, du verre ou des avions.
La plupart des magasins et des sociétés de catalogue sont des distributeurs ou des détaillants.
Ils réalisent leurs bénéfices en vendant des biens et des services liés au sport.
Le domaine moderne a été établi par le mathématicien italien Luca Pacioli en 1494.
La finance peut également être définie comme la science de la gestion de l'argent.
Les propriétaires peuvent gérer eux-mêmes leur entreprise ou employer des gestionnaires pour le faire à leur place.
La gestion des processus métier (BPM) est une approche de gestion holistique axée sur l'alignement de tous les aspects d'une organisation sur les désirs et les besoins des clients.
De nombreuses entreprises sont exploitées par le biais d'une entité distincte telle qu'une société ou un partenariat (formé avec ou sans responsabilité limitée).
De manière générale, les actionnaires d'une société, les associés commanditaires d'une société en commandite et les membres d'une société à responsabilité limitée sont protégés de la responsabilité personnelle pour les dettes et obligations de l'entité, qui est légalement traitée comme une "personne" distincte.
Les termes d'un partenariat sont régis en partie par un contrat de société s'il en existe un, et en partie par la loi de la juridiction où le partenariat est situé.
Dans certains systèmes fiscaux, cela peut donner lieu à ce qu'on appelle une double imposition, car la société paie d'abord l'impôt sur les bénéfices, puis, lorsque la société distribue ses bénéfices à ses propriétaires, les particuliers doivent inclure les dividendes dans leur revenu lorsqu'ils terminent leur déclarations de revenus des particuliers, auquel cas une deuxième couche d'impôt sur le revenu est imposée.
L'"introduction en bourse" par le biais d'un processus connu sous le nom d'offre publique initiale (IPO) signifie qu'une partie de l'entreprise sera détenue par des membres du public.
Le Code d'Hammourabi date par exemple d'environ 1772 av. J.-C. et contient des dispositions qui concernent, entre autres, les frais de transport et les relations entre marchands et courtiers.
Les juridictions locales peuvent également exiger des licences et des taxes spéciales uniquement pour exploiter une entreprise.
La plupart des pays dotés de marchés de capitaux en ont au moins un.
D'autres pays occidentaux ont des organismes de réglementation comparables.
La prolifération et la complexité croissante des lois régissant les affaires ont imposé une spécialisation croissante en droit des sociétés.
La plupart des entreprises ont des noms, des logos et des techniques de marque similaires qui pourraient bénéficier de la marque.
L'économie est la science sociale qui étudie comment les gens interagissent avec la valeur ; en particulier, la production, la distribution et la consommation de biens et de services.
Il a affirmé que les économistes précédents ont généralement centré leurs études sur l'analyse de la richesse : comment la richesse est créée (production), distribuée et consommée ; et comment la richesse peut croître.
Si la guerre n'est pas gagnable ou si les coûts attendus l'emportent sur les avantages, les acteurs qui décident (en supposant qu'ils sont rationnels) peuvent ne jamais entrer en guerre (une décision) mais plutôt explorer d'autres alternatives.
Les préceptes économiques se retrouvent dans les écrits du poète béotien Hésiode et plusieurs historiens de l'économie ont décrit Hésiode lui-même comme le « premier économiste ».
Deux groupes, appelés plus tard «mercantilistes» et «physiocrates», ont plus directement influencé le développement ultérieur du sujet.
Elle soutenait que la richesse d'une nation dépendait de son accumulation d'or et d'argent.
Les physiocrates, un groupe de penseurs et d'écrivains français du XVIIIe siècle, ont développé l'idée de l'économie comme un flux circulaire de revenus et de produits.
Les physiocrates ont préconisé de remplacer les collectes d'impôts coûteuses sur le plan administratif par un impôt unique sur le revenu des propriétaires fonciers.
Smith examine les avantages potentiels de la spécialisation par division du travail, y compris l'augmentation de la productivité du travail et les gains du commerce, que ce soit entre la ville et la campagne ou entre les pays.
La force d'une population en croissance rapide contre une quantité limitée de terres signifiait des rendements décroissants du travail.
Alors qu'Adam Smith mettait l'accent sur la production de revenus, David Ricardo (1817) s'est concentré sur la répartition des revenus entre les propriétaires terriens, les travailleurs et les capitalistes.
Ricardo a été le premier à énoncer et à prouver le principe de l'avantage comparatif, selon lequel chaque pays devrait se spécialiser dans la production et l'exportation de biens en ce sens qu'il a un coût de production relatif inférieur, ne s'appuyant plutôt que sur sa propre production.
Mill a souligné une nette différence entre les deux rôles du marché : l'allocation des ressources et la distribution des revenus.
Smith a écrit que "le prix réel de chaque chose ... est le labeur et la peine de l'acquérir".
La définition de Say a prévalu jusqu'à nos jours, sauvée en substituant le mot "richesse" à "biens et services" signifiant que la richesse peut également inclure des objets immatériels.
Pour Robbins, l'insuffisance était résolue, et sa définition nous permet de proclamer, la conscience tranquille, l'économie de l'éducation, l'économie de la sûreté et de la sécurité, l'économie de la santé, l'économie de la guerre et, bien sûr, l'économie de la production, de la distribution et de la consommation comme sujets valables de l'économie. sciences économiques."
Bien que loin d'être unanimes, la plupart des économistes traditionnels accepteraient une version de la définition de Robbins, même si beaucoup ont soulevé de sérieuses objections à la portée et à la méthode de l'économie, émanant de cette définition.
Le terme «économie» a été popularisé par des économistes néoclassiques comme Alfred Marshall comme synonyme concis de «science économique» et substitut de l'ancienne «économie politique».
Elle s'est débarrassée de la théorie de la valeur travail héritée de l'économie classique au profit d'une théorie de l'utilité marginale de la valeur du côté de la demande et d'une théorie plus générale des coûts du côté de l'offre.
Un exemple immédiat de ceci est la théorie du consommateur de la demande individuelle, qui isole comment les prix (en tant que coûts) et le revenu affectent la quantité demandée.
L'économie traditionnelle moderne s'appuie sur l'économie néoclassique, mais avec de nombreux raffinements qui complètent ou généralisent les analyses antérieures, telles que l'économétrie, la théorie des jeux, l'analyse des défaillances du marché et de la concurrence imparfaite, et le modèle néoclassique de croissance économique pour analyser les variables à long terme affectant le revenu national. .
Il existe un problème économique, sujet à étude par la science économique, lorsqu'une décision (choix) est prise par un ou plusieurs acteurs contrôlant les ressources pour atteindre le meilleur résultat possible dans des conditions rationnelles limitées.
Le livre s'est concentré sur les déterminants du revenu national à court terme lorsque les prix sont relativement inflexibles.
L'économie keynésienne a deux successeurs.
Il est généralement associé à l'Université de Cambridge et aux travaux de Joan Robinson.
Ben Bernanke, ancien président de la Réserve fédérale, fait partie des économistes qui acceptent généralement aujourd'hui l'analyse de Friedman des causes de la Grande Dépression.
Lors de la création de théories, l'objectif est de trouver celles qui sont au moins aussi simples dans les exigences d'information, plus précises dans les prédictions et plus fructueuses pour générer des recherches supplémentaires que les théories antérieures.
Les premiers modèles macroéconomiques se concentraient sur la modélisation des relations entre les variables agrégées, mais comme les relations semblaient changer au fil du temps, les macroéconomistes, y compris les nouveaux keynésiens, ont reformulé leurs modèles en microfondations.
Parfois, une hypothèse économique n'est que qualitative et non quantitative.
Cependant, le domaine de l'économie expérimentale se développe et on utilise de plus en plus les expériences naturelles.
Par de tels moyens, une hypothèse peut être acceptée, bien que dans un sens probabiliste plutôt que certain.
Les critiques fondées sur les normes professionnelles et la non-reproductibilité des résultats servent de contrôles supplémentaires contre les biais, les erreurs et la généralisation excessive, bien qu'une grande partie de la recherche économique ait été accusée d'être non reproductible et que des revues prestigieuses aient été accusées de ne pas faciliter la réplication par fourniture du code et des données.
En économie appliquée, les modèles d'entrées-sorties utilisant des méthodes de programmation linéaire sont assez courants.
Cela a réduit la distinction de longue date entre l'économie et les sciences naturelles, car cela permet de tester directement ce qui était auparavant considéré comme des axiomes.
Des tests empiriques similaires se produisent en neuroéconomie.
Sur des marchés parfaitement concurrentiels, aucun participant n'est suffisamment important pour avoir le pouvoir de fixer le prix d'un produit homogène.
La microéconomie étudie les marchés individuels en simplifiant le système économique en supposant que l'activité sur le marché analysé n'affecte pas les autres marchés.
La théorie de l'équilibre général étudie divers marchés et leur comportement.
Des choix doivent être faits entre des actions souhaitables mais mutuellement exclusives.
Une partie du coût de la fabrication des bretzels est que ni la farine ni le matin ne sont plus disponibles, pour être utilisés d'une autre manière.
Les intrants utilisés dans le processus de production comprennent des facteurs de production primaires tels que les services de main-d'œuvre, le capital (biens durables utilisés dans la production, comme une usine existante) et la terre (y compris les ressources naturelles).
L'efficacité est améliorée si plus de sortie est générée sans changer les entrées, ou en d'autres termes, la quantité de "déchets" est réduite.
Dans le cas le plus simple, une économie ne peut produire que deux biens (disons "des armes à feu" et du "beurre").
La rareté est représentée dans la figure par les personnes désireuses mais incapables dans l'ensemble de consommer au-delà du PPF (comme à X) et par la pente négative de la courbe.
La pente de la courbe en un point donne le compromis entre les deux biens.
Le long du PPF, la rareté implique que choisir plus d'un bien dans l'agrégat implique de faire avec moins de l'autre bien.
Un point à l'intérieur de la courbe (comme en A) est faisable mais représente l'inefficacité de la production (utilisation inutile d'intrants), dans la mesure où la production d'un ou des deux biens pourrait augmenter en se déplaçant dans une direction nord-est vers un point de la courbe.
Il a été observé qu'un volume élevé d'échanges se produit entre les régions, même avec un accès à une technologie et à une combinaison d'intrants de facteurs similaires, y compris les pays à revenu élevé.
Parmi chacun de ces systèmes de production, il peut y avoir une division du travail correspondante avec différents groupes de travail spécialisés, ou en conséquence différents types de biens d'équipement et des utilisations différenciées des terres.
La théorie et l'observation établissent les conditions telles que les prix du marché des extrants et des intrants productifs sélectionnent une allocation d'intrants de facteurs par avantage comparatif, de sorte que les intrants (relativement) à faible coût servent à produire des extrants à faible coût.
En microéconomie, cela s'applique à la détermination des prix et de la production pour un marché en concurrence parfaite, ce qui inclut la condition d'absence d'acheteurs ou de vendeurs suffisamment importants pour avoir un pouvoir de fixation des prix.
La théorie de la demande décrit les consommateurs individuels comme choisissant rationnellement la quantité préférée de chaque bien, compte tenu du revenu, des prix, des goûts, etc.
La loi de la demande stipule qu'en général, le prix et la quantité demandée sur un marché donné sont inversement liés.
De plus, le pouvoir d'achat résultant de la baisse des prix augmente la capacité d'achat (l'effet revenu).
L'offre est le rapport entre le prix d'un bien et la quantité disponible à la vente à ce prix.
L'offre est généralement représentée comme une fonction reliant le prix et la quantité, si les autres facteurs sont inchangés.
Tout comme du côté de la demande, la position de l'offre peut changer, par exemple à la suite d'un changement du prix d'un intrant productif ou d'une amélioration technique.
L'équilibre du marché se produit lorsque la quantité offerte est égale à la quantité demandée, l'intersection des courbes d'offre et de demande dans la figure ci-dessus.
A un prix supérieur à l'équilibre, il y a un surplus de quantité offerte par rapport à la quantité demandée.
Les types d'entreprises les plus évidents sont les sociétés par actions, les partenariats et les fiducies.
Dans les marchés parfaitement concurrentiels étudiés dans la théorie de l'offre et de la demande, il existe de nombreux producteurs, dont aucun n'influence significativement le prix.
Outre la concurrence parfaite, les structures de marché communes étudiées comprennent la concurrence monopolistique, diverses formes d'oligopole et le monopole.
Compte tenu de ses différentes formes, il existe différentes manières de représenter l'incertitude et de modéliser les réponses des agents économiques à celle-ci.
En économie comportementale, il a été utilisé pour modéliser les stratégies choisies par les agents lorsqu'ils interagissent avec d'autres dont les intérêts sont au moins partiellement opposés aux leurs.
Il a des applications importantes apparemment en dehors de l'économie dans des sujets aussi divers que la formulation de stratégies nucléaires, l'éthique, la science politique et la biologie de l'évolution.
Il analyse également la tarification des instruments financiers, la structure financière des entreprises, l'efficacité et la fragilité des marchés financiers, les crises financières et les politiques ou réglementations gouvernementales connexes.
Les clients qui ne savent pas si une voiture est un "citron" abaissent son prix en dessous de ce que serait une voiture d'occasion de qualité.
Ces deux problèmes peuvent augmenter les coûts d'assurance et réduire l'efficacité en éloignant du marché des agents autrement volontaires ("marchés incomplets").
Les asymétries d'information et les marchés incomplets peuvent entraîner une inefficacité économique, mais aussi une possibilité d'améliorer l'efficacité grâce à des solutions commerciales, juridiques et réglementaires, comme indiqué ci-dessus.
Les biens publics sont des biens qui sont sous-approvisionnés sur un marché typique.
Par exemple, la pollution de l'air peut générer une externalité négative, et l'éducation peut générer une externalité positive (moins de criminalité, etc.).
Dans de nombreux domaines, une certaine forme de rigidité des prix est postulée pour tenir compte des quantités plutôt que des prix, s'ajustant à court terme aux changements du côté de la demande ou du côté de l'offre.
Des exemples d'une telle rigidité des prix sur des marchés particuliers incluent les taux de salaire sur les marchés du travail et les prix affichés sur des marchés qui s'écartent de la concurrence parfaite.
Ces agrégats comprennent le revenu et la production nationaux, le taux de chômage, l'inflation des prix et des sous-agrégats comme la consommation totale et les dépenses d'investissement et leurs composantes.
Cela a répondu à une préoccupation de longue date concernant les développements incohérents du même sujet.
Keynes a soutenu que la demande globale de biens pourrait être insuffisante pendant les ralentissements économiques, entraînant un chômage inutilement élevé et des pertes de production potentielle.
La nouvelle macroéconomie classique, à la différence de la vision keynésienne du cycle économique, postule un équilibre du marché avec des informations imparfaites.
La population active ne comprend que les travailleurs en recherche active d'emploi.
Les modèles classiques de chômage se produisent lorsque les salaires sont trop élevés pour que les employeurs soient disposés à embaucher plus de travailleurs.
Un chômage structurel important peut survenir lorsqu'une économie est en transition industrielle et que les travailleurs constatent que leur ensemble de compétences antérieur n'est plus en demande.
L'argent a une acceptabilité générale, une cohérence relative en valeur, une divisibilité, une durabilité, une portabilité, une élasticité de l'offre et une longévité avec la confiance du public de masse.
Pour reprendre les mots de Francis Amasa Walker, un économiste bien connu du 19ème siècle, "L'argent est ce que l'argent fait" ("L'argent est ce que l'argent fait" dans l'original).
Sa fonction économique peut être opposée au troc (échange non monétaire).
Lorsque la demande globale tombe en dessous de la production potentielle de l'économie, il y a un écart de production où une partie de la capacité de production est laissée au chômage.
Par exemple, des constructeurs de maisons sans emploi peuvent être embauchés pour agrandir des autoroutes.
Les effets de la politique budgétaire peuvent être limités par l'éviction.
Certains économistes pensent que l'éviction est toujours un problème tandis que d'autres ne pensent pas que ce soit un problème majeur lorsque la production est déprimée.
Cette dernière, un aspect de la théorie des choix publics, modélise le comportement du secteur public de manière analogue à la microéconomie, impliquant des interactions d'électeurs, de politiciens et de bureaucrates intéressés.
Elle concerne également la taille et la distribution des gains du commerce.
On dit souvent que Carlyle a donné à l'économie le surnom de "science lamentable" en réponse aux écrits de la fin du XVIIIe siècle du révérend Thomas Robert Malthus, qui prédisait sinistrement que la famine en résulterait, car la croissance démographique prévue dépassait le taux d'augmentation de la population. approvisionnement alimentaire.
La relation étroite de la théorie et de la pratique économiques avec la politique est un sujet de discorde qui peut obscurcir ou déformer les principes originaux les moins prétentieux de l'économie, et est souvent confondue avec des agendas sociaux et des systèmes de valeurs spécifiques.
Certaines revues économiques universitaires ont intensifié leurs efforts pour évaluer le consensus des économistes sur certaines questions politiques dans l'espoir de créer un environnement politique plus éclairé.
Des questions telles que l'indépendance de la banque centrale, les politiques de la banque centrale et la rhétorique dans le discours des gouverneurs de la banque centrale ou les prémisses des politiques macroéconomiques (politique monétaire et budgétaire) de l'État sont au centre des controverses et des critiques.
Le domaine de l'économie de l'information comprend à la fois la recherche mathématique et économique ainsi que l'économie comportementale, apparentée aux études en psychologie comportementale, et les facteurs confondants des hypothèses néoclassiques font l'objet d'études approfondies dans de nombreux domaines de l'économie.
Joskow avait le fort sentiment que le travail important dans l'oligopole était effectué par le biais d'observations informelles tandis que les modèles formels étaient "étalés ex post".
Un autre thème majeur est l'évolution, qui explique l'unité et la diversité de la vie.
Ses travaux tels que History of Animals étaient particulièrement importants car ils révélaient ses penchants naturalistes, et plus tard des travaux plus empiriques axés sur la causalité biologique et la diversité de la vie.
La médecine a été particulièrement bien étudiée par les érudits islamiques travaillant dans les traditions des philosophes grecs, tandis que l'histoire naturelle s'inspirait fortement de la pensée aristotélicienne, en particulier pour maintenir une hiérarchie fixe de la vie.
Les recherches de Jan Swammerdam ont suscité un nouvel intérêt pour l'entomologie et ont aidé à développer les techniques de base de la dissection et de la coloration microscopiques.
Puis, en 1838, Schleiden et Schwann ont commencé à promouvoir les idées désormais universelles selon lesquelles (1) l'unité de base des organismes est la cellule et (2) que les cellules individuelles ont toutes les caractéristiques de la vie, bien qu'ils se soient opposés à l'idée que (3) toutes les cellules proviennent de la division d'autres cellules.
Carl Linnaeus a publié une taxonomie de base pour le monde naturel en 1735 (dont des variantes sont utilisées depuis) et, dans les années 1750, a introduit des noms scientifiques pour toutes ses espèces.
Lamarck croyait que ces traits acquis pourraient ensuite être transmis à la progéniture de l'animal, qui les développerait et les perfectionnerait davantage.
La base de la génétique moderne a commencé avec les travaux de Gregor Mendel, qui a présenté son article, "Versuche über Pflanzenhybriden" ("Expériences sur l'hybridation des plantes"), en 1865, qui décrivait les principes de l'héritage biologique, servant de base à la génétique moderne. .
L'accent mis sur de nouveaux types d'organismes modèles tels que les virus et les bactéries, ainsi que la découverte de la structure en double hélice de l'ADN par James Watson et Francis Crick en 1953, ont marqué la transition vers l'ère de la génétique moléculaire.
Enfin, le projet du génome humain a été lancé en 1990 dans le but de cartographier le génome humain général.
La vie sur Terre a commencé à partir de l'eau et y est restée pendant environ trois milliards d'années avant de migrer vers la terre.
Le noyau est composé d'un ou plusieurs protons et d'un certain nombre de neutrons.
L'atome de chaque élément spécifique contient un nombre unique de protons, connu sous le nom de numéro atomique, et la somme de ses protons et de ses neutrons est le nombre de masse d'un atome.
Le carbone, par exemple, peut exister en tant qu'isotope stable (carbone 12 ou carbone 13) ou en tant qu'isotope radioactif (carbone 14), ce dernier pouvant être utilisé dans la datation radiométrique (en particulier la datation au radiocarbone) pour déterminer l'âge de matières organiques.
La liaison ionique implique l'attraction électrostatique entre des ions chargés de manière opposée, ou entre deux atomes avec des électronégativités très différentes, et est la principale interaction se produisant dans les composés ioniques.
Contrairement aux liaisons ioniques, une liaison covalente implique le partage de paires d'électrons entre les atomes.
Un exemple omniprésent de liaison hydrogène se trouve entre les molécules d'eau.
L'eau est importante pour la vie car c'est un solvant efficace, capable de dissoudre des solutés tels que les ions sodium et chlorure ou d'autres petites molécules pour former une solution aqueuse.
Comme les liaisons O-H sont polaires, l'atome d'oxygène a une légère charge négative et les deux atomes d'hydrogène ont une légère charge positive.
L'eau est également adhésive car elle est capable d'adhérer à la surface de toutes les molécules non aqueuses polaires ou chargées.
La densité plus faible de la glace par rapport à l'eau liquide est due au nombre inférieur de molécules d'eau qui forment la structure du réseau cristallin de la glace, ce qui laisse une grande quantité d'espace entre les molécules d'eau.
Ainsi, une grande quantité d'énergie est nécessaire pour rompre les liaisons hydrogène entre les molécules d'eau afin de transformer l'eau liquide en gaz (ou vapeur d'eau).
À l'exception de l'eau, presque toutes les molécules qui composent chaque organisme contiennent du carbone.
Par exemple, un seul atome de carbone peut former quatre liaisons covalentes simples comme dans le méthane, deux doubles liaisons covalentes comme dans le dioxyde de carbone ou une triple liaison covalente comme dans le monoxyde de carbone (CO).
Un squelette hydrocarboné peut être remplacé par d'autres éléments tels que l'oxygène (O), l'hydrogène (H), le phosphore (P) et le soufre (S), qui peuvent modifier le comportement chimique de ce composé.
Lorsque deux monosaccharides tels que le glucose et le fructose sont liés ensemble, ils peuvent former un disaccharide tel que le saccharose.
Ces lipides sont des composés organiques largement non polaires et hydrophobes.
Le groupe glycérol et phosphate constitue ensemble la région polaire et hydrophile (ou tête) de la molécule tandis que les acides gras constituent la région non polaire et hydrophobe (ou queue).
Les protéines sont les macromolécules les plus diverses, qui comprennent les enzymes, les protéines de transport, les grandes molécules de signalisation, les anticorps et les protéines structurelles.
La polarité et la charge des chaînes latérales affectent la solubilité des acides aminés.
La structure primaire consiste en une séquence unique d'acides aminés qui sont liés de manière covalente par des liaisons peptidiques.
Le repliement des hélices alpha et des feuillets bêta donne à une protéine sa structure tridimensionnelle ou tertiaire.
Les purines comprennent la guanine (G) et l'adénine (A) tandis que les pyrimidines sont constituées de cytosine (T), d'uracile (U) et de thymine (T).
Une membrane cellulaire est constituée d'une bicouche lipidique, comprenant des cholestérols qui se trouvent entre les phospholipides pour maintenir leur fluidité à différentes températures.
Les membranes cellulaires sont impliquées dans divers processus cellulaires tels que l'adhésion cellulaire, le stockage de l'énergie électrique et la signalisation cellulaire et servent de surface de fixation pour plusieurs structures extracellulaires telles qu'une paroi cellulaire, un glycocalyx et un cytosquelette.
Le texte d'Alberts explique comment les "blocs de construction cellulaires" se déplacent pour façonner les embryons en développement.
Les cellules végétales ont des organites supplémentaires qui les distinguent des cellules animales telles qu'une paroi cellulaire qui fournit un support à la cellule végétale, des chloroplastes qui récoltent l'énergie solaire pour produire du sucre et des vacuoles qui fournissent un stockage et un support structurel tout en étant impliquées dans la reproduction et la dégradation. de graines de plantes.
Selon la première loi de la thermodynamique, l'énergie est conservée, c'est-à-dire qu'elle ne peut être ni créée ni détruite.
En conséquence, un organisme nécessite un apport continu d'énergie pour maintenir un faible état d'entropie.
Habituellement, le catabolisme libère de l'énergie et l'anabolisme en consomme.
La réaction globale se produit en une série d'étapes biochimiques, dont certaines sont des réactions redox.
L'acétyl-Coa entre dans le cycle de l'acide citrique, qui se déroule à l'intérieur de la matrice mitochondriale.
La phosphorylation oxydative comprend la chaîne de transport d'électrons, qui est une série de quatre complexes protéiques qui transfèrent des électrons d'un complexe à un autre, libérant ainsi de l'énergie de NADH et FADH2 qui est couplée au pompage de protons (ions hydrogène) à travers la membrane mitochondriale interne ( chimiosmose), qui génère une force motrice de protons.
Si l'oxygène n'était pas présent, le pyruvate ne serait pas métabolisé par la respiration cellulaire mais subirait un processus de fermentation.
La fermentation oxyde le NADH en NAD+ afin qu'il puisse être réutilisé dans la glycolyse.
Dans les muscles squelettiques, le déchet est l'acide lactique.
Au cours de la glycolyse anaérobie, le NAD+ se régénère lorsque des paires d'hydrogène se combinent avec du pyruvate pour former du lactate.
Pendant la récupération, lorsque l'oxygène devient disponible, le NAD+ se fixe à l'hydrogène du lactate pour former de l'ATP.
Dans la plupart des cas, l'oxygène est également libéré en tant que déchet.
Ceci est analogue à la force proton-motrice générée à travers la membrane mitochondriale interne lors de la respiration aérobie.
Dans la signalisation autocrine, le ligand affecte la même cellule qui le libère.
Chez les eucaryotes (c'est-à-dire les cellules animales, végétales, fongiques et protistes), il existe deux types distincts de division cellulaire : la mitose et la méiose.
Après la division cellulaire, chacune des cellules filles entame l'interphase d'un nouveau cycle.
Ces deux cycles de division cellulaire sont utilisés dans le processus de reproduction sexuée à un moment donné de leur cycle de vie.
Contrairement aux processus de mitose et de méiose chez les eucaryotes, la fission binaire prend chez les procaryotes sans la formation d'un appareil à fuseau sur la cellule.
L'hérédité mendélienne, en particulier, est le processus par lequel les gènes et les traits sont transmis des parents à la progéniture.
La première est que les caractéristiques génétiques, qui sont maintenant appelées allèles, sont discrètes et ont des formes alternées (par exemple, violet contre blanc ou grand contre nain), chacune héritée de l'un des deux parents.
Mendel a noté que lors de la formation des gamètes, les allèles de chaque gène se séparent les uns des autres de sorte que chaque gamète ne porte qu'un seul allèle pour chaque gène, ce qui est énoncé par sa loi de ségrégation.
Les nucléotides sont reliés les uns aux autres dans une chaîne par des liaisons covalentes entre le sucre d'un nucléotide et le phosphate du suivant, ce qui donne un squelette sucre-phosphate alterné.
Les bases sont divisées en deux groupes : les pyrimidines et les purines.
L'ADN est répliqué une fois les deux brins séparés.
Un chromosome est une structure organisée composée d'ADN et d'histones.
Chez les procaryotes, l'ADN est contenu dans un corps de forme irrégulière dans le cytoplasme appelé le nucléoïde.
L'information génétique stockée dans l'ADN représente le génotype, tandis que le phénotype résulte de la synthèse de protéines qui contrôlent la structure et le développement d'un organisme, ou qui agissent comme des enzymes catalysant des voies métaboliques spécifiques.
Sous le code génétique, ces brins d'ARNm spécifient la séquence d'acides aminés dans les protéines dans un processus appelé traduction, qui se produit dans les ribosomes.
Le séquençage et l'analyse des génomes peuvent être effectués en utilisant le séquençage d'ADN à haut débit et la bioinformatique pour assembler et analyser la fonction et la structure de génomes entiers.
Les génomes des procaryotes sont petits, compacts et diversifiés.
Il existe quatre processus clés qui sous-tendent le développement : la détermination, la différenciation, la morphogenèse et la croissance.
Les cellules souches sont des cellules indifférenciées ou partiellement différenciées qui peuvent se différencier en différents types de cellules et proliférer indéfiniment pour produire davantage de la même cellule souche.
L'apoptose, ou mort cellulaire programmée, se produit également au cours de la morphogenèse, comme la mort des cellules entre les doigts dans le développement embryonnaire humain, ce qui libère les doigts et les orteils individuels.
Ces gènes de la boîte à outils sont hautement conservés parmi les phylums, ce qui signifie qu'ils sont anciens et très similaires dans des groupes d'animaux largement séparés.
Les gènes Hox déterminent où les parties répétitives, telles que les nombreuses vertèbres des serpents, se développeront dans un embryon ou une larve en développement.
Un gène de la boîte à outils peut être exprimé selon un schéma différent, comme lorsque le bec du grand pinson terrestre de Darwin a été agrandi par le gène BMP, ou lorsque les serpents ont perdu leurs pattes lorsque les gènes Distal-less (Dlx) sont devenus sous-exprimés ou non exprimés à tous dans les endroits où d'autres reptiles continuaient à former leurs membres.
Cette perspective soutient que l'évolution se produit lorsqu'il y a des changements dans les fréquences alléliques au sein d'une population d'organismes qui se reproduisent.
Lorsque les forces sélectives sont absentes ou relativement faibles, les fréquences alléliques sont également susceptibles de dériver vers le haut ou vers le bas à chaque génération successive car les allèles sont sujets à une erreur d'échantillonnage.
L'isolement reproductif a également tendance à augmenter avec la divergence génétique.
Lorsqu'une lignée se divise en deux, elle est représentée comme un nœud (ou scission) sur l'arbre phylogénétique.
Dans un arbre, tout groupe d'espèces désigné par un nom est un taxon (par exemple, les humains, les primates, les mammifères ou les vertébrés) et un taxon composé de tous ses descendants évolutifs est un clade, autrement appelé taxon monophylétique.
Une espèce ou un groupe qui est étroitement lié à l'endogroupe mais qui en est phylogénétiquement extérieur est appelé l'exogroupe, qui sert de point de référence dans l'arbre.
Basé sur le principe de parcimonie (ou rasoir d'Occam), l'arbre qui est privilégié est celui qui a le moins de changements évolutifs à supposer sur tous les traits dans tous les groupes.
Sur la base de ce système, chaque espèce reçoit deux noms, un pour son genre et un autre pour son espèce.
Les biologistes considèrent l'ubiquité du code génétique comme la preuve d'une descendance commune universelle pour toutes les bactéries, archées et eucaryotes.
Plus tard, il y a environ 1,7 milliard d'années, des organismes multicellulaires ont commencé à apparaître, avec des cellules différenciées remplissant des fonctions spécialisées.
Les plantes terrestres ont connu un tel succès qu'on pense qu'elles ont contribué à l'événement d'extinction du Dévonien supérieur.
Lors de la récupération de cette catastrophe, les archosaures sont devenus les vertébrés terrestres les plus abondants; un groupe d'archosaures, les dinosaures, a dominé les périodes du Jurassique et du Crétacé.
Les bactéries habitent le sol, l'eau, les sources chaudes acides, les déchets radioactifs et la biosphère profonde de la croûte terrestre.
Les archées constituent l'autre domaine des cellules procaryotes et ont été initialement classées comme bactéries, recevant le nom d'archaebactéries (dans le royaume des archéobactéries), un terme qui est tombé en désuétude.
Les archées et les bactéries sont généralement de taille et de forme similaires, bien que quelques archées aient des formes très différentes, comme les cellules plates et carrées de Haloquadratum walsbyi.
Les archées utilisent plus de sources d'énergie que les eucaryotes : celles-ci vont des composés organiques, tels que les sucres, à l'ammoniac, aux ions métalliques ou même à l'hydrogène gazeux.
Les premières archées observées étaient des extrêmophiles, vivant dans des environnements extrêmes, tels que des sources chaudes et des lacs salés sans autres organismes.
Les archées font partie intégrante de la vie sur Terre.
Cinq de ces clades sont également connus collectivement sous le nom de protistes, qui sont pour la plupart des organismes eucaryotes microscopiques qui ne sont pas des plantes, des champignons ou des animaux.
La plupart des protistes sont unicellulaires, également appelés eucaryotes microbiens.
Les dinoflagellés sont photosynthétiques et peuvent être trouvés dans l'océan où ils jouent un rôle de producteurs primaires de matière organique.
Les ciliés sont des alvéolés qui possèdent de nombreuses structures ressemblant à des cheveux appelées cils.
Les fouilles sont des groupes de protistes qui ont commencé à se diversifier il y a environ 1,5 milliard d'années peu après l'origine des eucaryotes.
Les straménopiles, dont la plupart peuvent être caractérisés par la présence de poils tubulaires sur le plus long de leurs deux flagelles, comprennent les diatomées et les algues brunes.
Les rhizaires comprennent trois groupes principaux : les cercozoaires, les foraminifères et les radiolaires.
Les algues comprennent plusieurs clades distincts tels que les glaucophytes, qui sont des algues d'eau douce microscopiques qui peuvent avoir ressemblé par leur forme à l'ancêtre unicellulaire précoce de Plantae.
Les plantes terrestres (embryophytes) sont apparues pour la première fois dans les environnements terrestres il y a environ 450 à 500 millions d'années.
En revanche, les trois autres clades sont des plantes non vasculaires car elles n'ont pas de trachéides.
Ils ont tendance à se trouver dans les zones où l'eau est facilement disponible.
La plupart des plantes non vasculaires sont terrestres, quelques-unes vivant dans des environnements d'eau douce et aucune ne vivant dans les océans.
Les gymnospermes comprennent les conifères, les cycadales, le ginkgo et les gnetophytes.
Ils le font par un processus appelé hétérotrophie absorbante par lequel ils sécrètent d'abord des enzymes digestives qui décomposent les grosses molécules alimentaires avant de les absorber à travers leurs membranes cellulaires.
Les champignons, ainsi que deux autres lignées, les choanoflagellés et les animaux, peuvent être regroupés en opisthokonts.
Les champignons multicellulaires, quant à eux, ont un corps appelé mycélium, qui est composé d'une masse de filaments tubulaires individuels appelés hyphes qui permettent l'absorption des nutriments.
À quelques exceptions près, les animaux consomment de la matière organique, respirent de l'oxygène, sont capables de se déplacer, peuvent se reproduire sexuellement et se développer à partir d'une sphère creuse de cellules, la blastula, au cours du développement embryonnaire.
Les animaux peuvent être distingués en deux groupes en fonction de leurs caractéristiques de développement.
Dans les protostomes, le blastopore donne naissance à la bouche, qui est ensuite suivie par la formation de l'anus.
Les corps de la plupart des animaux sont symétriques, la symétrie étant radiale ou bilatérale.
Enfin, les animaux peuvent être distingués en fonction du type et de l'emplacement de leurs appendices tels que des antennes pour détecter l'environnement ou des griffes pour capturer des proies.
La majorité (~ 97 %) des espèces animales sont des invertébrés, c'est-à-dire des animaux qui ne possèdent ni ne développent de colonne vertébrale (communément appelée colonne vertébrale ou colonne vertébrale), dérivée de la notocorde.
De nombreux taxons d'invertébrés ont un plus grand nombre et une plus grande variété d'espèces que l'ensemble du sous-embranchement des vertébrés.
Plus de 6 000 espèces de virus ont été décrites en détail.
Lorsqu'ils ne sont pas à l'intérieur d'une cellule infectée ou en train d'infecter une cellule, les virus existent sous la forme de particules indépendantes, ou virions, constituées du matériel génétique (ADN ou ARN), d'une enveloppe protéique appelée capside et, dans certains cas, d'une enveloppe externe. enveloppe de lipides.
Les origines des virus dans l'histoire évolutive de la vie ne sont pas claires : certains peuvent avoir évolué à partir de plasmides - des morceaux d'ADN qui peuvent se déplacer entre les cellules - tandis que d'autres peuvent avoir évolué à partir de bactéries.
Les virus peuvent se propager de plusieurs façons.
Les norovirus et les rotavirus, causes courantes de gastro-entérite virale, sont transmis par la voie féco-orale, par contact main-bouche ou dans les aliments ou l'eau.
Le système de pousses est composé de tiges, de feuilles et de fleurs.
La direction du mouvement de l'eau à travers une membrane semi-perméable est déterminée par le potentiel de l'eau à travers cette membrane.
La plupart des graines de plantes sont généralement dormantes, une condition dans laquelle l'activité normale de la graine est suspendue.
L'imbibition est la première étape de la germination, au cours de laquelle l'eau est absorbée par la graine.
Ces monomères sont obtenus à partir de l'hydrolyse de l'amidon, des protéines et des lipides stockés dans les cotylédons ou l'endosperme.
Leurs fleurs sont des organes qui facilitent la reproduction, généralement en fournissant un mécanisme pour l'union des spermatozoïdes avec les ovules.
La pollinisation croisée est le transfert de pollen de l'anthère d'une fleur au stigmate d'une autre fleur sur un individu différent de la même espèce.
Ces changements peuvent être affectés par des facteurs génétiques, chimiques et physiques.
Les protéines photoréceptrices transmettent des informations telles que s'il fait jour ou nuit, la durée du jour, l'intensité de la lumière disponible et la source de lumière.
De nombreuses plantes à fleurs fleurissent au moment opportun en raison de composés sensibles à la lumière qui réagissent à la durée de la nuit, un phénomène connu sous le nom de photopériodisme.
Les animaux peuvent être classés comme régulateurs ou conformateurs.
En revanche, les animaux tels que les poissons et les grenouilles sont des conformateurs car ils adaptent leur environnement interne (par exemple, la température corporelle) pour correspondre à leurs environnements externes.
Les souris, par exemple, sont capables de consommer trois fois plus de nourriture que les lapins proportionnellement à leur poids car le taux métabolique de base par unité de poids chez les souris est supérieur à celui des lapins.
Cependant, la relation n'est pas linéaire chez les animaux qui nagent ou volent.
À basse vitesse de vol, un oiseau doit maintenir un taux métabolique élevé pour rester en l'air.
Enfin, les animaux d'eau douce ont des fluides corporels qui sont hyperosmotiques à l'eau douce.
Si un animal devait consommer des aliments qui contiennent une quantité excessive d'énergie chimique, il stockera la majeure partie de cette énergie sous forme de lipides pour une utilisation future et une partie de cette énergie sous forme de glycogène pour une utilisation plus immédiate (par exemple, répondre aux besoins énergétiques du cerveau ).
En plus de leur tube digestif, les animaux vertébrés ont des glandes accessoires telles qu'un foie et un pancréas dans le cadre de leur système digestif.
En quittant l'estomac, les aliments pénètrent dans l'intestin moyen, qui est la première partie de l'intestin (ou intestin grêle chez les mammifères) et est le principal site de digestion et d'absorption.
Les échanges gazeux dans les poumons se produisent dans des millions de petits sacs aériens; chez les mammifères et les reptiles, on les appelle des alvéoles et chez les oiseaux, on les appelle des oreillettes.
Ceux-ci pénètrent dans les poumons où ils se ramifient dans des bronches secondaires et tertiaires de plus en plus étroites qui se ramifient en de nombreux tubes plus petits, les bronchioles.
Il existe deux types de systèmes circulatoires : ouvert et fermé.
La circulation chez les animaux se produit entre deux types de tissus : les tissus systémiques et les organes respiratoires (ou pulmonaires).
Chez les oiseaux et les mammifères, les systèmes systémique et pulmonaire sont connectés en série.
Les contractions des muscles squelettiques sont neurogènes car elles nécessitent une entrée synaptique des motoneurones.
La contraction produite peut être décrite comme une secousse, une sommation ou un tétanos, selon la fréquence des potentiels d'action.
Les mécanismes de contraction sont similaires dans les trois tissus musculaires.
D'autres animaux tels que les mollusques et les nématodes possèdent des muscles striés obliquement, qui contiennent des bandes de filaments épais et minces disposés en hélice plutôt que transversalement, comme dans les muscles squelettiques ou cardiaques des vertébrés.
Ils peuvent transmettre ou recevoir des informations sur des sites de contacts appelés synapses.
Des cellules telles que des neurones ou des cellules musculaires peuvent être excitées ou inhibées lors de la réception d'un signal provenant d'un autre neurone.
Chez les vertébrés, le système nerveux se compose du système nerveux central (SNC), qui comprend le cerveau et la moelle épinière, et du système nerveux périphérique (SNP), qui se compose de nerfs qui relient le SNC à toutes les autres parties du corps.
Le SNP est divisé en trois sous-systèmes distincts, les systèmes nerveux somatique, autonome et entérique.
Le système nerveux sympathique est activé en cas d'urgence pour mobiliser l'énergie, tandis que le système nerveux parasympathique est activé lorsque les organismes sont dans un état détendu.
Les nerfs qui sortent directement du cerveau sont appelés nerfs crâniens tandis que ceux qui sortent de la moelle épinière sont appelés nerfs rachidiens.
Chez l'homme en particulier, les principales glandes endocrines sont la glande thyroïde et les glandes surrénales.
Les hormones peuvent être des complexes d'acides aminés, des stéroïdes, des eicosanoïdes, des leucotriènes ou des prostaglandines.
Ils produisent des gamètes haploïdes par méiose.
Dans la plupart des cas, une troisième couche germinale, le mésoderme, se développe également entre eux.
La gastrulation se produit, par laquelle les mouvements morphogénétiques convertissent la masse cellulaire en trois couches germinales qui comprennent l'ectoderme, le mésoderme et l'endoderme.
La différenciation cellulaire est influencée par des signaux extracellulaires tels que des facteurs de croissance qui sont échangés avec les cellules adjacentes, appelée signalisation juxtracrine, ou avec les cellules voisines sur de courtes distances, appelée signalisation paracrine.
Le système immunitaire adaptatif fournit une réponse adaptée à chaque stimulus en apprenant à reconnaître les molécules qu'il a rencontrées précédemment.
Les bactéries ont un système immunitaire rudimentaire sous la forme d'enzymes qui protègent contre les infections virales.
Les vertébrés à mâchoires, y compris les humains, ont des mécanismes de défense encore plus sophistiqués, notamment la capacité de s'adapter pour reconnaître plus efficacement les agents pathogènes.
Les modèles d'action fixes, par exemple, sont des comportements génétiquement déterminés et stéréotypés qui se produisent sans apprentissage.
La communauté d'organismes vivants (biotiques) en conjonction avec les composants non vivants (abiotiques) (par exemple, l'eau, la lumière, le rayonnement, la température, l'humidité, l'atmosphère, l'acidité et le sol) de leur environnement s'appelle un écosystème.
En se nourrissant de plantes et les uns des autres, les animaux jouent un rôle important dans le mouvement de la matière et de l'énergie à travers le système.
L'environnement physique de la Terre est façonné par l'énergie solaire et la topographie.
Le temps est la température quotidienne et l'activité des précipitations, tandis que le climat est la moyenne à long terme des conditions météorologiques, généralement calculée sur une période de 30 ans.
En conséquence, les environnements humides permettent à une végétation luxuriante de se développer.
La croissance démographique pendant des intervalles à court terme peut être déterminée à l'aide de l'équation du taux de croissance démographique, qui tient compte des taux de natalité, de mortalité et d'immigration.
Une interaction biologique est l'effet que deux organismes vivant ensemble dans une communauté ont l'un sur l'autre.
Une interaction à long terme s'appelle une symbiose.
Il existe différents niveaux trophiques dans tout réseau trophique, le niveau le plus bas étant les producteurs primaires (ou autotrophes) tels que les plantes et les algues qui convertissent l'énergie et les matières inorganiques en composés organiques, qui peuvent ensuite être utilisés par le reste de la communauté.
Et ceux qui mangent des consommateurs secondaires sont des consommateurs tertiaires et ainsi de suite.
Dans certains cycles, il existe des réservoirs où une substance reste ou est séquestrée pendant une longue période de temps.
Le principal facteur de réchauffement est l'émission de gaz à effet de serre, dont plus de 90 % sont du dioxyde de carbone et du méthane.
La biodiversité affecte le fonctionnement des écosystèmes, qui fournissent une variété de services dont dépendent les populations.
Traditionnellement, la botanique a également inclus l'étude des champignons et des algues par les mycologues et les phycologues respectivement, l'étude de ces trois groupes d'organismes restant dans la sphère d'intérêt du Congrès international de botanique.
Les jardins médicaux médiévaux, souvent rattachés aux monastères, contenaient des plantes d'importance médicale.
Ces jardins ont facilité l'étude académique des plantes.
Au cours des deux dernières décennies du XXe siècle, les botanistes ont exploité les techniques d'analyse génétique moléculaire, y compris la génomique et la protéomique et les séquences d'ADN pour classer les plantes avec plus de précision.
La botanique moderne trouve ses racines dans la Grèce antique, en particulier à Théophraste (vers 371-287 avant notre ère), un élève d'Aristote qui a inventé et décrit nombre de ses principes et est largement considéré dans la communauté scientifique comme le "père de la botanique".
De Materia Medica a été largement lu pendant plus de 1 500 ans.
Au milieu du XVIe siècle, des jardins botaniques ont été créés dans plusieurs universités italiennes.
Ils ont soutenu la croissance de la botanique en tant que matière académique.
Pendant toute cette période, la botanique est restée fermement subordonnée à la médecine.
Bock a créé son propre système de classification des plantes.
Le choix et l'enchaînement des caractères peuvent être artificiels dans les clés d'identification (clés diagnostiques) ou plus étroitement liés à l'ordre naturel ou phylétique des taxons dans les clés synoptiques.
Cela a établi un schéma de dénomination binomial ou en deux parties standardisé où le premier nom représentait le genre et le second identifiait l'espèce au sein du genre.
La connaissance croissante de l'anatomie, de la morphologie et des cycles de vie des plantes a conduit à la réalisation qu'il y avait plus d'affinités naturelles entre les plantes que le système sexuel artificiel de Linnaeus.
Les travaux de Katherine Esau (1898-1997) sur l'anatomie végétale sont toujours un fondement majeur de la botanique moderne.
Le concept selon lequel la composition des communautés végétales telles que les forêts de feuillus tempérées change par un processus de succession écologique a été développé par Henry Chandler Cowles, Arthur Tansley et Frederic Clements.
La découverte et l'identification des hormones végétales auxine par Kenneth V. Thimann en 1948 ont permis la régulation de la croissance des plantes par des produits chimiques appliqués de l'extérieur.
Les développements du 20ème siècle en biochimie végétale ont été conduits par des techniques modernes d'analyse chimique organique, telles que la spectroscopie, la chromatographie et l'électrophorèse.
Ces technologies permettent l'utilisation biotechnologique de plantes entières ou de cultures de cellules végétales cultivées dans des bioréacteurs pour synthétiser des pesticides, des antibiotiques ou d'autres produits pharmaceutiques, ainsi que l'application pratique de cultures génétiquement modifiées conçues pour des caractéristiques telles que l'amélioration du rendement.
La systématique moderne vise à réfléchir et à découvrir les relations phylogénétiques entre les plantes.
En tant que sous-produit de la photosynthèse, les plantes libèrent de l'oxygène dans l'atmosphère, un gaz nécessaire à presque tous les êtres vivants pour effectuer la respiration cellulaire.
Historiquement, tous les êtres vivants étaient classés comme animaux ou plantes et la botanique couvrait l'étude de tous les organismes non considérés comme des animaux.
La définition la plus stricte de «plante» comprend uniquement les «plantes terrestres» ou embryophytes, qui comprennent les plantes à graines (gymnospermes, y compris les pins et les plantes à fleurs) et les cryptogames à spores libres, y compris les fougères, les lycopodes, les hépatiques, les hornworts et les mousses.
La phase haploïde sexuelle des embryophytes, connue sous le nom de gamétophyte, nourrit le sporophyte embryonnaire diploïde en développement dans ses tissus pendant au moins une partie de sa vie, même dans les plantes à graines, où le gamétophyte lui-même est nourri par son sporophyte parent.
Les paléobotanistes étudient les plantes anciennes dans les archives fossiles pour fournir des informations sur l'histoire évolutive des plantes.
C'est ce que les écologistes appellent le premier niveau trophique.
Les botanistes étudient également les mauvaises herbes, qui constituent un problème considérable en agriculture, ainsi que la biologie et le contrôle des agents pathogènes des plantes dans l'agriculture et les écosystèmes naturels.
L'énergie lumineuse capturée par la chlorophylle a se présente initialement sous la forme d'électrons (et plus tard d'un gradient de protons) qui sont utilisés pour fabriquer des molécules d'ATP et de NADPH qui stockent et transportent temporairement l'énergie.
Une partie du glucose est convertie en amidon qui est stocké dans le chloroplaste.
Contrairement aux animaux (qui manquent de chloroplastes), les plantes et leurs parents eucaryotes ont délégué de nombreux rôles biochimiques à leurs chloroplastes, notamment la synthèse de tous leurs acides gras et de la plupart des acides aminés.
Les plantes terrestres vasculaires fabriquent de la lignine, un polymère utilisé pour renforcer les parois cellulaires secondaires des trachéides et des vaisseaux du xylème afin de les empêcher de s'effondrer lorsqu'une plante aspire de l'eau à travers elles en cas de stress hydrique.
D'autres, comme les huiles essentielles d'huile de menthe poivrée et d'huile de citron sont utiles pour leur arôme, comme arômes et épices (par exemple, la capsaïcine), et en médecine comme produits pharmaceutiques comme dans l'opium du pavot à opium.
Par exemple, l'aspirine analgésique est l'ester acétylique de l'acide salicylique, isolé à l'origine de l'écorce des saules, et une large gamme d'analgésiques opiacés comme l'héroïne sont obtenus par modification chimique de la morphine obtenue à partir du pavot à opium.
Les Amérindiens utilisent diverses plantes comme moyen de traiter la maladie ou la maladie depuis des milliers d'années.
Le sucre, l'amidon, le coton, le lin, le chanvre, certains types de cordes, le bois et les panneaux de particules, le papyrus et le papier, les huiles végétales, la cire et le caoutchouc naturel sont des exemples de matériaux commercialement importants fabriqués à partir de tissus végétaux ou de leurs produits secondaires.
Les produits fabriqués à partir de cellulose comprennent la rayonne et la cellophane, la colle à papier peint, le biobutanol et le coton à canon.
Certains écologistes s'appuient même sur des données empiriques provenant de peuples autochtones qui sont recueillies par des ethnobotanistes.
Les plantes dépendent de certains facteurs édaphiques (sol) et climatiques de leur environnement mais peuvent également modifier ces facteurs.
Ils interagissent avec leurs voisins à diverses échelles spatiales dans des groupes, des populations et des communautés qui constituent collectivement la végétation.
Gregor Mendel a découvert les lois génétiques de l'hérédité en étudiant des traits hérités tels que la forme chez Pisum sativum (pois).
Néanmoins, il existe des différences génétiques distinctives entre les plantes et les autres organismes.
Les nombreuses variétés de blé cultivées sont le résultat de multiples croisements inter et intra-spécifiques entre des espèces sauvages et leurs hybrides.
Dans de nombreuses plantes terrestres, les gamètes mâles et femelles sont produits par des individus distincts.
La formation de tubercules de tige dans la pomme de terre en est un exemple.
L'apomixie peut également se produire dans une graine, produisant une graine qui contient un embryon génétiquement identique au parent.
Une plante allopolyploïde peut résulter d'un événement d'hybridation entre deux espèces différentes.
Certains polyploïdes végétaux autrement stériles peuvent encore se reproduire végétativement ou par apomixie des graines , formant des populations clonales d'individus identiques.
Le pissenlit commun est un triploïde qui produit des graines viables par graines apomictiques.
Le séquençage de certains autres génomes relativement petits, du riz ( Oryza sativa ) et de Brachypodium distachyon , en a fait des espèces modèles importantes pour comprendre la génétique, la biologie cellulaire et moléculaire des céréales, des graminées et des monocotylédones en général.
Les épinards, les pois, le soja et une mousse Physcomitrella patens sont couramment utilisés pour étudier la biologie des cellules végétales.
L'expression génique peut également être contrôlée par des protéines répresseurs qui se fixent aux régions silencieuses de l'ADN et empêchent l'expression de cette région du code ADN.
Certains changements épigénétiques se sont avérés héréditaires, tandis que d'autres sont réinitialisés dans les cellules germinales.
Contrairement aux animaux, de nombreuses cellules végétales, en particulier celles du parenchyme, ne se différencient pas en phase terminale, restant totipotentes avec la capacité de donner naissance à une nouvelle plante individuelle.
Les algues sont un groupe polyphylétique et sont placées dans diverses divisions, certaines plus étroitement liées aux plantes que d'autres.
La classe Charophyte Charophyceae et le sous-royaume des plantes terrestres Embryophyta forment ensemble le groupe monophylétique ou clade Streptophytina.
Les plantes vasculaires ptéridophytes avec un vrai xylème et un phloème qui se reproduisaient par des spores germinant en gamétophytes libres ont évolué au cours de la période silurienne et se sont diversifiées en plusieurs lignées au cours du Silurien supérieur et du Dévonien précoce.
Leurs gamétophytes réduits se sont développés à partir de mégaspores retenues dans les organes producteurs de spores (mégasporanges) du sporophyte, une condition connue sous le nom d'endosporie.
Les premières plantes à graines connues datent du dernier stade du Dévonien Famennien.
Les produits chimiques extraits de l'air, du sol et de l'eau constituent la base de tout le métabolisme des plantes.
Les hétérotrophes, y compris tous les animaux, tous les champignons, toutes les plantes complètement parasites et les bactéries non photosynthétiques, absorbent les molécules organiques produites par les photoautotrophes et les respirent ou les utilisent dans la construction de cellules et de tissus.
Le transport subcellulaire d'ions, d'électrons et de molécules telles que l'eau et les enzymes se produit à travers les membranes cellulaires.
Des exemples d'éléments que les plantes doivent transporter sont l'azote, le phosphore, le potassium, le calcium, le bamnésium et le soufre.
Ce composé médie les réponses tropiques des pousses et des racines à la lumière et à la gravité.
La zéatine cytokinine naturelle a été découverte dans le maïs, Zea mays, et est un dérivé de la purine adénine.
Ils sont impliqués dans la promotion de la germination et de la levée de dormance des graines, dans la régulation de la hauteur des plantes en contrôlant l'allongement des tiges et le contrôle de la floraison.
Il a été ainsi nommé car on pensait à l'origine qu'il contrôlait l'abscission.
Une autre classe de phytohormones est constituée des jasmonates, d'abord isolés de l'huile de Jasminum grandiflorum qui régule les réponses aux blessures chez les plantes en débloquant l'expression des gènes nécessaires à la réponse systémique de résistance acquise à l'attaque des agents pathogènes.
Les plantes non vasculaires, les hépatiques, les anthocérotes et les mousses ne produisent pas de racines vasculaires pénétrant dans le sol et la majeure partie de la plante participe à la photosynthèse.
Les cellules de chaque système sont capables de créer des cellules de l'autre et de produire des pousses ou des racines adventives.
Dans le cas où l'un des systèmes est perdu, l'autre peut souvent le faire repousser.
Chez les plantes vasculaires, le xylème et le phloème sont les tissus conducteurs qui transportent les ressources entre les pousses et les racines.
Les feuilles recueillent la lumière du soleil et effectuent la photosynthèse.
Les angiospermes sont des plantes productrices de graines qui produisent des fleurs et ont des graines enfermées.
Certaines plantes se reproduisent sexuellement, d'autres de manière asexuée et d'autres par les deux moyens.
La classification biologique est une forme de taxonomie scientifique.
Alors que les scientifiques ne sont pas toujours d'accord sur la façon de classer les organismes, la phylogénétique moléculaire, qui utilise des séquences d'ADN comme données, a entraîné de nombreuses révisions récentes dans le sens de l'évolution et continuera probablement à le faire.
La nomenclature des organismes botaniques est codifiée dans le Code international de nomenclature pour les algues, les champignons et les plantes (ICN) et administrée par le Congrès international de botanique.
Le nom scientifique d'une plante représente son genre et ses espèces au sein du genre, ce qui donne un nom mondial unique pour chaque organisme.
La combinaison est le nom de l'espèce.
Les relations évolutives et l'hérédité d'un groupe d'organismes s'appellent sa phylogénie.
À titre d'exemple, les espèces de Pereskia sont des arbres ou des arbustes à feuilles proéminentes.
Juger les relations sur la base de caractères communs nécessite de la prudence, car les plantes peuvent se ressembler par une évolution convergente dans laquelle les caractères sont apparus indépendamment.
Seuls les caractères dérivés, tels que les aréoles productrices d'épines des cactus, fournissent la preuve de la descendance d'un ancêtre commun.
La différence est que le code génétique lui-même est utilisé pour décider des relations évolutives, au lieu d'être utilisé indirectement via les caractères qu'il engendre.
Les preuves génétiques suggèrent que la véritable relation évolutive des organismes multicellulaires est celle indiquée dans le cladogramme ci-dessous - les champignons sont plus étroitement liés aux animaux qu'aux plantes.
L'étude des relations entre les espèces végétales permet aux botanistes de mieux comprendre le processus d'évolution des plantes.
Bien que les humains se soient toujours intéressés à l'histoire naturelle des animaux qu'ils voyaient autour d'eux et aient utilisé ces connaissances pour domestiquer certaines espèces, on peut dire que l'étude formelle de la zoologie est née avec Aristote.
La zoologie moderne a ses origines à la Renaissance et au début de la période moderne, avec Carl Linnaeus, Antonie van Leeuwenhoek, Robert Hooke, Charles Darwin, Gregor Mendel et bien d'autres.
Il existe des peintures rupestres, des gravures et des sculptures en France datant de 15 000 ans montrant des bisons, des chevaux et des cerfs dans des détails soigneusement rendus.
Les connaissances anciennes sur la faune sont illustrées par les représentations réalistes d'animaux sauvages et domestiques du Proche-Orient, de Mésopotamie et d'Égypte, y compris les pratiques et techniques d'élevage, de chasse et de pêche.
Aristote, au IVe siècle av. J.-C., considérait les animaux comme des organismes vivants, étudiant leur structure, leur développement et les phénomènes vitaux.
Quatre cents ans plus tard, le médecin romain Galien a disséqué des animaux pour étudier leur anatomie et la fonction des différentes parties, car la dissection des cadavres humains était alors interdite.
En Europe, les travaux de Galien sur l'anatomie sont restés largement inégalés et incontestés jusqu'au XVIe siècle.
Autrefois domaine des gentlemen naturalistes, la zoologie est devenue au cours des XVIIIe, XIXe et XXe siècles une discipline scientifique de plus en plus professionnalisée.
Ces développements, ainsi que les résultats de l'embryologie et de la paléontologie, ont été synthétisés dans la publication en 1859 de la théorie de l'évolution par sélection naturelle de Charles Darwin ; en cela, Darwin a placé la théorie de l'évolution organique sur une nouvelle base, en expliquant les processus par lesquels elle peut se produire et en fournissant des preuves d'observation qu'elle l'avait fait.
Darwin a donné une nouvelle direction à la morphologie et à la physiologie, en les unissant dans une théorie biologique commune : la théorie de l'évolution organique.
Une première nécessité était d'identifier les organismes et de les regrouper en fonction de leurs caractéristiques, différences et relations, et c'est le domaine du taxonomiste.
Ses idées étaient centrées sur la morphologie des animaux.
Ces groupements ont depuis été révisés pour améliorer la cohérence avec le principe darwinien de descendance commune.
Homo est le genre, et sapiens l'épithète spécifique, les deux combinés forment le nom de l'espèce.
Le système de classification dominant s'appelle la taxonomie linnéenne.
Comprendre la structure et la fonction des cellules est fondamental pour toutes les sciences biologiques.
Il se concentre sur la façon dont les organes et les systèmes d'organes fonctionnent ensemble dans les corps des humains et des animaux, en plus de la façon dont ils fonctionnent de manière indépendante.
Les études physiologiques ont traditionnellement été divisées en physiologie végétale et physiologie animale, mais certains principes de physiologie sont universels, quel que soit l'organisme particulier étudié.
Par exemple, cela implique généralement des scientifiques qui ont une formation spéciale sur des organismes particuliers tels que la mammalogie, l'ornithologie, l'herpétologie ou l'entomologie, mais utilisent ces organismes comme systèmes pour répondre à des questions générales sur l'évolution.
Les éthologues se sont particulièrement intéressés à l'évolution du comportement et à la compréhension du comportement en termes de théorie de la sélection naturelle.
Alors que les chercheurs pratiquent des techniques propres à la biologie moléculaire, il est courant de les combiner avec des méthodes issues de la génétique et de la biochimie.
La systématique biologique est l'étude de la diversification des formes de vie, passées et présentes, et des relations entre les êtres vivants à travers le temps.
Les arbres phylogénétiques des espèces et des taxons supérieurs sont utilisés pour étudier l'évolution des traits (par exemple, les caractéristiques anatomiques ou moléculaires) et la distribution des organismes (biogéographie).
La systématique biologique classe les espèces en utilisant trois branches spécifiques.
La systématique expérimentale identifie et classe les animaux en fonction des unités évolutives qui composent une espèce, ainsi que de leur importance dans l'évolution elle-même.
Expliquer la biodiversité de la planète et de ses organismes.
La taxonomie est la partie de la systématique concernée par les sujets (a) à (d) ci-dessus.
Cependant, dans l'usage moderne, ils peuvent tous être considérés comme des synonymes les uns des autres.
Certains prétendent que la systématique traite à elle seule spécifiquement des relations dans le temps et qu'elle peut être synonyme de phylogénétique , traitant globalement de la hiérarchie déduite des organismes.
Les classifications scientifiques sont des aides à l'enregistrement et à la communication d'informations à d'autres scientifiques et à des profanes.
En biologie, une espèce est l'unité de base de classification et un rang taxonomique d'un organisme, ainsi qu'une unité de biodiversité.
De plus, les paléontologues utilisent le concept de chronoespèce puisque la reproduction fossile ne peut pas être examinée.
Toutes les espèces (à l'exception des virus) reçoivent un nom en deux parties, un "binôme".
Par exemple, Boa constrictor est l'une des quatre espèces du genre Boa, constrictor étant l'épithète de l'espèce.
De plus, parmi les organismes qui ne se reproduisent que de manière asexuée, le concept d'espèce reproductrice s'effondre et chaque clone est potentiellement une micro-espèce.
Les espèces ont été vues depuis Aristote jusqu'au XVIIIe siècle comme des catégories fixes pouvant être hiérarchisées, la grande chaîne des êtres.
Cette compréhension s'est considérablement élargie au XXe siècle grâce à la génétique et à l'écologie des populations.
Ernst Mayr a mis l'accent sur l'isolement reproductif, mais cela, comme d'autres concepts d'espèces, est difficile, voire impossible, à tester.
Cette méthode a été utilisée comme méthode "classique" de détermination des espèces, comme avec Linnaeus au début de la théorie de l'évolution.
En règle générale, les microbiologistes ont supposé que les types de bactéries ou d'archaea avec des séquences de gènes d'ARN ribosomal 16S plus similaires à 97% les unes aux autres doivent être vérifiées par hybridation ADN-ADN pour décider si elles appartiennent à la même espèce ou non.
Les approches modernes comparent la similarité des séquences à l'aide de méthodes informatiques.
Une base de données, Barcode of Life Data Systems (BOLD), contient des séquences de codes-barres d'ADN de plus de 190 000 espèces.
Par exemple, dans une étude réalisée sur des champignons, l'étude des caractères nucléotidiques à l'aide d'espèces cladistiques a produit les résultats les plus précis dans la reconnaissance des nombreuses espèces de champignons de tous les concepts étudiés.
Pourtant, d'autres défendent cette approche, considérant «l'inflation taxonomique» comme péjorative et qualifiant le point de vue opposé de «conservatisme taxonomique»; affirmant qu'il est politiquement opportun de diviser les espèces et de reconnaître des populations plus petites au niveau de l'espèce, car cela signifie qu'elles peuvent plus facilement être incluses comme en voie de disparition dans la liste rouge de l'UICN et peuvent attirer une législation et un financement de conservation.
Si les scientifiques veulent dire que quelque chose s'applique à toutes les espèces d'un genre, ils utilisent le nom du genre sans le nom ou l'épithète spécifique.
Au fur et à mesure que de nouvelles informations arrivent, l'hypothèse peut être corroborée ou réfutée.
La division d'un taxon en plusieurs taxons, souvent nouveaux, s'appelle la division.
Le terme quasi-espèce est parfois utilisé pour désigner des entités à mutation rapide comme les virus.
Dans les espèces en anneau, lorsque les membres de populations adjacentes dans une aire de répartition largement continue se croisent avec succès, mais pas les membres de populations plus éloignées.
Les espèces en anneau présentent donc une difficulté pour tout concept d'espèce qui repose sur l'isolement reproductif.
La spéciation dépend d'une mesure d'isolement reproductif, un flux de gènes réduit.
Les bactéries peuvent échanger des plasmides avec des bactéries d'autres espèces, y compris certaines apparemment éloignées dans différents domaines phylogénétiques, ce qui rend difficile l'analyse de leurs relations et affaiblit le concept d'espèce bactérienne.
Les extinctions de masse ont eu diverses causes, notamment l'activité volcanique, le changement climatique et les changements dans la chimie océanique et atmosphérique, et elles ont à leur tour eu des effets majeurs sur l'écologie, l'atmosphère, la surface terrestre et les eaux de la Terre.
Certains observateurs affirment qu'il existe un conflit inhérent entre le désir de comprendre les processus de spéciation et le besoin d'identifier et de catégoriser.
L'un des cas classiques en Amérique du Nord est celui de la chouette tachetée du nord protégée qui s'hybride avec la chouette tachetée de Californie non protégée et la chouette rayée ; cela a conduit à des débats juridiques.
Une forme se distinguait par le fait qu'elle était partagée par tous ses membres, les jeunes héritant des variations qu'ils pouvaient avoir de leurs parents.
Il a établi l'idée d'une hiérarchie taxonomique de classification basée sur des caractéristiques observables et destinée à refléter les relations naturelles.
Jean-Baptiste Lamarck, dans sa Philosophie zoologique de 1809, décrit la transmutation des espèces, proposant qu'une espèce puisse changer avec le temps, en rupture radicale avec la pensée aristotélicienne.
Le genre (pluriel des genres) est un rang taxonomique utilisé dans la classification biologique des organismes vivants et fossiles ainsi que des virus.
Par exemple, Panthera leo (lion) et Panthera onca (jaguar) sont deux espèces du genre Panthera.
Un exemple botanique serait Hibiscus arnottianus, une espèce particulière du genre Hibiscus originaire d'Hawaï.
Les noms disponibles sont ceux publiés conformément au Code international de nomenclature zoologique et non autrement supprimés par des décisions ultérieures de la Commission internationale de nomenclature zoologique (ICZN) ; le plus ancien nom de ce type pour un taxon (par exemple, un genre) doit alors être sélectionné comme nom "valide" (c'est-à-dire, actuel ou accepté) pour le taxon en question.
En botanique, des concepts similaires existent mais avec des étiquettes différentes.
Cependant, de nombreux noms ont été attribués (généralement involontairement) à deux ou plusieurs genres différents.
Un nom qui signifie deux choses différentes est un homonyme.
Cependant, un genre d'un royaume est autorisé à porter un nom scientifique utilisé comme nom générique (ou le nom d'un taxon d'un autre rang) dans un royaume régi par un code de nomenclature différent.
Par exemple, parmi les reptiles (non aviaires), qui comptent environ 1180 genres, la plupart (>300) n'ont qu'une seule espèce, ~360 ont entre 2 et 4 espèces, 260 ont 5 à 10 espèces, ~200 ont 11 à 50 espèces, et seulement 27 genres ont plus de 50 espèces.
Les espèces attribuées à un genre sont quelque peu arbitraires.
Ce qui appartient à une famille - ou si une famille décrite doit être reconnue du tout - est proposé et déterminé par des taxonomistes praticiens.
Souvent, il n'y a pas d'accord exact, différents taxonomistes adoptant chacun une position différente.
Michael Novacek (1986) les a insérés à la même position.
Il n'y a pas de règles objectives pour décrire une classe, mais pour les animaux bien connus, il est probable qu'il y ait consensus.
En botanique, les cours sont désormais rarement abordés.
De manière informelle, les phylums peuvent être considérés comme des groupements d'organismes basés sur la spécialisation générale du plan corporel.
Ainsi, les phylums peuvent être fusionnés ou divisés s'il devient évident qu'ils sont liés les uns aux autres ou non.
Selon la définition de Budd et Jensen, un phylum est défini par un ensemble de caractères partagés par tous ses représentants vivants.
Cependant, comme il est basé sur les caractères, il est facile de l'appliquer aux archives fossiles.
Cependant, prouver qu'un fossile appartient au groupe de la couronne d'un phylum est difficile, car il doit afficher un caractère unique à un sous-ensemble du groupe de la couronne.
Le tableau ci-dessous suit le système Cavalier-Smith influent (bien que controversé) en assimilant "Plantae" à Archaeplastida , un groupe contenant Viridiplantae et les divisions algales Rhodophyta et Glaucophyta .
La division Pinophyta peut être utilisée pour tous les gymnospermes (y compris les cycadales, les ginkgos et les gnetophytes), ou pour les conifères seuls comme ci-dessous.
Protista est un taxon polyphylétique, moins acceptable pour les biologistes d'aujourd'hui que par le passé.
Carl Linnaeus (1707-1778) a jeté les bases de la nomenclature biologique moderne, désormais réglementée par les codes de nomenclature, en 1735.
En 1937, Édouard Chatton a introduit les termes "procaryote" et "eucaryote" pour différencier ces organismes.
Robert Whittaker a reconnu un royaume supplémentaire pour les Fungi.
Les deux royaumes restants, Protista et Monera, comprenaient des colonies cellulaires unicellulaires et simples.
Dans d'autres systèmes, tels que le système des cinq royaumes de Lynn Margulis, les plantes ne comprenaient que les plantes terrestres ( Embryophyta ), et Protoctista a une définition plus large.
Les progrès technologiques de la microscopie électronique ont permis de séparer les Chromista du royaume Plantae.
Enfin, certains protistes dépourvus de mitochondries ont été découverts.
Ce super-royaume s'opposait au super-royaume Metakaryota, regroupant les cinq autres royaumes eucaryotes (Animalia, Protozoa, Fungi, Plantae et Chromista).
Cavalier-Smith n'acceptait plus l'importance de la division fondamentale Eubacteria-Archaebacteria mise en avant par Woese et d'autres et soutenue par des recherches récentes.
Cavalier-Smith n'accepte pas l'exigence que les taxons soient monophylétiques ("holophylétique" dans sa terminologie) pour être valide.
Les avancées des études phylogénétiques ont permis à Cavalier-Smith de se rendre compte que tous les phyla supposés être des archézoaires (c'est-à-dire des eucaryotes primitivement amitochondriaux) avaient en fait secondairement perdu leurs mitochondries, typiquement en les transformant en de nouveaux organites : les Hydrogénosomes.
Sur la base de ces études sur l'ARN, Carl Woese pensait que la vie pouvait être divisée en trois grandes divisions et les appelait le modèle des «trois royaumes primaires» ou le modèle «urkingdom».
Woese a divisé les procaryotes (précédemment classés sous le nom de Kingdom Monera) en deux groupes, appelés Eubacteria et Archaebacteria, soulignant qu'il y avait autant de différence génétique entre ces deux groupes qu'entre l'un d'eux et tous les eucaryotes.
Ils soutenaient que seuls les groupes monophylétiques devaient être acceptés comme rangs formels dans une classification et que - bien que cette approche ait été irréalisable auparavant (nécessitant "littéralement des dizaines de royaumes eucaryotes) - il était désormais possible de diviser les eucaryotes en "juste quelques grands groupes qui sont probablement tous monophylétiques ».
Il a divisé les eucaryotes en les mêmes six "supergroupes".
On pense que les plantes sont plus éloignées des animaux et des champignons.
Les dix arguments contre incluent le fait qu'il s'agit de parasites intracellulaires obligatoires qui manquent de métabolisme et ne sont pas capables de se répliquer en dehors d'une cellule hôte.
Les deux premiers sont tous des micro-organismes procaryotes, ou principalement des organismes unicellulaires dont les cellules ont un noyau déformé ou non lié à la membrane.
Les halophiles, des organismes qui se développent dans des environnements très salés, et les hyperthermophiles, des organismes qui se développent dans des environnements extrêmement chauds, sont des exemples d'Archaea.
Les cyanobactéries et les mycoplasmes sont deux exemples de bactéries.
L'évolution est le changement des caractéristiques héréditaires des populations biologiques au cours des générations successives.
L'évolution se produit lorsque des processus évolutifs tels que la sélection naturelle (y compris la sélection sexuelle) et la dérive génétique agissent sur cette variation, ce qui fait que certaines caractéristiques deviennent plus courantes ou rares au sein d'une population.
La théorie scientifique de l'évolution par sélection naturelle a été conçue indépendamment par Charles Darwin et Alfred Russel Wallace au milieu du XIXe siècle et a été exposée en détail dans le livre de Darwin Sur l'origine des espèces.
Ainsi, dans les générations successives, les membres d'une population sont plus susceptibles d'être remplacés par les descendants de parents présentant des caractéristiques favorables qui leur ont permis de survivre et de se reproduire dans leurs environnements respectifs.
Les archives fossiles comprennent une progression du graphite biogénique précoce, aux fossiles de tapis microbiens, aux organismes multicellulaires fossilisés.
Il a cherché des explications des phénomènes naturels en termes de lois physiques qui étaient les mêmes pour toutes les choses visibles et qui n'exigeaient pas l'existence de catégories naturelles fixes ou d'un ordre cosmique divin.
La classification biologique introduite par Carl Linnaeus en 1735 reconnaissait explicitement la nature hiérarchique des relations entre les espèces, mais considérait toujours les espèces comme fixées selon un plan divin.
Ces idées ont été condamnées par les naturalistes établis comme des spéculations dépourvues de fondement empirique.
Partiellement influencé par An Essay on the Principle of Population (1798) de Thomas Robert Malthus, Darwin nota que la croissance démographique conduirait à une "lutte pour l'existence" dans laquelle des variations favorables prévalaient alors que d'autres périraient.
Darwin a développé sa théorie de la "sélection naturelle" à partir de 1838 et écrivait son "gros livre" sur le sujet quand Alfred Russel Wallace lui a envoyé une version de pratiquement la même théorie en 1858.
À cette fin, Darwin a développé sa théorie provisoire de la pangenèse.
Pour expliquer l'origine des nouvelles variantes, de Vries a développé une théorie de la mutation qui a conduit à une rupture temporaire entre ceux qui ont accepté l'évolution darwinienne et les biométriciens qui se sont alliés à de Vries.
La publication de la structure de l'ADN par James Watson et Francis Crick avec la contribution de Rosalind Franklin en 1953 a démontré un mécanisme physique d'hérédité.
En 1973, le biologiste évolutionniste Theodosius Dobzhansky a écrit que "rien en biologie n'a de sens sauf à la lumière de l'évolution", parce qu'il a mis en lumière les relations de ce qui semblait d'abord des faits disjoints dans l'histoire naturelle dans un ensemble explicatif cohérent de connaissances qui décrit et prédit de nombreux faits observables sur la vie sur cette planète.
L'ensemble complet des traits observables qui composent la structure et le comportement d'un organisme est appelé son phénotype.
Par exemple, la peau bronzée provient de l'interaction entre le génotype d'une personne et la lumière du soleil ; ainsi, le bronzage n'est pas transmis aux enfants des gens.
L'ADN est un long biopolymère composé de quatre types de bases.
Les portions d'une molécule d'ADN qui spécifient une seule unité fonctionnelle sont appelées gènes ; différents gènes ont différentes séquences de bases.
Si la séquence d'ADN à un locus varie d'un individu à l'autre, les différentes formes de cette séquence sont appelées allèles.
Cependant, bien que cette simple correspondance entre un allèle et un trait fonctionne dans certains cas, la plupart des traits sont plus complexes et sont contrôlés par des locus de traits quantitatifs (plusieurs gènes en interaction).
La méthylation de l'ADN marquant la chromatine, les boucles métaboliques auto-entretenues, le silençage génique par interférence ARN et la conformation tridimensionnelle des protéines (telles que les prions) sont des domaines où des systèmes d'hérédité épigénétique ont été découverts au niveau de l'organisme.
Par exemple, l'héritage écologique à travers le processus de construction de niche est défini par les activités régulières et répétées des organismes dans leur environnement.
Malgré l'introduction constante de nouvelles variations par mutation et flux de gènes, la majeure partie du génome d'une espèce est identique chez tous les individus de cette espèce.
Une partie substantielle de la variation phénotypique dans une population est causée par la variation génotypique.
La variation disparaît lorsqu'un nouvel allèle atteint le point de fixation - lorsqu'il disparaît de la population ou remplace entièrement l'allèle ancestral.
Lorsque des mutations se produisent, elles peuvent altérer le produit d'un gène, empêcher le fonctionnement du gène ou n'avoir aucun effet.
Des copies supplémentaires de gènes sont une source majeure de la matière première nécessaire à l'évolution de nouveaux gènes.
De nouveaux gènes peuvent être générés à partir d'un gène ancestral lorsqu'une copie dupliquée mute et acquiert une nouvelle fonction.
La génération de nouveaux gènes peut également impliquer la duplication de petites parties de plusieurs gènes, ces fragments se recombinant ensuite pour former de nouvelles combinaisons avec de nouvelles fonctions.
La recombinaison et le réassortiment ne modifient pas les fréquences alléliques, mais modifient plutôt les allèles associés les uns aux autres, produisant une progéniture avec de nouvelles combinaisons d'allèles.
Le premier coût est que chez les espèces sexuellement dimorphes, un seul des deux sexes peut porter des petits.
Pourtant, la reproduction sexuée est le moyen de reproduction le plus courant chez les eucaryotes et les organismes multicellulaires.
Le transfert de gènes entre espèces comprend la formation d'organismes hybrides et le transfert horizontal de gènes.
Un transfert horizontal de gènes de bactéries à des eucaryotes tels que la levure Saccharomyces cerevisiae et le charançon du haricot adzuki Callosobruchus chinensis s'est produit.
Différents traits confèrent différents taux de survie et de reproduction (aptitude différentielle).
Par conséquent, les organismes avec des traits qui leur donnent un avantage sur leurs concurrents sont plus susceptibles de transmettre leurs traits à la génération suivante que ceux dont les traits ne confèrent pas d'avantage.
Le concept central de la sélection naturelle est l'aptitude évolutive d'un organisme.
Par exemple, si un organisme pouvait bien survivre et se reproduire rapidement, mais que sa progéniture était trop petite et trop faible pour survivre, cet organisme apporterait peu de contribution génétique aux générations futures et aurait donc une faible forme physique.
Des exemples de traits qui peuvent augmenter la forme physique sont une survie améliorée et une fécondité accrue.
Cependant, même si la direction de la sélection s'inverse de cette manière, les traits qui ont été perdus dans le passé peuvent ne pas réévoluer sous une forme identique (voir la loi de Dollo).
La première est la sélection directionnelle, qui est un changement de la valeur moyenne d'un trait au fil du temps, par exemple, les organismes deviennent lentement plus grands.
Enfin, dans la stabilisation de la sélection, il y a sélection contre des valeurs de trait extrêmes aux deux extrémités, ce qui entraîne une diminution de la variance autour de la valeur moyenne et moins de diversité.
Cette large compréhension de la nature permet aux scientifiques de délimiter des forces spécifiques qui, ensemble, constituent la sélection naturelle.
Cependant, le taux de recombinaison est faible (environ deux événements par chromosome par génération).
Un ensemble d'allèles qui est généralement hérité dans un groupe est appelé un haplotype.
Cette dérive s'arrête lorsqu'un allèle finit par se fixer, soit en disparaissant de la population, soit en remplaçant entièrement les autres allèles.
La théorie neutre de l'évolution moléculaire a proposé que la plupart des changements évolutifs sont le résultat de la fixation de mutations neutres par dérive génétique.
Cependant, une version plus récente et mieux étayée de ce modèle est la théorie presque neutre, où une mutation qui serait effectivement neutre dans une petite population n'est pas nécessairement neutre dans une grande population.
Le nombre d'individus dans une population n'est pas critique, mais plutôt une mesure connue sous le nom de taille effective de la population.
La présence ou l'absence de flux de gènes modifie fondamentalement le cours de l'évolution.
Cet argument de pressions opposées a longtemps été utilisé pour écarter la possibilité de tendances internes dans l'évolution, jusqu'à ce que l'ère moléculaire suscite un regain d'intérêt pour l'évolution neutre.
Par exemple, les biais de mutation sont fréquemment invoqués dans les modèles d'utilisation des codons.
Différents biais d'insertion par rapport à la suppression dans différents taxons peuvent conduire à l'évolution de différentes tailles de génome.
La réflexion contemporaine sur le rôle des biais de mutation reflète une théorie différente de celle de Haldane et Fisher.
Les organismes peuvent également répondre à la sélection en coopérant les uns avec les autres, généralement en aidant leurs proches ou en s'engageant dans une symbiose mutuellement bénéfique.
La macroévolution fait référence à l'évolution qui se produit au niveau ou au-dessus du niveau des espèces, en particulier la spéciation et l'extinction ; tandis que la microévolution fait référence à de plus petits changements évolutifs au sein d'une espèce ou d'une population, en particulier des changements dans la fréquence et l'adaptation des allèles.
Cependant, dans la macroévolution, les traits de l'espèce entière peuvent être importants.
Une idée fausse courante est que l'évolution a des objectifs, des plans à long terme ou une tendance innée au «progrès», comme l'expriment des croyances telles que l'orthogenèse et l'évolutionnisme; cependant, de manière réaliste, l'évolution n'a pas d'objectif à long terme et ne produit pas nécessairement une plus grande complexité.
De plus, le terme adaptation peut faire référence à un trait important pour la survie d'un organisme.
Un trait adaptatif est un aspect du modèle de développement de l'organisme qui permet ou augmente la probabilité que cet organisme survive et se reproduise.
D'autres exemples frappants sont la bactérie Escherichia coli développant la capacité d'utiliser l'acide citrique comme nutriment dans une expérience de laboratoire à long terme, Flavobacterium développant une nouvelle enzyme qui permet à ces bactéries de se développer sur les sous-produits de la fabrication du nylon et la bactérie du sol. Sphingobium développe une voie métabolique entièrement nouvelle qui dégrade le pesticide synthétique pentachlorophénol.
Par conséquent, des structures ayant une organisation interne similaire peuvent avoir des fonctions différentes dans des organismes apparentés.
Cependant, étant donné que tous les organismes vivants sont apparentés dans une certaine mesure, même les organes qui semblent avoir peu ou pas de similarité structurelle, tels que les yeux des arthropodes, des calmars et des vertébrés, ou les membres et les ailes des arthropodes et des vertébrés, peuvent dépendre d'un ensemble commun de des gènes homologues qui contrôlent leur assemblage et leur fonction ; c'est ce qu'on appelle l'homologie profonde.
Les exemples incluent les pseudogènes, les restes non fonctionnels des yeux chez les poissons aveugles vivant dans les cavernes, les ailes chez les oiseaux incapables de voler, la présence d'os de la hanche chez les baleines et les serpents et les traits sexuels chez les organismes qui se reproduisent par reproduction asexuée.
Un exemple est le lézard africain Holaspis guentheri, qui a développé une tête extrêmement plate pour se cacher dans les crevasses, comme on peut le voir en regardant ses proches parents.
Un autre exemple est le recrutement d'enzymes de la glycolyse et du métabolisme xénobiotique pour servir de protéines structurelles appelées cristallines dans les lentilles des yeux des organismes.
Ces études ont montré que l'évolution peut modifier le développement pour produire de nouvelles structures, telles que des structures osseuses embryonnaires qui se développent dans la mâchoire chez d'autres animaux au lieu de faire partie de l'oreille moyenne chez les mammifères.
Ces changements chez la seconde espèce provoquent alors à leur tour de nouvelles adaptations chez la première espèce.
Par exemple, une coopération extrême existe entre les plantes et les champignons mycorhiziens qui poussent sur leurs racines et aident la plante à absorber les nutriments du sol.
Des coalitions entre organismes d'une même espèce ont également évolué.
Ici, les cellules somatiques répondent à des signaux spécifiques qui leur indiquent de croître, de rester telles quelles ou de mourir.
Il existe plusieurs façons de définir le concept d'« espèce ».
Malgré la diversité des divers concepts d'espèces, ces divers concepts peuvent être placés dans l'une des trois grandes approches philosophiques : métissage, écologique et phylogénétique.
Malgré son utilisation large et à long terme, le BSC comme d'autres n'est pas sans controverse, par exemple parce que ces concepts ne peuvent pas être appliqués aux procaryotes, et c'est ce qu'on appelle le problème des espèces.
Le flux de gènes peut ralentir ce processus en propageant les nouvelles variantes génétiques également aux autres populations.
Dans ce cas, des espèces étroitement apparentées peuvent régulièrement se croiser, mais les hybrides seront sélectionnés contre et les espèces resteront distinctes.
La spéciation a été observée plusieurs fois dans des conditions de laboratoire contrôlées (voir les expériences de laboratoire sur la spéciation) et dans la nature.
La plus courante chez les animaux est la spéciation allopatrique, qui se produit dans des populations initialement isolées géographiquement, comme par la fragmentation de l'habitat ou la migration.
Le deuxième mode de spéciation est la spéciation périatrique, qui se produit lorsque de petites populations d'organismes s'isolent dans un nouvel environnement.
Le troisième mode est la spéciation parapatrique.
Généralement, cela se produit lorsqu'il y a eu un changement radical de l'environnement dans l'habitat de l'espèce parentale.
La sélection contre le croisement avec la population parentale sensible aux métaux a produit un changement graduel de la période de floraison des plantes résistantes aux métaux, ce qui a finalement produit un isolement reproductif complet.
Cette forme est rare car même une petite quantité de flux de gènes peut éliminer les différences génétiques entre les parties d'une population.
Ce n'est pas courant chez les animaux car les hybrides d'animaux sont généralement stériles.
Cela permet aux chromosomes de chaque espèce parentale de former des paires correspondantes pendant la méiose, puisque les chromosomes de chaque parent sont déjà représentés par une paire.
En effet, le doublement des chromosomes au sein d'une espèce peut être une cause fréquente d'isolement reproductif, car la moitié des chromosomes doublés seront inégalés lors de la reproduction avec des organismes non doublés.
Presque toutes les espèces animales et végétales qui ont vécu sur Terre sont maintenant éteintes, et l'extinction semble être le destin ultime de toutes les espèces.
Malgré l'extinction estimée de plus de 99% de toutes les espèces qui ont jamais vécu sur Terre, on estime qu'environ 1 billion d'espèces se trouvent actuellement sur Terre avec seulement un millième de un pour cent décrit.
La première preuve incontestée de la vie sur Terre date d'il y a au moins 3,5 milliards d'années, pendant l'ère éoarchéenne après qu'une croûte géologique a commencé à se solidifier à la suite de l'éon hadéen en fusion antérieur.
Commentant les découvertes australiennes, Stephen Blair Hedges a écrit: "Si la vie est apparue relativement rapidement sur Terre, alors elle pourrait être courante dans l'univers."
Les estimations du nombre d'espèces actuelles de la Terre vont de 10 millions à 14 millions, dont environ 1,9 million auraient été nommées et 1,6 million documentées dans une base de données centrale à ce jour, laissant au moins 80% non encore décrites.
La descendance commune des organismes a d'abord été déduite de quatre faits simples sur les organismes : premièrement, ils ont des distributions géographiques qui ne peuvent être expliquées par une adaptation locale.
Quatrièmement, les organismes peuvent être classés en utilisant ces similitudes dans une hiérarchie de groupes imbriqués, semblable à un arbre généalogique.
Ce point de vue remonte à une idée brièvement mentionnée par Darwin mais abandonnée plus tard.
En comparant les anatomies des espèces modernes et éteintes, les paléontologues peuvent déduire les lignées de ces espèces.
Plus récemment, la preuve d'une descendance commune est venue de l'étude des similitudes biochimiques entre les organismes.
Les cellules eucaryotes sont apparues il y a entre 1,6 et 2,7 milliards d'années.
Un autre engloutissement d'organismes de type cyanobactérien a conduit à la formation de chloroplastes dans les algues et les plantes.
En janvier 2016, des scientifiques ont rapporté qu'il y a environ 800 millions d'années, un changement génétique mineur dans une seule molécule appelée GK-PID aurait pu permettre aux organismes de passer d'un organisme à cellule unique à l'une des nombreuses cellules.
Divers déclencheurs de l'explosion cambrienne ont été proposés, notamment l'accumulation d'oxygène dans l'atmosphère à partir de la photosynthèse.
La sélection artificielle est la sélection intentionnelle de traits dans une population d'organismes.
Les protéines aux propriétés précieuses ont évolué par des cycles répétés de mutation et de sélection (par exemple des enzymes modifiées et de nouveaux anticorps) dans un processus appelé évolution dirigée.
L'élevage de différentes populations de ce poisson aveugle a produit une progéniture aux yeux fonctionnels, car différentes mutations s'étaient produites dans les populations isolées qui avaient évolué dans différentes grottes.
De nombreuses maladies humaines ne sont pas des phénomènes statiques, mais capables d'évolution.
Il est possible que nous soyons confrontés à la fin de la durée de vie effective de la plupart des antibiotiques disponibles et que prédire l'évolution et l'évolutivité de nos agents pathogènes et concevoir des stratégies pour la ralentir ou la contourner nécessite une connaissance plus approfondie des forces complexes à l'origine de l'évolution au niveau moléculaire.
Il a utilisé des stratégies d'évolution pour résoudre des problèmes d'ingénierie complexes.
Dans certains pays, notamment aux États-Unis, ces tensions entre science et religion ont alimenté l'actuelle controverse création-évolution, un conflit religieux axé sur la politique et l'éducation publique.
La décision Scopes Trial de 1925 a rendu le sujet très rare dans les manuels de biologie secondaire américains pendant une génération, mais il a été progressivement réintroduit plus tard et est devenu légalement protégé avec la décision Epperson v. Arkansas de 1968 .
La sélection naturelle est la survie et la reproduction différentielles des individus en raison de différences de phénotype.
La variation existe au sein de toutes les populations d'organismes.
L'environnement d'un génome comprend la biologie moléculaire dans la cellule, les autres cellules, les autres individus, les populations, les espèces, ainsi que l'environnement abiotique.
La sélection naturelle est une pierre angulaire de la biologie moderne.
Le concept de sélection naturelle s'est développé à l'origine en l'absence d'une théorie valable de l'hérédité ; au moment où Darwin écrivait, la science n'avait pas encore développé les théories modernes de la génétique.
Les arguments classiques ont été réintroduits au 18ème siècle par Pierre Louis Maupertuis et d'autres, dont le grand-père de Darwin, Erasmus Darwin.
Le succès de cette théorie a fait prendre conscience de la vaste échelle des temps géologiques et a rendu plausible l'idée que de minuscules changements pratiquement imperceptibles au cours des générations successives pouvaient avoir des conséquences sur l'échelle des différences entre les espèces.
Il était en train d'écrire son "gros livre" pour présenter ses recherches lorsque le naturaliste Alfred Russel Wallace conçut indépendamment le principe et le décrivit dans un essai qu'il envoya à Darwin pour le transmettre à Charles Lyell.
Dans la 3e édition de 1861, Darwin a reconnu que d'autres - comme William Charles Wells en 1813 et Patrick Matthew en 1831 - avaient proposé des idées similaires, mais ne les avaient ni développées ni présentées dans des publications scientifiques notables.
Dans une lettre à Charles Lyell en septembre 1860, Darwin regrettait l'utilisation du terme " Sélection Naturelle ", préférant le terme " Préservation Naturelle ".
Cependant, la sélection naturelle est restée controversée en tant que mécanisme, en partie parce qu'elle était perçue comme trop faible pour expliquer la gamme des caractéristiques observées des organismes vivants, et en partie parce que même les partisans de l'évolution rechignaient à sa nature "non guidée" et non progressive, un réponse qui a été qualifiée d'obstacle le plus important à l'acceptation de l'idée.
Avec l'intégration de l'évolution au début du XXe siècle avec les lois de l'hérédité de Mendel, la soi-disant synthèse moderne, les scientifiques en sont généralement venus à accepter la sélection naturelle.
JBS Haldane a introduit le concept du "coût" de la sélection naturelle.
Cependant, la sélection naturelle est « aveugle » dans le sens où des changements de phénotype peuvent donner un avantage reproductif, que le trait soit héréditaire ou non.
Si les traits qui confèrent à ces individus un avantage reproductif sont également héréditaires, c'est-à-dire transmis du parent à la progéniture, il y aura alors une reproduction différentielle, c'est-à-dire une proportion légèrement plus élevée de lapins rapides ou d'algues efficaces dans la génération suivante.
Cela donne l'apparence d'un but, mais dans la sélection naturelle, il n'y a pas de choix intentionnel.
Cela a donné aux papillons de couleur foncée une meilleure chance de survivre pour produire une progéniture de couleur foncée, et en seulement cinquante ans après la capture du premier papillon de nuit noir, presque tous les papillons de nuit du Manchester industriel étaient sombres.
Si un organisme vit deux fois moins longtemps que les autres de son espèce, mais a deux fois plus de descendants survivant jusqu'à l'âge adulte, ses gènes deviennent plus courants dans la population adulte de la génération suivante.
Une distinction doit être faite entre le concept de "survie du plus apte" et "l'amélioration de la condition physique". "
Haldane a appelé ce processus "substitution" ou plus communément en biologie, cela s'appelle "fixation".
La probabilité qu'une mutation bénéfique se produise sur un membre d'une population dépend du nombre total de réplications de cette variante.
Dans cette expérience, "l'amélioration de la condition physique" dépend du nombre de réplications de la variante particulière pour qu'une nouvelle variante apparaisse qui soit capable de se développer dans la région de concentration de médicament la plus élevée suivante.
L'expérience classique d'évolution à long terme d'E. coli de Richard Lenski est un exemple d'adaptation dans un environnement compétitif ("amélioration de la condition physique" pendant la "survie du plus apte").
La sélection perturbatrice rare agit également pendant les périodes de transition lorsque le mode actuel est sous-optimal, mais modifie le trait dans plus d'une direction.
Certains biologistes n'en reconnaissent que deux types: la sélection de viabilité (ou de survie), qui agit pour augmenter la probabilité de survie d'un organisme, et la sélection de fécondité (ou de fertilité ou de reproduction), qui agit pour augmenter le taux de reproduction, compte tenu de la survie.
Dans la sélection des parents et les conflits intragénomiques, la sélection au niveau des gènes fournit une explication plus appropriée du processus sous-jacent.
La sélection écologique est la sélection naturelle par tout autre moyen que la sélection sexuelle, comme la sélection des parents, la compétition et l'infanticide.
Cependant, chez certaines espèces, le choix du partenaire revient principalement aux mâles, comme chez certains poissons de la famille des Syngnathidae.
Depuis la découverte de la pénicilline en 1928, les antibiotiques ont été utilisés pour lutter contre les maladies bactériennes.
La variation génétique est le résultat de mutations, de recombinaisons génétiques et d'altérations du caryotype (le nombre, la forme, la taille et la disposition interne des chromosomes).
Cependant, de nombreuses mutations dans l'ADN non codant ont des effets délétères.
Les modifications de ceux-ci ont souvent des effets importants sur le phénotype de l'individu car elles régulent la fonction de nombreux autres gènes.
Lorsque de telles mutations entraînent une meilleure forme physique, la sélection naturelle favorise ces phénotypes et le nouveau trait se propage dans la population.
Cependant, il est intrinsèque au concept d'espèce contre laquelle les hybrides sont sélectionnés, s'opposant à l'évolution de l'isolement reproductif, un problème reconnu par Darwin.
Le phénotype est déterminé par la constitution génétique d'un organisme (génotype) et l'environnement dans lequel l'organisme vit.
Un exemple est les antigènes de groupe sanguin ABO chez l'homme, où trois allèles régissent le phénotype.
Ce processus peut se poursuivre jusqu'à ce que l'allèle soit fixé et que l'ensemble de la population partage le phénotype plus adapté.
La sélection stabilisante conserve les caractéristiques génétiques fonctionnelles, telles que les gènes codant pour les protéines ou les séquences régulatrices, au fil du temps par une pression sélective contre les variants délétères.
Certaines formes de sélection d'équilibrage n'entraînent pas de fixation, mais maintiennent un allèle à des fréquences intermédiaires dans une population.
Le maintien de la variation allélique peut également se produire par une sélection perturbatrice ou diversifiée, qui favorise les génotypes qui s'écartent de la moyenne dans les deux sens (c'est-à-dire le contraire de la surdominance), et peut entraîner une distribution bimodale des valeurs des traits.
Cependant, après une période sans nouvelles mutations, la variation génétique à ces sites est éliminée en raison de la dérive génétique.
Le résultat exact des deux processus dépend à la fois de la vitesse à laquelle les nouvelles mutations se produisent et de la force de la sélection naturelle, qui est fonction du caractère défavorable de la mutation.
La probabilité qu'un tel remaniement se produise entre deux allèles est inversement proportionnelle à la distance qui les sépare.
Un fort balayage sélectif aboutit à une région du génome où l'haplotype sélectionné positivement (l'allèle et ses voisins) sont essentiellement les seuls qui existent dans la population.
La sélection d'arrière-plan est l'opposé d'un balayage sélectif.
Selon les mots du philosophe Daniel Dennett, "l'idée dangereuse de Darwin" de l'évolution par la sélection naturelle est un "acide universel", qui ne peut être limité à aucun récipient ou récipient, car il s'échappe rapidement, se frayant un chemin dans des alentours.
Ces conditions sont : l'héritabilité, la variation de type et la compétition pour des ressources limitées.
Herbert Spencer et le défenseur de l'eugénisme L'interprétation de Francis Galton de la sélection naturelle comme nécessairement progressive, conduisant à des progrès supposés de l'intelligence et de la civilisation, est devenue une justification du colonialisme, de l'eugénisme et du darwinisme social.
L'idée raciale en tant que base de notre État a déjà beaucoup accompli à cet égard."
L'exemple le plus frappant de la psychologie évolutionniste, notamment avancé dans les premiers travaux de Noam Chomsky et plus tard par Steven Pinker, est l'hypothèse selon laquelle le cerveau humain s'est adapté pour acquérir les règles grammaticales du langage naturel.
Il a observé que les organismes (plantes de pois) héritent des traits au moyen d '«unités d'héritage» discrètes.
La structure et la fonction, la variation et la distribution des gènes sont étudiées dans le contexte de la cellule, de l'organisme (par exemple la dominance) et dans le contexte d'une population.
Les processus génétiques fonctionnent en combinaison avec l'environnement et les expériences d'un organisme pour influencer le développement et le comportement, souvent appelés nature contre culture.
La science moderne de la génétique, cherchant à comprendre ce processus, a commencé avec les travaux du frère augustin Gregor Mendel au milieu du XIXe siècle.
Sa deuxième loi est la même que celle publiée par Mendel.
Une théorie populaire au 19ème siècle, et impliquée par Charles Darwin de 1859 Sur l'origine des espèces, était l'héritage mixte : l'idée que les individus héritent d'un mélange homogène de traits de leurs parents.
Dans son article «Versuche über Pflanzenhybriden» («Expériences sur l'hybridation végétale»), présenté en 1865 à la Naturforschender Verein (Société pour la recherche dans la nature) à Brünn, Mendel a retracé les modèles d'héritage de certains traits chez les plantes de pois et les a décrits mathématiquement.
William Bateson, un partisan des travaux de Mendel, a inventé le mot génétique en 1905 (l'adjectif génétique, dérivé du mot grec genèse - γένεσις , "origine", est antérieur au nom et a été utilisé pour la première fois dans un sens biologique en 1860).
Au cours des 11 années suivantes, elle a découvert que les femmes n'avaient que le chromosome X et que les hommes avaient les chromosomes X et Y.
James Watson et Francis Crick ont déterminé la structure de l'ADN en 1953, en utilisant les travaux de cristallographie aux rayons X de Rosalind Franklin et Maurice Wilkins qui indiquaient que l'ADN avait une structure hélicoïdale (c'est-à-dire en forme de tire-bouchon).
La structure a également suggéré une méthode simple de réplication : si les brins sont séparés, de nouveaux brins partenaires peuvent être reconstruits pour chacun en fonction de la séquence de l'ancien brin.
Au cours des années suivantes, les scientifiques ont tenté de comprendre comment l'ADN contrôle le processus de production des protéines.
Avec la nouvelle compréhension moléculaire de l'hérédité, une explosion de la recherche s'est produite.
Un développement important a été le séquençage de l'ADN par terminaison de chaîne en 1977 par Frederick Sanger.
Dans ses expériences étudiant le trait de couleur des fleurs, Mendel a observé que les fleurs de chaque plante de pois étaient soit violettes, soit blanches, mais jamais un intermédiaire entre les deux couleurs.
De nombreuses espèces, y compris les humains, ont ce modèle d'hérédité.
Lorsque les organismes sont hétérozygotes au niveau d'un gène, un allèle est souvent appelé dominant car ses qualités dominent le phénotype de l'organisme, tandis que l'autre allèle est appelé récessif car ses qualités diminuent et ne sont pas observées.
Souvent, un symbole "+" est utilisé pour marquer l'allèle habituel non mutant d'un gène.
L'un des diagrammes couramment utilisés pour prédire le résultat du croisement est le carré de Punnett.
Certains gènes ne s'assortissent pas indépendamment, démontrant une liaison génétique, un sujet abordé plus loin dans cet article.)
Un autre gène, cependant, contrôle si les fleurs ont une couleur ou si elles sont blanches.
De nombreux traits ne sont pas des caractéristiques discrètes (par exemple, des fleurs violettes ou blanches), mais plutôt des caractéristiques continues (par exemple, la taille humaine et la couleur de la peau).
Le degré auquel les gènes d'un organisme contribuent à un trait complexe est appelé héritabilité.
L'ADN est composé d'une chaîne de nucléotides, dont il existe quatre types : l'adénine (A), la cytosine (C), la guanine (G) et la thymine (T).
Les virus ne peuvent pas se reproduire sans hôte et ne sont pas affectés par de nombreux processus génétiques, ils ont donc tendance à ne pas être considérés comme des organismes vivants.
Cette structure de l'ADN est la base physique de l'hérédité : la réplication de l'ADN duplique l'information génétique en divisant les brins et en utilisant chaque brin comme modèle pour la synthèse d'un nouveau brin partenaire.
Ces brins d'ADN sont souvent extrêmement longs ; le plus grand chromosome humain, par exemple, a une longueur d'environ 247 millions de paires de bases.
L'ADN se trouve le plus souvent dans le noyau des cellules, mais Ruth Sager a contribué à la découverte de gènes non chromosomiques trouvés à l'extérieur du noyau.
Alors que les organismes haploïdes n'ont qu'une seule copie de chaque chromosome, la plupart des animaux et de nombreuses plantes sont diploïdes, contenant deux de chaque chromosome et donc deux copies de chaque gène.
Chez l'homme et de nombreux autres animaux, le chromosome Y contient le gène qui déclenche le développement des caractéristiques spécifiquement masculines.
Ce processus, appelé mitose, est la forme de reproduction la plus simple et constitue la base de la reproduction asexuée.
Les organismes eucaryotes utilisent souvent la reproduction sexuée pour générer une progéniture qui contient un mélange de matériel génétique hérité de deux parents différents.
Certaines bactéries peuvent subir une conjugaison, transférant un petit morceau circulaire d'ADN à une autre bactérie.
De cette façon, de nouvelles combinaisons de gènes peuvent se produire dans la progéniture d'un couple d'accouplement.
Pendant le croisement, les chromosomes échangent des tronçons d'ADN, mélangeant efficacement les allèles du gène entre les chromosomes.
La première démonstration cytologique de croisement a été réalisée par Harriet Creighton et Barbara McClintock en 1931.
Pour une distance arbitrairement longue, la probabilité de croisement est suffisamment élevée pour que l'héritage des gènes soit effectivement non corrélé.
La séquence spécifique d'acides aminés donne une structure tridimensionnelle unique pour cette protéine, et les structures tridimensionnelles des protéines sont liées à leurs fonctions.
La structure des protéines est dynamique ; la protéine hémoglobine se plie en des formes légèrement différentes car elle facilite la capture, le transport et la libération de molécules d'oxygène dans le sang des mammifères.
Par exemple, la drépanocytose est une maladie génétique humaine qui résulte d'une seule différence de base dans la région codante de la section β -globine de l'hémoglobine, provoquant un seul changement d'acide aminé qui modifie les propriétés physiques de l'hémoglobine.
Certaines séquences d'ADN sont transcrites en ARN mais ne sont pas traduites en produits protéiques - ces molécules d'ARN sont appelées ARN non codant.
Un exemple intéressant est la coloration du pelage du chat siamois.
Mais ces protéines productrices de poils foncés sont sensibles à la température (c'est-à-dire qu'elles ont une mutation provoquant une sensibilité à la température) et se dénaturent dans des environnements à température plus élevée, ne produisant pas de pigment de poils foncés dans les zones où le chat a une température corporelle plus élevée.
Après la chute de l'Empire romain d'Occident, la connaissance des conceptions grecques du monde s'est détériorée en Europe occidentale au cours des premiers siècles (400 à 1000 CE) du Moyen Âge, mais a été préservée dans le monde musulman pendant l'âge d'or islamique.
La science moderne est généralement divisée en trois grandes branches qui comprennent les sciences naturelles (par exemple, la biologie, la chimie et la physique), qui étudient la nature au sens le plus large ; les sciences sociales (par exemple, l'économie, la psychologie et la sociologie), qui étudient les individus et les sociétés ; et les sciences formelles (par exemple, la logique, les mathématiques et l'informatique théorique), qui traitent des symboles régis par des règles.
Les nouvelles connaissances scientifiques sont avancées par la recherche de scientifiques motivés par la curiosité du monde et le désir de résoudre des problèmes.
En particulier, c'était le type de connaissances que les gens peuvent communiquer entre eux et partager.
Cependant, aucune distinction consciente cohérente n'a été faite entre la connaissance de telles choses, qui sont vraies dans chaque communauté, et d'autres types de connaissances communautaires, telles que les mythologies et les systèmes juridiques.
Ils ont même développé un calendrier officiel qui contenait douze mois, trente jours chacun, et cinq jours à la fin de l'année.
Pour cette raison, on prétend que ces hommes furent les premiers philosophes au sens strict, et aussi les premiers à distinguer clairement « nature » et « convention ».
En revanche, essayer d'utiliser la connaissance de la nature pour imiter la nature (artifice ou technologie, technē grec) était considéré par les scientifiques classiques comme un intérêt plus approprié pour les artisans de la classe sociale inférieure.
La théorie des atomes a été développée par le philosophe grec Leucippe et son élève Démocrite.
La méthode socratique telle que documentée par les dialogues de Platon est une méthode dialectique d'élimination des hypothèses : de meilleures hypothèses sont trouvées en identifiant et en éliminant régulièrement celles qui conduisent à des contradictions.
Socrate a critiqué l'ancien type d'étude de la physique comme étant trop purement spéculatif et manquant d'autocritique.
Aristote a créé plus tard un programme systématique de philosophie téléologique : le mouvement et le changement sont décrits comme l'actualisation des potentiels déjà présents dans les choses, selon les types de choses qu'elles sont.
Les socratiques ont également insisté sur le fait que la philosophie devait être utilisée pour considérer la question pratique de la meilleure façon de vivre pour un être humain (une étude Aristote divisée en éthique et philosophie politique).
Le modèle d'Aristarque a été largement rejeté car on croyait qu'il violait les lois de la physique.
John Philoponus, un érudit byzantin des années 500, a remis en question l'enseignement de la physique d'Aristote, notant ses défauts.
Les quatre causes d'Aristote prescrivaient de répondre de quatre manières à la question « pourquoi » afin d'expliquer les choses scientifiquement.
Cependant, les textes originaux d'Aristote ont finalement été perdus en Europe occidentale, et un seul texte de Platon était largement connu, le Timée, qui était le seul dialogue platonicien, et l'une des rares œuvres originales de philosophie naturelle classique, disponible pour les lecteurs latins dans le début du Moyen Âge.
De nombreuses traductions syriaques ont été réalisées par des groupes tels que les nestoriens et les monophysites.
p. 465 : "ce n'est que lorsque l'influence d'ibn al-Haytam et d'autres sur le courant dominant des écrits physiques médiévaux ultérieurs a été sérieusement étudiée que l'affirmation de Schramm selon laquelle ibn al-Haytam était le véritable fondateur de la physique moderne peut être évaluée."
Le canon d'Avicenne est considéré comme l'une des publications les plus importantes en médecine et ils ont tous deux contribué de manière significative à la pratique de la médecine expérimentale, en utilisant des essais cliniques et des expériences pour étayer leurs affirmations.
De plus, les textes grecs classiques ont commencé à être traduits de l'arabe et du grec vers le latin, donnant un niveau plus élevé de discussion scientifique en Europe occidentale.
Des copies manuscrites du Livre d'optique d'Alhazen se sont également propagées à travers l'Europe avant 1240, comme en témoigne son incorporation dans la Perspectiva de Vitello.
L'afflux de textes anciens a provoqué la Renaissance du XIIe siècle et l'épanouissement d'une synthèse du catholicisme et de l'aristotélisme connue sous le nom de scolastique en Europe occidentale, qui est devenue un nouveau centre géographique de la science.
Un modèle de vision connu plus tard sous le nom de perspectivisme a été exploité et étudié par les artistes de la Renaissance.
Ceci était basé sur un théorème selon lequel les périodes orbitales des planètes sont plus longues car leurs orbes sont plus éloignées du centre du mouvement, ce qu'il a trouvé en désaccord avec le modèle de Ptolémée.
Il a découvert que toute la lumière d'un seul point de la scène était concentrée en un seul point à l'arrière de la sphère de verre.
Kepler n'a pas rejeté la métaphysique aristotélicienne et a décrit son travail comme une recherche de l'Harmonie des Sphères.
Galilée avait utilisé des arguments du pape et les avait mis dans la voix du niais dans l'ouvrage "Dialogue concernant les deux principaux systèmes mondiaux", qui avait grandement offensé Urbain VIII.
Descartes a mis l'accent sur la pensée individuelle et a soutenu que les mathématiques plutôt que la géométrie devraient être utilisées pour étudier la nature.
Cette nouvelle science a commencé à se considérer comme décrivant les "lois de la nature".
Dans le style de Francis Bacon, Leibniz a supposé que différents types de choses fonctionnent tous selon les mêmes lois générales de la nature, sans causes formelles ou finales particulières pour chaque type de chose.
Selon les mots de Bacon, "le but réel et légitime des sciences est de doter la vie humaine de nouvelles inventions et richesses", et il a découragé les scientifiques de poursuivre des idées philosophiques ou spirituelles intangibles, qui, selon lui, contribuaient peu au bonheur humain au-delà de "la fumée de spéculation subtile, sublime ou agréable ».
Un autre développement important a été la vulgarisation de la science parmi une population de plus en plus alphabétisée.
Les philosophes des Lumières ont choisi une brève histoire des prédécesseurs scientifiques - Galilée, Boyle et Newton principalement - comme guides et garants de leurs applications du concept singulier de nature et de loi naturelle à tous les domaines physiques et sociaux de l'époque.
Hume et d'autres penseurs écossais des Lumières ont développé une "science de l'homme", qui s'est exprimée historiquement dans des œuvres d'auteurs tels que James Burnett, Adam Ferguson, John Millar et William Robertson, qui ont tous fusionné une étude scientifique sur le comportement des humains dans les temps anciens et primitifs. cultures avec une forte conscience des forces déterminantes de la modernité.
John Herschel et William Whewell ont systématisé la méthodologie : ce dernier a inventé le terme scientifique.
Par ailleurs, Gregor Mendel a présenté son article, "Versuche über Pflanzenhybriden" ("Expériences sur l'hybridation végétale"), en 1865, qui décrivait les principes de l'héritage biologique, servant de base à la génétique moderne.
Les phénomènes qui permettraient la déconstruction de l'atome ont été découverts dans la dernière décennie du XIXe siècle : la découverte des rayons X a inspiré la découverte de la radioactivité.
En outre, l'utilisation intensive de l'innovation technologique stimulée par les guerres de ce siècle a entraîné des révolutions dans les transports (automobiles et avions), le développement des ICBM, une course à l'espace et une course aux armements nucléaires.
La découverte du rayonnement de fond diffus cosmologique en 1964 a conduit au rejet de la théorie de l'état stationnaire de l'univers au profit de la théorie du Big Bang de Georges Lemaître.
L'utilisation généralisée des circuits intégrés dans le dernier quart du XXe siècle, combinée aux satellites de communication, a conduit à une révolution des technologies de l'information et à l'essor de l'Internet mondial et de l'informatique mobile, y compris les smartphones.
Les sciences naturelles et sociales sont des sciences empiriques, car leurs connaissances sont basées sur des observations empiriques et peuvent être testées pour leur validité par d'autres chercheurs travaillant dans les mêmes conditions.
Par exemple, les sciences physiques peuvent être subdivisées en physique, chimie, astronomie et sciences de la terre.
Pourtant, les perspectives philosophiques, les conjectures et les présupposés, souvent négligés, restent nécessaires en sciences naturelles.
Il comprend les mathématiques, la théorie des systèmes et l'informatique théorique.
Les sciences formelles sont donc a priori des disciplines et de ce fait, il y a désaccord sur le point de savoir si elles constituent réellement une science.
L'ingénierie elle-même englobe une gamme de domaines d'ingénierie plus spécialisés, chacun mettant un accent plus spécifique sur des domaines particuliers des mathématiques appliquées, des sciences et des types d'application.
il a répondu: "Monsieur, à quoi sert un enfant nouveau-né?".
Cette nouvelle explication est utilisée pour faire des prédictions falsifiables qui sont testables par expérience ou observation.
Cela se fait en partie par l'observation de phénomènes naturels, mais aussi par l'expérimentation qui tente de simuler des événements naturels dans des conditions contrôlées adaptées à la discipline (dans les sciences d'observation, telles que l'astronomie ou la géologie, une observation prédite peut remplacer une observation contrôlée). expérience).
Si l'hypothèse a survécu aux tests, elle peut être adoptée dans le cadre d'une théorie scientifique, un modèle ou un cadre logiquement raisonné et cohérent pour décrire le comportement de certains phénomènes naturels.
Dans cette veine, les théories sont formulées selon la plupart des mêmes principes scientifiques que les hypothèses.
Ceci peut être réalisé par une conception expérimentale minutieuse, la transparence et un processus approfondi d'examen par les pairs des résultats expérimentaux ainsi que des conclusions.
Les statistiques, une branche des mathématiques, sont utilisées pour résumer et analyser des données, ce qui permet aux scientifiques d'évaluer la fiabilité et la variabilité de leurs résultats expérimentaux.
Cela peut être mis en contraste avec l'anti-réalisme, l'idée que le succès de la science ne dépend pas de sa précision sur des entités non observables telles que les électrons.
Il existe différentes écoles de pensée en philosophie des sciences.
Cela est nécessaire car le nombre de prédictions que font ces théories est infini, ce qui signifie qu'elles ne peuvent pas être connues à partir d'une quantité finie de preuves en utilisant uniquement la logique déductive.
Le rationalisme critique est une approche contrastée de la science du XXe siècle, définie pour la première fois par le philosophe austro-britannique Karl Popper.
Popper a proposé de remplacer la vérifiabilité par la falsification comme point de repère des théories scientifiques et de remplacer l'induction par la falsification comme méthode empirique.
Une autre approche, l'instrumentalisme, met l'accent sur l'utilité des théories comme instruments d'explication et de prédiction des phénomènes.
Proche de l'instrumentalisme se trouve l'empirisme constructif, selon lequel le principal critère de succès d'une théorie scientifique est de savoir si ce qu'elle dit sur les entités observables est vrai.
Chaque paradigme a ses propres questions, objectifs et interprétations.
Autrement dit, le choix d'un nouveau paradigme est basé sur des observations, même si ces observations sont faites dans le contexte de l'ancien paradigme.
Son point principal est qu'il faut faire une différence entre les explications naturelles et surnaturelles et que la science doit être limitée méthodologiquement aux explications naturelles.
Autrement dit, aucune théorie n'est jamais considérée comme strictement certaine car la science accepte le concept de faillibilisme.
Les nouvelles connaissances scientifiques entraînent rarement de vastes changements dans notre compréhension.
Les connaissances scientifiques sont acquises par une synthèse progressive des informations issues de différentes expériences menées par divers chercheurs dans différentes branches de la science; c'est plus comme une montée qu'un saut.
Le philosophe Barry Stroud ajoute que, bien que la meilleure définition de la « connaissance » soit contestée, être sceptique et divertir la possibilité que l'on soit incorrect est compatible avec le fait d'être correct.
C'est particulièrement le cas dans les domaines scientifiques plus macroscopiques (par exemple la psychologie, la cosmologie physique).
Depuis lors, le nombre total de périodiques actifs n'a cessé d'augmenter.
Bien que les revues soient en 39 langues, 91 % des articles indexés sont publiés en anglais.
Les bamazines scientifiques telles que New Scientist, Science & Vie et Scientific American répondent aux besoins d'un lectorat beaucoup plus large et fournissent un résumé non technique des domaines de recherche populaires, y compris des découvertes et des avancées notables dans certains domaines de recherche.
Divers types de publicité commerciale, allant du battage publicitaire à la fraude, peuvent appartenir à ces catégories.
De nombreux scientifiques poursuivent des carrières dans divers secteurs de l'économie tels que les universités, l'industrie, le gouvernement et les organisations à but non lucratif.
Par exemple, Christine Ladd (1847-1930) a pu entrer dans un doctorat. programme comme "C. Ladd"; Christine "Kitty" Ladd a terminé les exigences en 1882, mais n'a obtenu son diplôme qu'en 1926, après une carrière qui s'étendait sur l'algèbre de la logique (voir table de vérité), la vision des couleurs et la psychologie.
À la fin du XXe siècle, le recrutement actif de femmes et l'élimination de la discrimination institutionnelle fondée sur le sexe ont considérablement augmenté le nombre de femmes scientifiques, mais de grandes disparités entre les sexes subsistent dans certains domaines; au début du XXIe siècle, plus de la moitié des nouveaux biologistes étaient des femmes, tandis que 80 % des doctorats en physique sont décernés à des hommes.
L'adhésion peut être ouverte à tous, peut nécessiter la possession de certaines références scientifiques ou peut être un honneur conféré par élection.
La politique scientifique traite ainsi de l'ensemble des questions qui concernent les sciences naturelles.
Parmi les exemples historiques les plus marquants, citons la Grande Muraille de Chine, achevée au cours de deux millénaires grâce au soutien de l'État de plusieurs dynasties, et le Grand Canal du fleuve Yangtze, un immense exploit d'ingénierie hydraulique commencé par Sunshu Ao (孫叔敖7e siècle).
Ces processus, qui sont gérés par le gouvernement, des entreprises ou des fondations, allouent des fonds rares.
La proportion de financement public dans certaines industries est plus élevée et domine la recherche en sciences sociales et humaines.
De nombreux facteurs peuvent agir comme des facettes de la politisation de la science, tels que l'anti-intellectualisme populiste, les menaces perçues pour les croyances religieuses, le subjectivisme postmoderne et la peur des intérêts commerciaux.
Une expérience est une procédure effectuée pour étayer ou infirmer une hypothèse.
Les expériences peuvent augmenter les résultats des tests et aider un élève à s'engager et à s'intéresser davantage au matériel qu'il apprend, en particulier lorsqu'il est utilisé au fil du temps.
Les expériences incluent généralement des contrôles, qui sont conçus pour minimiser les effets de variables autres que la variable indépendante unique.
Les chercheurs utilisent également l'expérimentation pour tester des théories existantes ou de nouvelles hypothèses pour les soutenir ou les réfuter.
Si une expérience est menée avec soin, les résultats confirment ou réfutent généralement l'hypothèse.
En médecine et en sciences sociales, la prévalence de la recherche expérimentale varie considérablement d'une discipline à l'autre.
Une seule étude n'implique généralement pas de répétitions de l'expérience, mais des études distinctes peuvent être agrégées par un examen systématique et une méta-analyse.
De cette manière, nous pouvons éventuellement arriver à la vérité qui satisfait le cœur et atteindre graduellement et prudemment la fin à laquelle la certitude apparaît ; tandis que par la critique et la prudence, nous pouvons saisir la vérité qui dissipe le désaccord et résout les questions douteuses.
Dans ce processus de réflexion critique, l'homme lui-même ne doit pas oublier qu'il tend vers des opinions subjectives – par « préjugés » et « indulgence » – et doit donc être critique sur sa propre manière de construire des hypothèses.
Bacon voulait une méthode reposant sur des observations répétables ou des expériences.
Par exemple, Galileo Galilei (1564–1642) a mesuré avec précision le temps et a fait des expériences pour faire des mesures et des conclusions précises sur la vitesse d'un corps qui tombe.
Dans certaines disciplines (par exemple, la psychologie ou les sciences politiques), une « véritable expérience » est une méthode de recherche sociale dans laquelle il existe deux types de variables.
Un bon exemple serait un essai de drogue.
Les résultats des échantillons répétés peuvent souvent être moyennés, ou si l'un des réplicats est manifestement incohérent avec les résultats des autres échantillons, il peut être rejeté comme étant le résultat d'une erreur expérimentale (une étape de la procédure de test peut avoir été erronée omis pour cet échantillon).
Un contrôle négatif est connu pour donner un résultat négatif.
Le plus souvent, la valeur du contrôle négatif est traitée comme une valeur "de fond" à soustraire des résultats de l'échantillon de test.
Les élèves peuvent recevoir un échantillon de liquide contenant une quantité inconnue (pour l'élève) de protéines.
Les élèves pourraient préparer plusieurs échantillons de contrôle positif contenant diverses dilutions de l'étalon protéique.
Le test est un test colorimétrique dans lequel un spectrophotomètre peut mesurer la quantité de protéines dans des échantillons en détectant un complexe coloré formé par l'interaction de molécules de protéines et de molécules d'un colorant ajouté.
Dans ce cas, l'expérience commence par créer deux groupes d'échantillons ou plus qui sont probabilistes équivalents, ce qui signifie que les mesures des traits doivent être similaires entre les groupes et que les groupes doivent répondre de la même manière s'ils reçoivent le même traitement.
Une fois les groupes équivalents constitués, l'expérimentateur essaie de les traiter à l'identique à l'exception de la seule variable qu'il souhaite isoler.
Cela garantit que tous les effets sur le volontaire sont dus au traitement lui-même et ne sont pas une réponse à la connaissance qu'il est traité.
Ces hypothèses suggèrent des raisons pour expliquer un phénomène, ou prédire les résultats d'une action.
L'hypothèse nulle est qu'il n'y a pas d'explication ou de pouvoir prédictif du phénomène par le raisonnement étudié.
Dans la mesure du possible, ils tentent de collecter des données pour le système de manière à ce que la contribution de toutes les variables puisse être déterminée et où les effets de la variation de certaines variables restent approximativement constants afin que les effets d'autres variables puissent être discernés.
Habituellement, cependant, il existe une certaine corrélation entre ces variables, ce qui réduit la fiabilité des expériences naturelles par rapport à ce qui pourrait être conclu si une expérience contrôlée était réalisée.
Par exemple, en astronomie il est clairement impossible, en testant l'hypothèse "Les étoiles sont des nuages d'hydrogène effondrés", de partir d'un nuage géant d'hydrogène, puis de faire l'expérience d'attendre quelques milliards d'années pour qu'il forme une étoile .
Pour cette raison, les expériences sur le terrain sont parfois considérées comme ayant une validité externe plus élevée que les expériences en laboratoire.
Dans ces situations, les études observationnelles ont de la valeur car elles suggèrent souvent des hypothèses qui peuvent être testées avec des expériences randomisées ou en collectant de nouvelles données.
De plus, les études d'observation (par exemple, dans les systèmes biologiques ou sociaux) impliquent souvent des variables difficiles à quantifier ou à contrôler.
Sans modèle statistique reflétant une randomisation objective, l'analyse statistique repose sur un modèle subjectif.
Par exemple, les études épidémiologiques sur le cancer du côlon montrent systématiquement des corrélations bénéfiques avec la consommation de brocoli, tandis que les expériences ne trouvent aucun avantage.
Pour tout essai randomisé, une certaine variation par rapport à la moyenne est bien sûr attendue, mais la randomisation garantit que les groupes expérimentaux ont des valeurs moyennes proches, en raison du théorème central limite et de l'inégalité de Markov.
Pour éviter les conditions qui rendent une expérience beaucoup moins utile, les médecins menant des essais médicaux - par exemple pour l'approbation de la Food and Drug Administration des États-Unis - quantifient et randomisent les covariables qui peuvent être identifiées.
Il est également généralement contraire à l'éthique (et souvent illégal) de mener des expériences randomisées sur les effets de traitements de qualité inférieure ou nocifs, tels que les effets de l'ingestion d'arsenic sur la santé humaine.
Un laboratoire de physique peut contenir un accélérateur de particules ou une chambre à vide, tandis qu'un laboratoire de métallurgie peut avoir des appareils pour couler ou affiner les métaux ou pour tester leur résistance.
Les scientifiques d'autres domaines utiliseront encore d'autres types de laboratoires.
Malgré la notion sous-jacente du lab comme espace confiné pour experts, le terme «laboratoire» s'applique également de plus en plus à des espaces d'ateliers tels que Living Labs, Fab Labs ou Hackerspaces, dans lesquels des personnes se réunissent pour travailler sur des problèmes de société ou réaliser des prototypes, travailler en collaboration ou partager des ressources.
Ce laboratoire a été créé lorsque Pythagore a mené une expérience sur les tonalités du son et la vibration de la corde.
Un laboratoire alchimique souterrain du XVIe siècle a été accidentellement découvert en 2002.
Les risques de laboratoire peuvent inclure des poisons ; Agents infectieux; matières inflammables, explosives ou radioactives ; machines mobiles; températures extrêmes; lasers, champs bamnétiques forts ou haute tension.
L'Occupational Safety and Health Administration (OSHA) aux États-Unis, reconnaissant les caractéristiques uniques du lieu de travail en laboratoire, a élaboré une norme pour l'exposition professionnelle aux produits chimiques dangereux dans les laboratoires.
Pour déterminer le plan d'hygiène chimique approprié pour une entreprise ou un laboratoire particulier, il est nécessaire de comprendre les exigences de la norme, l'évaluation des pratiques actuelles en matière de sécurité, de santé et d'environnement et l'évaluation des risques.
De plus, l'examen par un tiers est également utilisé pour fournir une "vue extérieure" objective qui offre un regard neuf sur les domaines et les problèmes qui peuvent être pris pour acquis ou négligés en raison de l'habitude.
La formation est essentielle au fonctionnement sûr et continu du laboratoire.
Par exemple, un groupe de recherche a un horaire où il effectue des recherches sur son propre sujet d'intérêt pour un jour de la semaine, mais pour le reste, il travaille sur un projet de groupe donné.
Un localisateur est un employé d'un laboratoire chargé de savoir où se trouve actuellement chaque membre du laboratoire, sur la base d'un signal unique émis par le badge de chaque membre du personnel.
A travers des études ethnographiques, un constat est que, parmi le personnel, chaque classe (chercheurs, administrateurs...) a un degré de droit différent, qui varie selon les laboratoires.
En examinant les diverses interactions entre les membres du personnel, nous pouvons déterminer leur position sociale dans l'organisation.
Ainsi, une conséquence de cette hiérarchie sociale est que le localisateur divulgue divers degrés d'informations, en fonction du membre du personnel et de ses droits.
La hiérarchie sociale est également liée aux attitudes envers les technologies.
Par exemple, un réceptionniste considérerait le badge comme utile, car il l'aiderait à localiser les membres du personnel pendant la journée.
Les membres du personnel se sentent mal à l'aise lorsqu'ils changent les schémas de droit, d'obligation, de respect, de hiérarchie informelle et formelle, etc.
La nature, au sens le plus large, est le monde ou l'univers naturel, physique et matériel. "
Bien que les humains fassent partie de la nature, l'activité humaine est souvent considérée comme une catégorie distincte des autres phénomènes naturels.
Le concept de nature dans son ensemble, l'univers physique, est l'une des nombreuses extensions de la notion originale; il a commencé avec certaines applications fondamentales du mot φύσις par les philosophes présocratiques (bien que ce mot ait alors eu une dimension dynamique, en particulier pour Héraclite), et n'a cessé de gagner en popularité depuis.
Cependant, une vision vitaliste de la nature, plus proche de celle présocratique, renaît à la même époque, notamment après Charles Darwin.
Il est souvent pris pour désigner «l'environnement naturel» ou la nature sauvage - les animaux sauvages, les rochers, la forêt et, en général, les éléments qui n'ont pas été considérablement modifiés par l'intervention humaine ou qui persistent malgré l'intervention humaine.
Ses caractéristiques climatiques les plus importantes sont ses deux grandes régions polaires, deux zones tempérées relativement étroites et une vaste région équatoriale tropicale à subtropicale.
Le reste se compose de continents et d'îles, la plupart des terres habitées se trouvant dans l'hémisphère nord.
L'intérieur reste actif, avec une épaisse couche de manteau en plastique et un noyau rempli de fer qui génère un champ bamnétique.
Les unités rocheuses sont d'abord mises en place soit par dépôt sur la surface, soit par intrusion dans la roche sus-jacente.
Le dégazage et l'activité volcanique ont produit l'atmosphère primordiale.
Les continents se sont formés, puis se sont séparés et reformés à mesure que la surface de la Terre s'est remodelée sur des centaines de millions d'années, se combinant parfois pour former un supercontinent.
Au cours de l'ère néoprotérozoïque, les températures glaciales couvraient une grande partie de la Terre dans les glaciers et les calottes glaciaires.
La dernière extinction massive s'est produite il y a environ 66 millions d'années, lorsqu'une collision de météorite a probablement déclenché l'extinction des dinosaures non aviaires et d'autres grands reptiles, mais a épargné les petits animaux tels que les mammifères.
L'avènement ultérieur de la vie humaine et le développement de l'agriculture et de la civilisation ont permis aux humains d'affecter la Terre plus rapidement que toute forme de vie précédente, affectant à la fois la nature et la quantité d'autres organismes ainsi que le climat mondial.
La fine couche de gaz qui enveloppe la Terre est maintenue en place par la gravité.
La couche d'ozone joue un rôle important dans l'épuisement de la quantité de rayonnement ultraviolet (UV) qui atteint la surface.
Le temps terrestre se produit presque exclusivement dans la partie inférieure de l'atmosphère et sert de système convectif pour redistribuer la chaleur.
Aussi, sans les redistributions d'énergie thermique par les courants océaniques et l'atmosphère, les tropiques seraient beaucoup plus chauds, et les régions polaires beaucoup plus froides.
La végétation de surface a développé une dépendance aux variations saisonnières du temps, et des changements soudains qui ne durent que quelques années peuvent avoir un effet dramatique, à la fois sur la végétation et sur les animaux qui dépendent de sa croissance pour leur alimentation.
D'après les archives historiques, la Terre est connue pour avoir subi des changements climatiques drastiques dans le passé, y compris des périodes glaciaires.
Il existe un certain nombre de ces régions, allant du climat tropical à l'équateur au climat polaire dans les extrêmes nord et sud.
Cette exposition alterne lorsque la Terre tourne sur son orbite.
L'eau couvre 71% de la surface de la Terre.
Les petites régions des océans sont appelées mers, golfes, baies et autres noms.
On ne sait pas si les lacs de Titan sont alimentés par des rivières, bien que la surface de Titan soit creusée par de nombreux lits de rivières.
Une grande variété de plans d'eau artificiels sont classés comme étangs, y compris les jardins d'eau conçus pour l'ornementation esthétique, les étangs à poissons conçus pour l'élevage commercial de poissons et les étangs solaires conçus pour stocker l'énergie thermique.
Les petites rivières peuvent également être appelées par plusieurs autres noms, notamment ruisseau, ruisseau, ruisseau, ruisseau et rigole; il n'y a pas de règle générale qui définit ce qu'on peut appeler une rivière.
La structure et la composition sont déterminées par divers facteurs environnementaux qui sont interdépendants.
Au cœur du concept d'écosystème se trouve l'idée que les organismes vivants interagissent avec tous les autres éléments de leur environnement local.
On peut aussi dire que la vie est simplement l'état caractéristique des organismes.
Cependant, toutes les définitions de la vie ne considèrent pas toutes ces propriétés comme essentielles.
Du point de vue géophysiologique le plus large, la biosphère est le système écologique global intégrant tous les êtres vivants et leurs relations, y compris leur interaction avec les éléments de la lithosphère (roches), de l'hydrosphère (eau) et de l'atmosphère (air).
Plus de 2 millions d'espèces végétales et animales ont été identifiées à ce jour, et les estimations du nombre réel d'espèces existantes vont de plusieurs millions à bien plus de 50 millions.
Les espèces incapables de s'adapter à l'environnement changeant et à la concurrence d'autres formes de vie ont disparu.
Lorsque les formes de base de la vie végétale ont développé le processus de photosynthèse, l'énergie du soleil a pu être récoltée pour créer des conditions permettant des formes de vie plus complexes.
Les micro-organismes sont des organismes unicellulaires qui sont généralement microscopiques et plus petits que ce que l'œil humain peut voir.
Leur reproduction est à la fois rapide et abondante.
Depuis lors, il est devenu clair que les Plantae telles que définies à l'origine comprenaient plusieurs groupes non apparentés, et les champignons et plusieurs groupes d'algues ont été déplacés vers de nouveaux royaumes.
Parmi les nombreuses façons de classer les plantes figurent les flores régionales, qui, selon l'objet de l'étude, peuvent également inclure des flores fossiles, vestiges de la vie végétale d'une époque antérieure.
Certains types de «flore indigène» ont en fait été introduits il y a des siècles par des personnes migrant d'une région ou d'un continent à un autre et sont devenus partie intégrante de la flore indigène ou naturelle de l'endroit où ils ont été introduits.
Les animaux en tant que catégorie ont plusieurs caractéristiques qui les distinguent généralement des autres êtres vivants.
Ils se distinguent également des plantes, des algues et des champignons par l'absence de parois cellulaires.
Il y a aussi généralement une chambre digestive interne.
Une étude de 2020 publiée dans Nature a révélé que la masse anthropique (matériaux fabriqués par l'homme) l'emporte sur toute la biomasse vivante sur terre, le plastique dépassant à lui seul la masse de tous les animaux terrestres et marins combinés.
Malgré ces progrès, le destin de la civilisation humaine reste cependant étroitement lié aux changements de l'environnement.
Les humains ont contribué à l'extinction de nombreuses plantes et animaux, avec environ 1 million d'espèces menacées d'extinction en quelques décennies.
Cela fausse les prix du marché des ressources naturelles et conduit en même temps à un sous-investissement dans nos actifs naturels.
Les gouvernements n'ont pas empêché ces externalités économiques.
Certaines activités, telles que la chasse et la pêche, sont utilisées à la fois pour la subsistance et les loisirs, souvent par des personnes différentes.
Le fait que la nature ait été dépeinte et célébrée par tant d'art, de photographie, de poésie et d'autres littératures montre la force avec laquelle de nombreuses personnes associent la nature et la beauté.
La nature et la nature sauvage ont été des sujets importants à diverses époques de l'histoire du monde.
Bien que les merveilles naturelles soient célébrées dans les Psaumes et le Livre de Job, les représentations de la nature sauvage dans l'art sont devenues plus répandues dans les années 1800, en particulier dans les œuvres du mouvement romantique.
Pour cette raison, la science la plus fondamentale est généralement comprise comme étant la «physique» - dont le nom est encore reconnaissable comme signifiant qu'il s'agit de «l'étude de la nature».
On pense maintenant que les composants visibles de l'univers ne composent que 4,9% de la masse totale.
Le comportement de la matière et de l'énergie dans tout l'univers observable semble suivre des lois physiques bien définies.
Il n'y a pas de frontière distincte entre l'atmosphère terrestre et l'espace, car l'atmosphère s'atténue progressivement avec l'augmentation de l'altitude.
Il y a aussi du gaz, du plasma et de la poussière, et de petits météores.
Bien que la Terre soit le seul corps du système solaire connu pour abriter la vie, des preuves suggèrent que dans un passé lointain, la planète Mars possédait des masses d'eau liquide à la surface.
Si la vie existe sur Mars, il est fort probable qu'elle se trouve sous terre, là où de l'eau liquide peut encore exister.
L'observation est l'acquisition active d'informations à partir d'une source primaire.
L'utilisation de la mesure s'est développée pour permettre l'enregistrement et la comparaison d'observations faites à des moments et à des endroits différents, par des personnes différentes.
Dans la mesure, le nombre d'unités standard qui est égal à l'observation est compté.
Des instruments scientifiques ont été développés pour aider les capacités humaines d'observation, tels que des balances, des horloges, des télescopes, des microscopes, des thermomètres, des caméras et des magnétophones, et traduisent également sous forme perceptible des événements qui ne sont pas observables par les sens, tels que des colorants indicateurs, des voltmètres. , spectromètres, caméras infrarouges, oscilloscopes, interféromètres, compteurs Geiger et récepteurs radio.
Par exemple, il n'est normalement pas possible de vérifier la pression d'air dans un pneu d'automobile sans laisser sortir une partie de l'air, modifiant ainsi la pression.
Par exemple, dans le paradoxe des jumeaux, un jumeau part en voyage à la vitesse de la lumière et rentre à la maison plus jeune que le jumeau qui est resté à la maison.
Mécanique quantique : En mécanique quantique, qui traite du comportement de très petits objets, il n'est pas possible d'observer un système sans changer le système, et « l'observateur » doit être considéré comme faisant partie du système observé.
La perception humaine se produit par un processus d'abstraction complexe et inconscient, dans lequel certains détails des données sensorielles entrantes sont remarqués et mémorisés, et le reste oublié.
Plus tard, lorsque les événements sont rappelés, les trous de mémoire peuvent même être comblés par des données «plausibles» que l'esprit invente pour s'adapter au modèle; c'est ce qu'on appelle la mémoire reconstructive.
En psychologie, cela s'appelle le biais de confirmation.
Par exemple, supposons qu'un observateur voit un parent battre son enfant ; et par conséquent peut observer qu'une telle action est bonne ou mauvaise.
La recherche est « un travail créatif et systématique entrepris pour accroître le stock de connaissances ».
Pour tester la validité des instruments, des procédures ou des expériences, la recherche peut reproduire des éléments de projets antérieurs ou du projet dans son ensemble.
Ce matériel est d'un caractère de source primaire.
Dans le travail expérimental, cela implique généralement une observation directe ou indirecte du ou des sujets de recherche, par exemple, en laboratoire ou sur le terrain, documente la méthodologie, les résultats et les conclusions d'une expérience ou d'un ensemble d'expériences, ou offre une nouvelle interprétation. des résultats précédents.
Le degré d'originalité de la recherche est l'un des critères majeurs pour les articles à publier dans des revues académiques et généralement établi par le biais d'un examen par les pairs.
Cette recherche fournit des informations scientifiques et des théories pour l'explication de la nature et des propriétés du monde.
La recherche scientifique peut être subdivisée en différentes classifications en fonction de leurs disciplines académiques et d'application.
Les spécialistes des sciences humaines ne recherchent généralement pas la réponse correcte ultime à une question, mais explorent plutôt les problèmes et les détails qui l'entourent.
Les historiens utilisent des sources primaires et d'autres preuves pour enquêter systématiquement sur un sujet, puis pour écrire des histoires sous la forme de récits du passé.
La recherche devra être justifiée en liant son importance aux connaissances déjà existantes sur le sujet.
Généralement, une hypothèse est utilisée pour faire des prédictions qui peuvent être testées en observant le résultat d'une expérience.
Ce langage prudent est utilisé parce que les chercheurs reconnaissent que des hypothèses alternatives peuvent également être cohérentes avec les observations.
Comme la précision de l'observation s'améliore avec le temps, l'hypothèse peut ne plus fournir une prédiction précise.
La recherche artistique a été définie par l'École de danse et de cirque (Dans och Cirkushögskolan, DOCH) de Stockholm de la manière suivante : "La recherche artistique consiste à enquêter et à tester dans le but d'acquérir des connaissances dans et pour nos disciplines artistiques.
La recherche artistique vise à améliorer les connaissances et la compréhension grâce à la présentation des arts.
Selon l'artiste Hakan Topal, dans la recherche artistique, "peut-être plus que d'autres disciplines, l'intuition est utilisée comme méthode pour identifier un large éventail de modalités productives nouvelles et inattendues".
La recherche de fond pourrait inclure, par exemple, une recherche géographique ou procédurale.
L'examen de la littérature identifie des défauts ou des lacunes dans les recherches antérieures qui justifient l'étude.
La question de recherche peut être parallèle à l'hypothèse.
Le ou les chercheurs analysent et interprètent ensuite les données via une variété de méthodes statistiques, s'engageant dans ce que l'on appelle la recherche empirique.
Cependant, certains chercheurs préconisent l'approche inverse : commencer par articuler les résultats et en discuter, puis remonter jusqu'à l'identification d'un problème de recherche qui émerge dans les résultats et l'examen de la littérature.
La recherche qualitative est souvent utilisée comme méthode de recherche exploratoire comme base pour des hypothèses de recherche quantitative ultérieures.
La recherche quantitative est liée à la position philosophique et théorique du positivisme.
La recherche quantitative consiste à tester des hypothèses dérivées de la théorie ou à pouvoir estimer la taille d'un phénomène d'intérêt.
Si l'intention est de généraliser les participants à la recherche à une population plus large, le chercheur utilisera un échantillonnage probabiliste pour sélectionner les participants.
Les données secondaires sont des données qui existent déjà, telles que les données de recensement, qui peuvent être réutilisées pour la recherche.
Cette méthode présente des avantages que l'utilisation d'une seule méthode ne peut pas offrir.
La recherche non empirique n'est pas une alternative absolue à la recherche empirique car elles peuvent être utilisées ensemble pour renforcer une approche de recherche.
La gestion de l'éthique de la recherche est incohérente d'un pays à l'autre et il n'y a pas d'approche universellement acceptée sur la façon dont elle devrait être abordée.
Quelle que soit l'approche, l'application de la théorie éthique à des sujets controversés spécifiques est connue sous le nom d'éthique appliquée et l'éthique de la recherche peut être considérée comme une forme d'éthique appliquée parce que la théorie éthique est appliquée dans des scénarios de recherche du monde réel.
L'éthique de la recherche est surtout développée en tant que concept dans la recherche médicale, le code le plus notable étant la Déclaration d'Helsinki de 1964.
La méta-recherche se préoccupe de la détection des biais, des défauts méthodologiques et d'autres erreurs et inefficacités.
Les chercheurs de la périphérie sont confrontés aux défis de l'exclusion et du linguicisme dans la recherche et la publication universitaire.
Pour la politique comparée, les pays occidentaux sont surreprésentés dans les études portant sur un seul pays, l'accent étant mis sur l'Europe occidentale, le Canada, l'Australie et la Nouvelle-Zélande.
Les études à portée étroite peuvent entraîner un manque de généralisabilité, ce qui signifie que les résultats peuvent ne pas être applicables à d'autres populations ou régions.
Habituellement, le processus d'examen par les pairs implique des experts dans le même domaine qui sont consultés par les éditeurs pour donner un avis sur les travaux savants produits par un de leurs collègues d'un point de vue impartial et impartial, et cela se fait généralement gratuitement.
Par exemple, la plupart des communautés autochtones considèrent que l'accès à certaines informations propres au groupe doit être déterminé par des relations.
Le système varie considérablement d'un domaine à l'autre et change constamment, bien que souvent lentement.
Ces formes de recherche peuvent être trouvées dans des bases de données explicitement pour les thèses et les mémoires.
Les types de publications qui sont acceptées comme contributions de connaissances ou de recherche varient considérablement d'un domaine à l'autre, du format imprimé au format électronique.
Les modèles commerciaux sont différents dans l'environnement électronique.
De nombreux chercheurs chevronnés (tels que les chefs de groupe) passent une grande partie de leur temps à demander des subventions pour des fonds de recherche.
La méthode scientifique est une méthode empirique d'acquisition de connaissances qui a caractérisé le développement de la science depuis au moins le XVIIe siècle (avec des praticiens notables au cours des siècles précédents).
Ce sont là des principes de la méthode scientifique, par opposition à une série définitive d'étapes applicables à toutes les entreprises scientifiques.
Une hypothèse est une conjecture, basée sur les connaissances obtenues en cherchant des réponses à la question.
Il y a cependant des difficultés dans un énoncé de méthode fondé sur une formule.
Le terme "méthode scientifique" est apparu au 19ème siècle, lorsqu'un développement institutionnel significatif de la science avait lieu et que des terminologies établissant des frontières claires entre la science et la non-science, telles que "scientifique" et "pseudoscience", sont apparues.
Gauch 2003 et Tow 2010 ne sont pas d'accord avec l'affirmation de Feyerabend ; résolveurs de problèmes, et les chercheurs doivent être prudents avec leurs ressources pendant leur enquête.
Les philosophes Robert Nola et Howard Sankey, dans leur livre de 2007 Theories of Scientific Method, ont déclaré que les débats sur la méthode scientifique se poursuivaient et ont soutenu que Feyerabend, malgré le titre de Against Method, acceptait certaines règles de méthode et tentait de justifier ces règles avec une méta. méthodologie.
L'élément omniprésent dans la méthode scientifique est l'empirisme.
La méthode scientifique contredit les affirmations selon lesquelles la révélation, le dogme politique ou religieux, les appels à la tradition, les croyances courantes, le bon sens ou les théories courantes constituent le seul moyen possible de démontrer la vérité.
À partir du XVIe siècle, des expériences ont été préconisées par Francis Bacon et réalisées par Giambattista della Porta, Johannes Kepler et Galileo Galilei.
Comme dans d'autres domaines de recherche, la science (par le biais de la méthode scientifique) peut s'appuyer sur des connaissances antérieures et développer une compréhension plus sophistiquée de ses sujets d'étude au fil du temps.
On peut voir que ce modèle est à la base de la révolution scientifique. : "
Une conjecture pourrait être qu'un nouveau médicament guérira la maladie chez certaines personnes de cette population, comme dans un essai clinique du médicament.
Ces prédictions sont des attentes pour les résultats des tests.
La différence entre attendu et réel indique quelle hypothèse explique le mieux les données résultant de l'expérience.
Selon la complexité de l'expérience, une itération du processus peut être nécessaire pour recueillir suffisamment de preuves pour répondre à la question avec confiance, ou pour élaborer d'autres réponses à des questions très spécifiques, afin de répondre à une seule question plus large.
Modèles de diffraction des rayons X de l'ADN par Florence Bell dans son doctorat. thèse (1939) étaient similaires (mais pas aussi bonnes que) "photo 51", mais cette recherche a été interrompue par les événements de la Seconde Guerre mondiale.
Juin 1952 - Watson avait réussi à obtenir des images radiographiques de TMV montrant un motif de diffraction compatible avec la transformation d'une hélice. : "
Cette prédiction était une construction mathématique, complètement indépendante du problème biologique en question.
L'ADN n'est pas une hélice."
Par exemple, le nombre de brins dans l'épine dorsale de l'hélice (Crick a suspecté 2 brins, mais a averti Watson d'examiner cela de manière plus critique), l'emplacement des paires de bases (à l'intérieur de l'épine dorsale ou à l'extérieur de l'épine dorsale), etc.
Mais Wilkins n'accepte de le faire qu'après le départ de Franklin. :"
Lui et Crick ont ensuite produit leur modèle, en utilisant ces informations ainsi que les informations précédemment connues sur la composition de l'ADN, en particulier les règles d'appariement de bases de Chargaff :
Pour des résultats significatifs ou surprenants, d'autres scientifiques peuvent également tenter de reproduire les résultats pour eux-mêmes, en particulier si ces résultats seraient importants pour leur propre travail.
L'examen par les pairs ne certifie pas l'exactitude des résultats, mais seulement que, de l'avis de l'examinateur, les expériences elles-mêmes étaient valables (sur la base de la description fournie par l'expérimentateur).
Ces éléments méthodologiques et l'organisation des procédures tendent à être plus caractéristiques des sciences expérimentales que des sciences sociales.
Les éléments ci-dessus sont souvent enseignés dans le système éducatif en tant que "méthode scientifique".
Par exemple, quand Einstein a développé les théories restreinte et générale de la relativité, il n'a en aucune façon réfuté ou écarté les Principia de Newton.
La collecte systématique et minutieuse de mesures ou de comptages de quantités pertinentes est souvent la différence essentielle entre les pseudo-sciences, telles que l'alchimie, et les sciences, telles que la chimie ou la biologie.
Les incertitudes peuvent également être calculées en tenant compte des incertitudes des quantités sous-jacentes individuelles utilisées.
La définition opérationnelle d'une chose repose souvent sur des comparaisons avec des normes : la définition opérationnelle de la "masse" repose finalement sur l'utilisation d'un artefact, tel qu'un kilogramme particulier de platine-iridium conservé dans un laboratoire en France.
Les quantités scientifiques sont souvent caractérisées par leurs unités de mesure qui peuvent ensuite être décrites en termes d'unités physiques conventionnelles lors de la communication du travail.
Il a fallu des milliers d'années de mesures, de la part des astronomes chaldéens, indiens, persans, grecs, arabes et européens, pour enregistrer pleinement le mouvement de la planète Terre.
La différence observée pour la précession de Mercure entre la théorie newtonienne et l'observation était l'une des choses qui sont venues à Albert Einstein comme un test précoce possible de sa théorie de la relativité générale.
Les scientifiques sont libres d'utiliser toutes les ressources dont ils disposent - leur propre créativité, des idées d'autres domaines, le raisonnement inductif, l'inférence bayésienne, etc. - pour étudier les explications possibles d'un phénomène étudié.
Les scientifiques utilisent souvent ces termes pour désigner une théorie qui suit les faits connus mais qui est néanmoins relativement simple et facile à manier.
Il est essentiel que le résultat du test d'une telle prédiction soit actuellement inconnu.
Si les prédictions ne sont pas accessibles par l'observation ou l'expérience, l'hypothèse n'est pas encore testable et restera donc dans cette mesure non scientifique au sens strict.
Cela impliquait que le diagramme de diffraction des rayons X de l'ADN serait «en forme de x».
Parfois, les expériences sont menées de manière incorrecte ou ne sont pas très bien conçues par rapport à une expérience cruciale.
Cette technique utilise le contraste entre plusieurs échantillons, ou observations, ou populations, dans des conditions différentes, pour voir ce qui varie ou ce qui reste le même.
L'analyse factorielle est une technique pour découvrir le facteur important dans un effet.
Même prendre un avion de New York à Paris est une expérience qui teste les hypothèses aérodynamiques utilisées pour la construction de l'avion.
Franklin a immédiatement repéré les défauts qui concernaient la teneur en eau.
Le fait de ne pas développer une hypothèse intéressante peut amener un scientifique à redéfinir le sujet à l'étude.
D'autres scientifiques peuvent commencer leurs propres recherches et entrer dans le processus à n'importe quelle étape.
Fondamentalement, les résultats expérimentaux et théoriques doivent être reproduits par d'autres au sein de la communauté scientifique.
Plus une explication est efficace pour faire des prédictions, plus elle peut être utile fréquemment et plus elle continuera à expliquer un ensemble de preuves mieux que ses alternatives.
Les modèles scientifiques varient dans la mesure dans laquelle ils ont été testés expérimentalement et pendant combien de temps, et dans leur acceptation par la communauté scientifique.
Si une telle preuve est trouvée, une nouvelle théorie peut être proposée, ou (plus communément) on constate que des modifications à la théorie précédente sont suffisantes pour expliquer la nouvelle preuve.
Par exemple, les lois de Newton expliquaient presque parfaitement des milliers d'années d'observations scientifiques des planètes.
Étant donné que les nouvelles théories pourraient être plus complètes que celles qui les ont précédées, et donc être capables d'expliquer plus que les précédentes, les théories suivantes pourraient être en mesure de répondre à un niveau plus élevé en expliquant un plus grand nombre d'observations que leurs prédécesseurs.
Une fois qu'un système d'opinions structurellement complet et fermé composé de nombreux détails et relations a été formé, il offre une résistance durable à tout ce qui le contredit".
Ses succès peuvent briller mais ont tendance à être transitoires.
La méthode de l'a priori - qui promeut moins brutalement le conformisme mais nourrit les opinions comme quelque chose comme des goûts, surgissant dans la conversation et les comparaisons de perspectives en termes de "ce qui est agréable à la raison".
C'est une destination aussi éloignée ou proche que la vérité elle-même pour vous ou moi ou la communauté finie donnée.
De l'abduction, Peirce distingue l'induction comme inférer, sur la base de tests, la proportion de vérité dans l'hypothèse.
Le plus souvent, même un esprit bien préparé devine mal.
Peirce, Charles S. (1902), application Carnegie, voir MS L75.329330, extrait du brouillon D du mémoire 27 : "Par conséquent, découvrir, c'est simplement accélérer un événement qui se produirait tôt ou tard, si nous ne nous étions pas donné la peine de faire la découverte.
Par conséquent, la conduite de l'enlèvement, qui est principalement une question d'heuretique et est la première question d'heuretique, doit être régie par des considérations économiques."
L'hypothèse, étant peu sûre, doit avoir des implications pratiques conduisant au moins à des tests mentaux et, en science, se prêtant à des tests scientifiques.
Einstein, Albert (1936, 1956) On peut dire "le mystère éternel du monde est sa compréhensibilité".
Ces hypothèses du naturalisme méthodologique forment une base sur laquelle la science peut être fondée.
Ses observations de la pratique scientifique sont essentiellement sociologiques et ne parlent pas de la façon dont la science est ou peut être pratiquée à d'autres époques et dans d'autres cultures.
Il ouvre le chapitre 1 par une discussion sur les corps de Golgi et leur rejet initial en tant qu'artefact de la technique de coloration, et une discussion sur Brahe et Kepler observant l'aube et voyant un lever de soleil "différent" malgré le même phénomène physiologique.
En substance, il dit que pour toute méthode ou norme scientifique spécifique, on peut trouver un épisode historique où la violer a contribué au progrès de la science.
Les critiques postmodernes de la science ont elles-mêmes fait l'objet d'intenses controverses.
Les modèles, à la fois en sciences et en mathématiques, doivent être cohérents en interne et doivent également être falsifiables (capables d'être réfutés).
Par exemple, le concept technique de temps est apparu dans la science, et l'intemporalité était la marque d'un sujet mathématique.
L'article d'Eugene Wigner, L'efficacité déraisonnable des mathématiques dans les sciences naturelles, est un compte rendu très connu de la question d'un physicien lauréat du prix Nobel.
Dans Preuves et réfutations, Lakatos a donné plusieurs règles de base pour trouver des preuves et des contre-exemples aux conjectures.
Cela peut expliquer pourquoi les scientifiques expriment si souvent qu'ils ont eu de la chance.
Mahwah, New Jersey : Lawrence Erlbaum Associates.
C'est ce que Nassim Nicholas Taleb appelle « l'anti-fragilité » ; alors que certains systèmes d'investigation sont fragiles face à l'erreur humaine, aux préjugés humains et au hasard, la méthode scientifique est plus que résistante ou dure - elle bénéficie en fait d'un tel hasard à bien des égards (elle est anti-fragile).
Ces résultats inattendus conduisent les chercheurs à essayer de corriger ce qu'ils pensent être une erreur dans leur méthode.
Une théorie scientifique est une explication d'un aspect du monde naturel et de l'univers qui a été testé et vérifié à plusieurs reprises conformément à la méthode scientifique, en utilisant des protocoles acceptés d'observation, de mesure et d'évaluation des résultats.
Les théories scientifiques établies ont résisté à un examen rigoureux et incarnent des connaissances scientifiques.
Stephen Jay Gould a écrit que "... les faits et les théories sont des choses différentes, et non des échelons dans une hiérarchie de certitude croissante.
Le sens du terme théorie scientifique (souvent contracté à la théorie pour la brièveté) tel qu'il est utilisé dans les disciplines scientifiques est significativement différent de l'usage vernaculaire commun de la théorie.
Dans le langage courant, la théorie peut impliquer une explication qui représente une supposition non fondée et spéculative, alors qu'en science, elle décrit une explication qui a été testée et largement acceptée comme valide.
Certaines théories sont si bien établies qu'il est peu probable qu'elles soient jamais fondamentalement modifiées (par exemple, les théories scientifiques telles que l'évolution, la théorie héliocentrique, la théorie cellulaire, la théorie de la tectonique des plaques, la théorie des germes de la maladie, etc.).
Les théories scientifiques sont vérifiables et font des prédictions falsifiables.
La caractéristique déterminante de toutes les connaissances scientifiques, y compris les théories, est la capacité de faire des prédictions falsifiables ou vérifiables.
Il est bien étayé par de nombreux éléments de preuve indépendants, plutôt que par une seule fondation.
La théorie de l'évolution biologique est plus qu'"une simple théorie".
Cela fournit des preuves pour ou contre l'hypothèse.
Cela peut prendre de nombreuses années, car il peut être difficile ou compliqué de rassembler suffisamment de preuves.
La force des preuves est évaluée par la communauté scientifique, et les expériences les plus importantes auront été reproduites par plusieurs groupes indépendants.
En chimie, il existe de nombreuses théories acide-base fournissant des explications très divergentes sur la nature sous-jacente des composés acides et basiques, mais elles sont très utiles pour prédire leur comportement chimique.
L'acceptation d'une théorie ne nécessite pas que toutes ses prédictions majeures soient testées, si elle est déjà étayée par des preuves suffisamment solides.
Les solutions peuvent nécessiter des modifications mineures ou majeures de la théorie, voire aucune si une explication satisfaisante est trouvée dans le cadre existant de la théorie.
Si les modifications de la théorie ou d'autres explications semblent insuffisantes pour rendre compte des nouveaux résultats, une nouvelle théorie peut être nécessaire.
En effet, il s'agit toujours de la meilleure explication disponible pour de nombreux autres phénomènes, comme en témoigne son pouvoir prédictif dans d'autres contextes.
Après les changements, la théorie acceptée expliquera plus de phénomènes et aura un plus grand pouvoir prédictif (si ce n'était pas le cas, les changements ne seraient pas adoptés) ; cette nouvelle explication sera alors ouverte à d'autres remplacements ou modifications.
Par exemple, l'électricité et le bamnétisme sont maintenant connus pour être deux aspects du même phénomène, appelé électrobamnétisme.
Cela a été résolu par la découverte de la fusion nucléaire, la principale source d'énergie du Soleil.
En omettant de la relativité restreinte l'éther luminifère, Einstein a déclaré que la dilatation du temps et la contraction de la longueur mesurées dans un objet en mouvement relatif sont inertielles, c'est-à-dire que l'objet présente une vitesse constante, qui est la vitesse avec la direction, lorsqu'elle est mesurée par son observateur.
Einstein a cherché à généraliser le principe d'invariance à tous les référentiels, qu'ils soient inertiels ou en accélération.
Même l'énergie sans masse exerce un mouvement gravitationnel sur les objets locaux en "incurvant" la "surface" géométrique de l'espace-temps 4D.
Cependant, les lois scientifiques sont des comptes rendus descriptifs de la façon dont la nature se comportera dans certaines conditions.
Une idée fausse courante est que les théories scientifiques sont des idées rudimentaires qui finiront par devenir des lois scientifiques lorsque suffisamment de données et de preuves auront été accumulées.
Les théories et les lois pourraient potentiellement être falsifiées par des preuves contraires.
La logique du premier ordre est un exemple de langage formel.
Les phénomènes expliqués par les théories, s'ils ne pouvaient pas être directement observés par les sens (par exemple, les atomes et les ondes radio), étaient traités comme des concepts théoriques.
L'expression "la vision reçue des théories" est utilisée pour décrire cette approche.
On peut utiliser le langage pour décrire un modèle ; cependant, la théorie est le modèle (ou un ensemble de modèles similaires), et non la description du modèle.
Les paramètres du modèle, par exemple la loi de la gravitation de Newton, déterminent comment les positions et les vitesses changent avec le temps.
Le mot "sémantique" fait référence à la manière dont un modèle représente le monde réel.
La pratique de l'ingénierie fait une distinction entre les « modèles mathématiques » et les « modèles physiques » ; le coût de fabrication d'un modèle physique peut être minimisé en créant d'abord un modèle mathématique à l'aide d'un progiciel informatique, tel qu'un outil de conception assistée par ordinateur.
Certaines hypothèses sont nécessaires pour toutes les affirmations empiriques (par exemple l'hypothèse que la réalité existe).
Cela peut être aussi simple que d'observer que la théorie fait des prédictions précises, ce qui est la preuve que toutes les hypothèses faites au départ sont correctes ou approximativement correctes dans les conditions testées.
La théorie fait des prédictions précises lorsque l'hypothèse est valide et ne fait pas de prédictions précises lorsque l'hypothèse n'est pas valide.
L'Oxford English Dictionary (OED) et le Wiktionnaire en ligne indiquent sa source latine comme assumere (« accepter, prendre pour soi, adopter, usurper »), qui est une conjonction de ad- (« vers, vers, à ») et sumere ( prendre).
Le terme était à l'origine employé dans des contextes religieux comme dans "recevoir au ciel", en particulier "la réception de la Vierge Marie au ciel, avec un corps préservé de la corruption", (1297 CE) mais il était aussi simplement utilisé pour désigner " recevoir en association » ou « adopter en société ».
Les confirmations ne devraient compter que si elles sont le résultat de prédictions risquées ; c'est-à-dire si, non éclairés par la théorie en question, nous nous attendions à un événement incompatible avec la théorie, un événement qui aurait réfuté la théorie.
Une théorie qui n'est réfutable par aucun événement concevable est non scientifique.
Certaines théories véritablement testables, lorsqu'elles se révèlent fausses, peuvent encore être soutenues par leurs admirateurs - par exemple en introduisant post hoc (après coup) une hypothèse ou hypothèse auxiliaire, ou en réinterprétant la théorie post hoc de telle manière qu'elle échappe réfutation.
Popper a résumé ces déclarations en disant que le critère central du statut scientifique d'une théorie est sa « falsifiabilité, ou réfutabilité, ou testabilité ».
Plusieurs philosophes et historiens des sciences ont cependant soutenu que la définition de Popper de la théorie comme un ensemble d'énoncés falsifiables est erronée parce que, comme l'a souligné Philip Kitcher, si l'on adoptait une vision strictement popperienne de la "théorie", les observations d'Uranus lors de la première découverte en 1781 aurait "falsifié" la mécanique céleste de Newton.
Fécondité : « Une grande théorie scientifique, comme celle de Newton, ouvre de nouveaux champs de recherche….
À tout moment, il soulève plus de questions qu'il ne peut actuellement répondre.
Comme d'autres définitions de théories, y compris celle de Popper, Kitcher précise qu'une théorie doit inclure des déclarations qui ont des conséquences observationnelles.
Elle peut être présentée sur le papier comme un système de règles, et c'est d'autant plus une théorie qu'elle peut être rédigée en ces termes.
Les aspects mathématiques spécifiques de la théorie électrobamnétique classique sont appelés «lois de l'électrobamnétisme», reflétant le niveau de preuves cohérentes et reproductibles qui les soutiennent.
Un exemple de ce dernier pourrait être la force de réaction de rayonnement.
Un scientifique est une personne qui mène des recherches scientifiques pour faire progresser les connaissances dans un domaine d'intérêt.
Les scientifiques de différentes époques (et avant eux, les philosophes de la nature, les mathématiciens, les historiens de la nature, les théologiens de la nature, les ingénieurs et d'autres qui ont contribué au développement de la science) ont eu des places très différentes dans la société, et les normes sociales, les valeurs éthiques et les valeurs épistémiques les vertus associées aux scientifiques – et attendues d'eux – ont également changé au fil du temps.
De nombreux proto-scientifiques de l'âge d'or islamique sont considérés comme des polymathes, en partie à cause du manque de tout ce qui correspond aux disciplines scientifiques modernes.
Les propositions auxquelles on arrive par des moyens purement logiques sont complètement vides de réalité.
Descartes n'était pas seulement un pionnier de la géométrie analytique, mais a formulé une théorie de la mécanique et des idées avancées sur les origines du mouvement et de la perception des animaux.
Il a fourni une formulation complète de la mécanique classique et a étudié la lumière et l'optique.
Il a découvert qu'une charge appliquée à la moelle épinière d'une grenouille pouvait générer des spasmes musculaires dans tout son corps.
Lazzaro Spallanzani est l'une des figures les plus influentes de la physiologie expérimentale et des sciences naturelles.
Cependant, il n'y a pas de processus formel pour déterminer qui est un scientifique et qui n'est pas un scientifique.
Un peu plus de la moitié des répondants souhaitaient poursuivre une carrière dans le milieu universitaire, avec de plus petites proportions espérant travailler dans l'industrie, le gouvernement et les environnements à but non lucratif.
Ils manifestent une forte curiosité pour la réalité.
Certains scientifiques ont le désir d'appliquer les connaissances scientifiques au profit de la santé des personnes, des nations, du monde, de la nature ou des industries (scientifique universitaire et scientifique industriel).
Ceux-ci incluent la cosmologie et la biologie, en particulier la biologie moléculaire et le projet du génome humain.
Ce chiffre comprenait deux fois plus d'hommes que de femmes.
Les phénomènes pertinents comprennent les explosions de supernova, les sursauts gamma, les quasars, les blazars, les pulsars et le rayonnement de fond cosmique des micro-ondes.
L'astronomie est l'une des plus anciennes sciences naturelles.
Dans le passé, l'astronomie comprenait des disciplines aussi diverses que l'astrométrie, la navigation céleste, l'astronomie d'observation et la fabrication de calendriers.
L'astronomie observationnelle se concentre sur l'acquisition de données à partir d'observations d'objets astronomiques.
Ces deux domaines se complètent.
D'après des définitions strictes du dictionnaire, «l'astronomie» fait référence à «l'étude des objets et de la matière en dehors de l'atmosphère terrestre et de leurs propriétés physiques et chimiques», tandis que «l'astrophysique» fait référence à la branche de l'astronomie traitant «du comportement, des propriétés physiques, et les processus dynamiques des objets et phénomènes célestes".
Certains domaines, comme l'astrométrie, relèvent purement de l'astronomie plutôt que de l'astrophysique.
À partir de ces observations, les premières idées sur les mouvements des planètes ont été formées et la nature du Soleil, de la Lune et de la Terre dans l'Univers a été explorée philosophiquement.
Un développement précoce particulièrement important a été le début de l'astronomie mathématique et scientifique, qui a commencé chez les Babyloniens, qui ont jeté les bases des traditions astronomiques ultérieures qui se sont développées dans de nombreuses autres civilisations.
L'astronomie grecque se caractérise dès le départ par la recherche d'une explication rationnelle et physique des phénomènes célestes.
Hipparque a également créé un catalogue complet de 1020 étoiles, et la plupart des constellations de l'hémisphère nord dérivent de l'astronomie grecque.
Georg von Peuerbach (1423–1461) et Regiomontanus (1436–1476) ont contribué à faire des progrès astronomiques un instrument du développement par Copernic du modèle héliocentrique des décennies plus tard.
En 964, la galaxie d'Andromède, la plus grande galaxie du groupe local, a été décrite par l'astronome musulman persan Abd al-Rahman al-Sufi dans son Livre des étoiles fixes.
Les astronomes de cette époque ont introduit de nombreux noms arabes désormais utilisés pour les étoiles individuelles.
L'historien songhaï Mahmud Kati a documenté une pluie de météores en août 1583.
Kepler a été le premier à concevoir un système décrivant correctement les détails du mouvement des planètes autour du Soleil.
L'astronome anglais John Flamsteed a catalogué plus de 3000 étoiles. Des catalogues d'étoiles plus complets ont été produits par Nicolas Louis de Lacaille.
Ce travail a été affiné par Joseph-Louis Lagrange et Pierre Simon Laplace, permettant d'estimer les masses des planètes et des lunes à partir de leurs perturbations.
Il a été prouvé que les étoiles étaient similaires au Soleil de la Terre, mais avec une large gamme de températures, de masses et de tailles.
L'astronomie théorique a conduit à des spéculations sur l'existence d'objets tels que les trous noirs et les étoiles à neutrons, qui ont été utilisés pour expliquer des phénomènes observés tels que les quasars, les pulsars, les blazars et les radiogalaxies.
L'astronomie observationnelle peut être classée en fonction de la région correspondante du spectre électrobamnétique sur laquelle les observations sont faites.
Bien que certaines ondes radio soient émises directement par des objets astronomiques, un produit d'émission thermique, la majeure partie de l'émission radio observée est le résultat du rayonnement synchrotron, qui est produit lorsque les électrons orbitent autour des champs bamnétiques.
Les observations du Wide-field Infrared Survey Explorer (WISE) ont été particulièrement efficaces pour dévoiler de nombreuses protoétoiles galactiques et leurs amas d'étoiles hôtes.
Les ibames d'observations étaient à l'origine dessinées à la main.
L'astronomie ultraviolette est la mieux adaptée à l'étude du rayonnement thermique et des raies d'émission spectrales des étoiles bleues chaudes (étoiles OB) qui sont très brillantes dans cette bande d'ondes.
Les rayons gamma peuvent être observés directement par des satellites tels que le Compton Gamma Ray Observatory ou par des télescopes spécialisés appelés télescopes atmosphériques Cherenkov.
L'astronomie des ondes gravitationnelles est un domaine émergent de l'astronomie qui utilise des détecteurs d'ondes gravitationnelles pour collecter des données d'observation sur des objets massifs éloignés.
Historiquement, la connaissance précise des positions du Soleil, de la Lune, des planètes et des étoiles a été essentielle dans la navigation céleste (l'utilisation d'objets célestes pour guider la navigation) et dans la fabrication des calendriers.
La mesure de la parallaxe stellaire des étoiles proches fournit une ligne de base fondamentale dans l'échelle de distance cosmique utilisée pour mesurer l'échelle de l'Univers.
Les modèles analytiques d'un processus sont meilleurs pour donner un aperçu plus large du cœur de ce qui se passe.
L'observation d'un phénomène prédit par un modèle permet aux astronomes de choisir entre plusieurs modèles alternatifs ou contradictoires celui qui est le mieux à même de décrire le phénomène.
Dans certains cas, une grande quantité de données incohérentes dans le temps peut conduire à l'abandon total d'un modèle.
Parce que l'astrophysique est un sujet très vaste, les astrophysiciens appliquent généralement de nombreuses disciplines de la physique, notamment la mécanique, l'électrobamnétisme, la mécanique statistique, la thermodynamique, la mécanique quantique, la relativité, la physique nucléaire et des particules et la physique atomique et moléculaire.
Le mot "astrochimie" peut être appliqué à la fois au système solaire et au milieu interstellaire.
Le terme exobiologie est similaire.
Les observations de la structure à grande échelle de l'Univers, une branche connue sous le nom de cosmologie physique, ont fourni une compréhension approfondie de la formation et de l'évolution du cosmos.
Une structure hiérarchique de la matière a commencé à se former à partir de variations infimes de la densité de masse de l'espace.
Des agrégations gravitationnelles se sont regroupées en filaments, laissant des vides dans les interstices.
Divers domaines de la physique sont cruciaux pour étudier l'univers.
Enfin, ce dernier est important pour la compréhension de la structure à grande échelle du cosmos.
Comme son nom l'indique, une galaxie elliptique a la forme en coupe d'une ellipse.
Les galaxies elliptiques se trouvent plus souvent au cœur des amas galactiques et peuvent avoir été formées par la fusion de grandes galaxies.
Les galaxies spirales sont généralement entourées d'un halo d'étoiles plus anciennes.
Environ un quart de toutes les galaxies sont irrégulières, et les formes particulières de ces galaxies peuvent être le résultat d'une interaction gravitationnelle.
Une radiogalaxie est une galaxie active qui est très lumineuse dans la partie radio du spectre et émet d'immenses panaches ou lobes de gaz.
La structure à grande échelle du cosmos est représentée par des groupes et des amas de galaxies.
Au centre de la Voie lactée se trouve le noyau, un renflement en forme de barre avec ce que l'on pense être un trou noir supermassif en son centre.
Le disque est entouré d'un halo sphéroïde d'étoiles plus anciennes de population II, ainsi que de concentrations relativement denses d'étoiles appelées amas globulaires.
Ceux-ci commencent comme un noyau pré-stellaire compact ou des nébuleuses sombres, qui se concentrent et s'effondrent (dans des volumes déterminés par la longueur de Jeans) pour former des protoétoiles compactes.
Ces amas se dispersent progressivement et les étoiles rejoignent la population de la Voie lactée.
7–18 La formation d'étoiles se produit dans des régions denses de poussière et de gaz, connues sous le nom de nuages moléculaires géants.
Presque tous les éléments plus lourds que l'hydrogène et l'hélium ont été créés à l'intérieur du cœur des étoiles.
Au fil du temps, cet hydrogène carburant est complètement converti en hélium et l'étoile commence à évoluer.
L'éjection des couches externes forme une nébuleuse planétaire.
Il s'agit d'une oscillation de 11 ans du nombre de taches solaires.
Le Soleil a également subi des changements périodiques de luminosité qui peuvent avoir un impact significatif sur la Terre.
Au-dessus de cette couche se trouve une région mince connue sous le nom de chromosphère.
Au-dessus du noyau se trouve la zone de rayonnement, où le plasma transporte le flux d'énergie au moyen d'un rayonnement.
Un vent solaire de particules de plasma s'écoule constamment du Soleil jusqu'à ce qu'il atteigne l'héliopause à la limite extrême du système solaire.
Les planètes se sont formées il y a 4,6 milliards d'années dans le disque protoplanétaire qui entourait le Soleil primitif.
Les planètes ont continué à balayer ou à éjecter la matière restante pendant une période de bombardements intenses, comme en témoignent les nombreux cratères d'impact sur la Lune.
Ce processus peut former un noyau pierreux ou métallique, entouré d'un manteau et d'une croûte externe.
Certaines planètes et lunes accumulent suffisamment de chaleur pour entraîner des processus géologiques tels que le volcanisme et la tectonique.
L'astrostatistique est l'application des statistiques à l'astrophysique à l'analyse d'une grande quantité de données astrophysiques d'observation.
La cosmochimie est l'étude des produits chimiques trouvés dans le système solaire, y compris les origines des éléments et les variations des rapports isotopiques.
Les clubs d'astronomie sont répartis dans le monde entier et beaucoup ont des programmes pour aider leurs membres à mettre en place et à compléter des programmes d'observation, y compris ceux pour observer tous les objets des catalogues Messier (110 objets) ou Herschel 400 de points d'intérêt dans le ciel nocturne.
La plupart des amateurs travaillent sur des longueurs d'onde visibles, mais une petite minorité expérimente des longueurs d'onde en dehors du spectre visible.
Un certain nombre d'astronomes amateurs utilisent soit des télescopes faits maison, soit des radiotélescopes qui ont été construits à l'origine pour la recherche en astronomie mais qui sont maintenant disponibles pour les amateurs (par exemple le télescope One-Mile).
Les réponses à ces questions peuvent nécessiter la construction de nouveaux instruments au sol et dans l'espace, et éventuellement de nouveaux développements en physique théorique et expérimentale.
Une compréhension plus profonde de la formation des étoiles et des planètes est nécessaire.
Si oui, quelle est l'explication du paradoxe de Fermi ?
Quelle est la nature de la matière noire et de l'énergie noire ?
Comment se sont formées les premières galaxies ?
L'astrobiologie, anciennement connue sous le nom d'exobiologie, est un domaine scientifique interdisciplinaire qui étudie les origines, l'évolution précoce, la distribution et l'avenir de la vie dans l'univers.
L'origine et l'évolution précoce de la vie font partie intégrante de la discipline de l'astrobiologie.
La biochimie a peut-être commencé peu de temps après le Big Bang, il y a 13,8 milliards d'années, à une époque habitable où l'Univers n'avait que 10 à 17 millions d'années.
Néanmoins, la Terre est le seul endroit de l'univers que les humains connaissent pour abriter la vie.
Le terme exobiologie a été inventé par le biologiste moléculaire et prix Nobel Joshua Lederberg.
Le terme xénobiologie est désormais utilisé dans un sens plus spécialisé, pour signifier « biologie basée sur une chimie étrangère », qu'elle soit d'origine extraterrestre ou terrestre (éventuellement synthétique).
Bien qu'autrefois considérée comme en dehors du courant dominant de la recherche scientifique, l'astrobiologie est devenue un domaine d'étude formalisé.
En 1959, la NASA a financé son premier projet d'exobiologie, et en 1960, la NASA a fondé un programme d'exobiologie, qui est maintenant l'un des quatre principaux éléments du programme actuel d'astrobiologie de la NASA.
Les progrès dans les domaines de l'astrobiologie, de l'astronomie d'observation et la découverte de grandes variétés d'extrêmophiles dotés d'une capacité extraordinaire à prospérer dans les environnements les plus difficiles de la Terre ont conduit à la spéculation selon laquelle la vie pourrait prospérer sur de nombreux corps extraterrestres de l'univers.
Les missions spécialement conçues pour rechercher la vie actuelle sur Mars étaient le programme Viking et les sondes Beagle 2.
Fin 2008, l'atterrisseur Phoenix a sondé l'environnement pour l'habitabilité planétaire passée et présente de la vie microbienne sur Mars et a étudié l'histoire de l'eau là-bas.
En novembre 2011, la NASA a lancé la mission Mars Science Laboratory transportant le rover Curiosity, qui a atterri sur Mars à Gale Crater en août 2012.
L'une est l'hypothèse éclairée selon laquelle la grande majorité des formes de vie dans notre galaxie sont basées sur la chimie du carbone, comme toutes les formes de vie sur Terre.
Le fait que les atomes de carbone se lient facilement à d'autres atomes de carbone permet la construction de molécules extrêmement longues et complexes.
Une troisième hypothèse consiste à se concentrer sur les planètes en orbite autour d'étoiles semblables au Soleil pour augmenter les probabilités d'habitabilité planétaire.
À cette fin, un certain nombre d'instruments conçus pour détecter des exoplanètes de la taille de la Terre ont été envisagés, notamment les programmes Terrestrial Planet Finder (TPF) de la NASA et Darwin de l'ESA, qui ont tous deux été annulés.
Drake a initialement formulé l'équation simplement comme un programme de discussion à la conférence de Green Bank, mais certaines applications de la formule avaient été prises à la lettre et liées à des arguments simplistes ou pseudoscientifiques.
La découverte des extrêmophiles, des organismes capables de survivre dans des environnements extrêmes, est devenue un élément de recherche essentiel pour les astrobiologistes, car ils sont importants pour comprendre quatre domaines dans les limites de la vie dans un contexte planétaire : le potentiel de panspermie, la contamination directe due aux projets d'exploration humaine. , la colonisation planétaire par les humains et l'exploration de la vie extraterrestre éteinte et existante.
Même la vie dans les profondeurs de l'océan, où la lumière du soleil ne peut pas atteindre, était censée se nourrir soit en consommant des détritus organiques pleuvant des eaux de surface, soit en mangeant des animaux qui le faisaient.
Cette chimiosynthèse a révolutionné l'étude de la biologie et de l'astrobiologie en révélant que la vie n'a pas besoin de dépendre du soleil ; elle n'a besoin que d'eau et d'un gradient d'énergie pour exister.
Dix organismes rustiques sélectionnés pour le projet LIFE, par Amir Alexander Deinococcus radiodurans, Bacillus subtilis, la levure Saccharomyces cerevisiae, des graines d'Arabidopsis thaliana ("arabette"), ainsi que l'animal invertébré Tardigrade.
La lune de Jupiter, Europe, et la lune de Saturne, Encelade, sont maintenant considérées comme les emplacements les plus probables pour la vie extraterrestre existante dans le système solaire en raison de leurs océans d'eau souterraine où le chauffage radiogénique et maréal permet à l'eau liquide d'exister.
La poussière cosmique qui imprègne l'univers contient des composés organiques complexes ("solides organiques amorphes à structure mixte aromatique-aliphatique") qui pourraient être créés naturellement, et rapidement, par les étoiles.
Les HAP semblent s'être formés peu après le Big Bang, sont répandus dans tout l'univers et sont associés à de nouvelles étoiles et exoplanètes.
L'astroécologie expérimentale étudie les ressources dans les sols planétaires, en utilisant des matériaux spatiaux réels dans les météorites.
À plus grande échelle, la cosmoécologie concerne la vie dans l'univers au cours des temps cosmologiques.
Les spécialisations comprennent la cosmochimie, la biochimie et la géochimie organique.
Certaines régions de la Terre, telles que le Pilbara en Australie occidentale et les vallées sèches de McMurdo en Antarctique, sont également considérées comme des analogues géologiques des régions de Mars et, à ce titre, pourraient fournir des indices sur la manière de rechercher la vie passée sur Mars.
En effet, il semble probable que les éléments de base de la vie seront partout similaires à ceux de la Terre, dans la généralité sinon dans les détails.
Seuls deux des atomes naturels, le carbone et le silicium, sont connus pour servir de squelettes de molécules suffisamment grosses pour transporter des informations biologiques.
Les quatre candidats les plus probables à la vie dans le système solaire sont la planète Mars, la lune jovienne Europe et les lunes de Saturne Titan et Encelade.
Aux basses températures et à la basse pression martiennes, l'eau liquide est susceptible d'être très saline.
Le 11 décembre 2013, la NASA a signalé la détection de «minéraux argileux» (en particulier des phyllosilicates), souvent associés à des matériaux organiques, sur la croûte glacée d'Europe.
Certains scientifiques pensent qu'il est possible que ces hydrocarbures liquides prennent la place de l'eau dans des cellules vivantes différentes de celles de la Terre.
Il n'y a aucun processus abiotique connu sur la planète qui pourrait causer sa présence.
Yamato 000593, la deuxième plus grande météorite de Mars, a été découverte sur Terre en 2000.
Le 5 mars 2011, Richard B. Hoover , un scientifique du Marshall Space Flight Center , a spéculé sur la découverte de microfossiles présumés similaires aux cyanobactéries dans les météorites carbonées CI1 dans le Journal of Cosmology , une histoire largement rapportée par les médias grand public.
Des preuves de perchlorates ont été trouvées dans tout le système solaire, et plus particulièrement sur Mars.
Des méthodes de détection améliorées et un temps d'observation accru permettront sans aucun doute de découvrir davantage de systèmes planétaires, et peut-être d'autres semblables au nôtre.
L'objectif est de détecter les organismes capables de survivre aux conditions de voyage dans l'espace et de maintenir leur capacité de prolifération.
Ces réponses au stress pourraient également leur permettre de survivre dans des conditions spatiales difficiles, bien que l'évolution impose également certaines restrictions à leur utilisation en tant qu'analogues de la vie extraterrestre.
La formation de spores lui permet de survivre dans des environnements extrêmes tout en étant capable de relancer la croissance cellulaire.
Les deux atterrisseurs étant identiques, les mêmes tests ont été effectués à deux endroits de la surface de Mars ; Viking 1 près de l'équateur et Viking 2 plus au nord.
En astronomie, l'extinction est l'absorption et la diffusion du rayonnement électrobamnétique par la poussière et le gaz entre un objet astronomique émetteur et l'observateur.
Pour les étoiles situées près du plan de la Voie lactée et à quelques milliers de parsecs de la Terre, l'extinction dans la bande de fréquences visuelles (système photométrique) est d'environ 1,8 bamnitudes par kiloparsec.
Le rougissement se produit en raison de la diffusion de la lumière sur la poussière et d'autres matières dans le milieu interstellaire.
Dans la plupart des systèmes photométriques, des filtres (bandes passantes) sont utilisés à partir desquels les lectures de la luminosité de la lumière peuvent tenir compte de la latitude et de l'humidité parmi les facteurs terrestres.
D'une manière générale, l'extinction interstellaire est la plus forte aux courtes longueurs d'onde, généralement observées en utilisant des techniques de spectroscopie.
La quantité d'extinction peut être significativement plus élevée que cela dans des directions spécifiques.
En conséquence, lors du calcul des distances cosmiques, il peut être avantageux de passer aux données d'étoiles du proche infrarouge (dont le filtre ou la bande passante Ks est assez standard) où les variations et la quantité d'extinction sont nettement inférieures, et des rapports similaires quant à R(Ks) : 0,49 ± 0,02 et 0,528 ± 0,015 ont été trouvés respectivement par des groupes indépendants.
Cette caractéristique a été observée pour la première fois dans les années 1960, mais son origine n'est toujours pas bien comprise.
Dans le SMC, une variation plus extrême est observée sans 2175 Å et une très forte extinction des UV lointains dans la barre de formation d'étoiles et une extinction ultraviolette assez normale observée dans l'aile plus calme.
Trouver des courbes d'extinction dans le LMC et le SMC qui sont similaires à celles trouvées dans la Voie lactée et trouver des courbes d'extinction dans la Voie lactée qui ressemblent davantage à celles trouvées dans la supershell LMC2 du LMC et dans la barre SMC a donné lieu à un nouvelle interprétation.
Cette extinction a trois composantes principales : la diffusion Rayleigh par les molécules d'air, la diffusion par les particules et l'absorption moléculaire.
La quantité d'une telle extinction est la plus faible au zénith de l'observateur et la plus élevée près de l'horizon.
L'équation de Drake spécule sur l'existence d'une vie consciente ailleurs dans l'univers.
Cela englobe une recherche de la vie extraterrestre actuelle et historique, et une recherche plus étroite de la vie intelligente extraterrestre.
Au fil des ans, la science-fiction a communiqué des idées scientifiques, exploré un large éventail de possibilités et influencé l'intérêt du public et les perspectives de la vie extraterrestre.
Selon cet argument, avancé par des scientifiques tels que Carl Sagan et Stephen Hawking, ainsi que des personnalités notables telles que Winston Churchill, il serait improbable que la vie n'existe pas ailleurs que sur Terre.
La vie peut avoir émergé indépendamment à de nombreux endroits dans l'univers.
À chaque niveau de l'organisme, il y aura des mécanismes en place pour éliminer les conflits, maintenir la coopération et maintenir le fonctionnement de l'organisme.
La vie basée sur l'ammoniac (plutôt que sur l'eau) a été suggérée comme alternative, bien que ce solvant semble moins approprié que l'eau.
Environ 95 % de la matière vivante est constituée de seulement six éléments : le carbone, l'hydrogène, l'azote, l'oxygène, le phosphore et le soufre.
L'atome de carbone a la capacité unique de créer quatre liaisons chimiques fortes avec d'autres atomes, y compris d'autres atomes de carbone.
Selon la stratégie d'astrobiologie de la NASA de 2015, "la vie sur d'autres mondes est très susceptible d'inclure des microbes, et tout système vivant complexe ailleurs est susceptible d'avoir surgi et d'être fondé sur la vie microbienne.
Rick Colwell, membre de l'équipe Deep Carbon Observatory de l'Oregon State University, a déclaré à la BBC: "Je pense qu'il est probablement raisonnable de supposer que le sous-sol d'autres planètes et leurs lunes sont habitables, d'autant plus que nous avons vu ici sur Terre que les organismes peuvent fonctionner loin de la lumière du soleil en utilisant l'énergie fournie directement par les roches profondes du sous-sol".
L'hypothèse de la panspermie propose que la vie ailleurs dans le système solaire puisse avoir une origine commune.
Au XIXe siècle, il a de nouveau été relancé sous une forme moderne par plusieurs scientifiques, dont Jöns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) et, un peu plus tard, par Svante Arrhenius (1903).
L'une des premières enquêtes scientifiques sur le sujet est apparue dans un article du Scientific American de 1878 intitulé « La Lune est-elle habitée ?
Les régions chaudes et sous pression à l'intérieur de la Lune pourraient encore contenir de l'eau liquide.
Il est prouvé que Mars a eu un passé plus chaud et plus humide : des lits de rivières asséchés, des calottes glaciaires polaires, des volcans et des minéraux qui se forment en présence d'eau ont tous été découverts.
La vapeur pourrait avoir été produite par des volcans de glace ou par de la glace près de la surface se sublimant (se transformant de solide en gaz).
Il est également possible qu'Europe puisse soutenir la macrofaune aérobie en utilisant l'oxygène créé par les rayons cosmiques impactant sa glace de surface.
Le 11 décembre 2013, la NASA a signalé la détection de «minéraux argileux» (en particulier des phyllosilicates), souvent associés à des matériaux organiques, sur la croûte glacée d'Europe.
Certains prétendent avoir identifié des preuves que la vie microbienne a existé sur Mars.
En 1996, un rapport controversé indiquait que des structures ressemblant à des nanobactéries avaient été découvertes dans une météorite, ALH84001, formée de roche éjectée de Mars.
Les responsables de la NASA ont rapidement distancé la NASA des affirmations des scientifiques, et Stoker elle-même a reculé sur ses affirmations initiales.
Il est conçu pour évaluer l'habitabilité passée et présente sur Mars à l'aide de divers instruments scientifiques.
Cependant, des progrès significatifs dans la capacité de trouver et de résoudre la lumière de petits mondes rocheux proches de leurs étoiles sont nécessaires avant que de telles méthodes spectroscopiques puissent être utilisées pour analyser les planètes extrasolaires.
En août 2011, les découvertes de la NASA, basées sur des études de météorites trouvées sur Terre, suggèrent que des composants d'ADN et d'ARN (adénine, guanine et molécules organiques apparentées), éléments constitutifs de la vie telle que nous la connaissons, pourraient être formés de manière extraterrestre dans l'espace.
En août 2012, et dans une première mondiale, des astronomes de l'Université de Copenhague ont signalé la détection d'une molécule de sucre spécifique, le glycolaldéhyde, dans un système stellaire lointain.
Le télescope spatial Kepler a également détecté quelques milliers de planètes candidates, dont environ 11% pourraient être des faux positifs.
La planète la plus massive répertoriée dans les archives d'exoplanètes de la NASA est DENIS-P J082303.1-491201 b, environ 29 fois la masse de Jupiter, bien que selon la plupart des définitions d'une planète, elle soit trop massive pour être une planète et peut être une naine brune à la place.
Un signe qu'une planète contient probablement déjà de la vie est la présence d'une atmosphère avec des quantités importantes d'oxygène, car ce gaz est très réactif et ne durerait généralement pas longtemps sans un réapprovisionnement constant.
Même si l'on suppose que seulement une sur un milliard de ces étoiles a des planètes supportant la vie, il y aurait quelque 6,25 milliards de systèmes planétaires supportant la vie dans l'univers observable.
La première affirmation enregistrée de la vie humaine extraterrestre se trouve dans les anciennes écritures du jaïnisme.
Des écrivains musulmans médiévaux comme Fakhr al-Din al-Razi et Muhammad al-Baqir ont soutenu le pluralisme cosmique sur la base du Coran.
Une fois qu'il est devenu clair que la Terre n'était qu'une planète parmi d'innombrables corps dans l'univers, la théorie de la vie extraterrestre a commencé à devenir un sujet dans la communauté scientifique.
La possibilité d'extraterrestres est restée une spéculation répandue alors que la découverte scientifique s'accélérait.
L'idée de la vie sur Mars a conduit l'écrivain britannique HG Wells à écrire le roman La guerre des mondes en 1897, racontant une invasion par des extraterrestres de Mars qui fuyaient l'assèchement de la planète.
La croyance aux êtres extraterrestres continue d'être exprimée dans la pseudoscience, les théories du complot et le folklore populaire, notamment la "Zone 51" et les légendes.
Ward et Brownlee sont ouverts à l'idée d'une évolution sur d'autres planètes qui ne soit pas basée sur des caractéristiques terrestres essentielles (telles que l'ADN et le carbone).
Si des extraterrestres nous rendaient visite, le résultat serait un peu comme lorsque Christophe Colomb a débarqué en Amérique, ce qui n'a pas bien tourné pour les Amérindiens", a-t-il déclaré.
Le COSPAR fournit également des lignes directrices pour la protection planétaire.
De plus, selon la réponse, il n'y a "aucune information crédible suggérant que des preuves soient cachées aux yeux du public".
En haut : Sources lumineuses de bamnitudes différentes.
Comète Borrelly, les couleurs montrent sa luminosité sur la gamme des trois ordres de bamnitude (à droite).
L'échelle est logarithmique et définie de telle sorte que chaque pas d'une bamnitude modifie la luminosité d'un facteur de la cinquième racine de 100, soit environ 2,512.
Les astronomes utilisent deux définitions différentes de la bamnitude : la bamnitude apparente et la bamnitude absolue.
La bamnitude absolue décrit la luminosité intrinsèque émise par un objet et est définie comme étant égale à la bamnitude apparente que l'objet aurait s'il était placé à une certaine distance de la Terre, 10 parsecs pour les étoiles.
Le développement du télescope a montré que ces grandes tailles étaient illusoires - les étoiles apparaissant beaucoup plus petites à travers le télescope.
Plus la valeur est négative, plus l'objet est lumineux.
Les étoiles qui ont des bamnitudes comprises entre 1,5 et 2,5 sont appelées deuxième bamnitude ; il y a une vingtaine d'étoiles plus brillantes que 1,5, qui sont des étoiles de première bamnitude (voir la liste des étoiles les plus brillantes).
Les bamnitudes absolues pour les objets du système solaire sont fréquemment citées sur la base d'une distance de 1 UA.
La forme la plus simple de technologie est le développement et l'utilisation d'outils de base.
Il a aidé à développer des économies plus avancées (y compris l'économie mondiale d'aujourd'hui) et a permis l'essor d'une classe de loisirs.
Citons par exemple la montée en puissance de la notion d'efficacité en termes de productivité humaine, et les enjeux de la bioéthique.
La signification du terme a changé au début du XXe siècle lorsque les spécialistes américains des sciences sociales, à commencer par Thorstein Veblen, ont traduit les idées du concept allemand de Technik en « technologie ».
En 1937, le sociologue américain Read Bain écrivait que "la technologie comprend tous les outils, machines, ustensiles, armes, instruments, logements, vêtements, appareils de communication et de transport et les compétences par lesquelles nous les produisons et les utilisons".
Plus récemment, des chercheurs ont emprunté aux philosophes européens de la « technique » pour étendre le sens de la technologie à diverses formes de raison instrumentale, comme dans les travaux de Foucault sur les techniques de soi.
pour inventer des choses utiles ou pour résoudre des problèmes » et « une machine, un équipement, une méthode, etc.,
Le terme est souvent utilisé pour impliquer un domaine technologique spécifique, ou pour désigner la haute technologie ou simplement l'électronique grand public, plutôt que la technologie dans son ensemble.
Dans cet usage, la technologie fait référence aux outils et aux machines qui peuvent être utilisés pour résoudre des problèmes du monde réel.
W. Brian Arthur définit la technologie d'une manière tout aussi large comme "un moyen d'accomplir un objectif humain".
Lorsqu'il est combiné avec un autre terme, tel que "technologie médicale" ou "technologie spatiale", il fait référence à l'état des connaissances et des outils du domaine respectif. "
De plus, la technologie est l'application des mathématiques, des sciences et des arts au profit de la vie telle qu'elle est connue.
L'ingénierie est le processus axé sur les objectifs de conception et de fabrication d'outils et de systèmes pour exploiter les phénomènes naturels à des fins humaines pratiques, souvent (mais pas toujours) en utilisant les résultats et les techniques de la science.
Par exemple, la science pourrait étudier le flux d'électrons dans les conducteurs électriques en utilisant des outils et des connaissances déjà existants.
Les relations exactes entre la science et la technologie, en particulier, ont été débattues par des scientifiques, des historiens et des décideurs à la fin du XXe siècle, en partie parce que le débat peut éclairer le financement des sciences fondamentales et appliquées.
Les premiers humains ont évolué à partir d'une espèce d'hominidés en quête de nourriture qui étaient déjà bipèdes, avec une masse cérébrale d'environ un tiers des humains modernes.
L'invention des haches en pierre polie a été une avancée majeure qui a permis le déboisement à grande échelle pour créer des fermes.
La première utilisation connue de l'énergie éolienne est le voilier; le premier enregistrement d'un navire à voile est celui d'un bateau du Nil datant du 8e millénaire avant notre ère.
Selon les archéologues, la roue a été inventée vers 4000 avant notre ère, probablement indépendamment et presque simultanément en Mésopotamie (dans l'actuel Irak), dans le Caucase du Nord (culture Maykop) et en Europe centrale.
Plus récemment, la plus ancienne roue en bois connue au monde a été découverte dans les marais de Ljubljana en Slovénie.
Les anciens Sumériens utilisaient le tour du potier et l'ont peut-être inventé.
Les premiers chariots à deux roues étaient dérivés du travois et ont été utilisés pour la première fois en Mésopotamie et en Iran vers 3000 avant notre ère.
Une baignoire pratiquement identique aux baignoires modernes a été découverte au Palais de Knossos.
L'égout primaire à Rome était le Cloaca Maxima ; sa construction a commencé au VIe siècle avant notre ère et il est toujours utilisé aujourd'hui.
La technologie médiévale a vu l'utilisation de machines simples (telles que le levier, la vis et la poulie) combinées pour former des outils plus compliqués, tels que la brouette, les moulins à vent et les horloges, et un système d'universités a développé et diffusé des idées et des pratiques scientifiques. .
Commençant au Royaume-Uni au XVIIIe siècle, la révolution industrielle a été une période de grandes découvertes technologiques, en particulier dans les domaines de l'agriculture, de la fabrication, de l'exploitation minière, de la métallurgie et des transports, entraînée par la découverte de la vapeur et l'application généralisée de la système d'usine.
L'essor de la technologie a conduit à des gratte-ciel et à de vastes zones urbaines dont les habitants dépendent de moteurs pour les transporter ainsi que leurs approvisionnements alimentaires.
Le XXe siècle a apporté son lot d'innovations.
Les technologies de l'information ont ensuite conduit à la naissance dans les années 1980 d'Internet, qui a inauguré l'ère de l'information actuelle.
Des techniques et des organisations complexes de fabrication et de construction sont nécessaires pour fabriquer et entretenir certaines des technologies les plus récentes, et des industries entières ont vu le jour pour soutenir et développer des générations successives d'outils de plus en plus complexes.
Les transhumanistes croient généralement que le but de la technologie est de surmonter les barrières, et que ce que nous appelons communément la condition humaine n'est qu'une autre barrière à franchir.
Ils suggèrent que le résultat inévitable d'une telle société est de devenir de plus en plus technologique au détriment de la liberté et de la santé psychologique.
Il espère révéler l'essence de la technologie d'une manière qui « ne nous confine en aucun cas à une compulsion abrutie de pousser aveuglément la technologie ou, ce qui revient au même, de se rebeller impuissant contre elle ».
Certaines des critiques les plus poignantes de la technologie se trouvent dans ce qui est maintenant considéré comme des classiques littéraires dystopiques tels que Brave New World d'Aldous Huxley, A Clockwork Orange d'Anthony Burgess et Nineteen Eighty-Four de George Orwell.
Le regretté critique culturel Neil Postman a distingué les sociétés utilisatrices d'outils des sociétés technologiques et de ce qu'il appelait les « technopoles », des sociétés qui sont dominées par l'idéologie du progrès technologique et scientifique à l'exclusion ou au détriment d'autres pratiques culturelles, valeurs et monde. vues.
Nikolas Kompridis a également écrit sur les dangers des nouvelles technologies, telles que le génie génétique, la nanotechnologie, la biologie synthétique et la robotique.
Un autre critique éminent de la technologie est Hubert Dreyfus, qui a publié des livres tels que On the Internet et What Computers Still Can't Do.
Dans son article, Jared Bernstein, chercheur principal au Center on Budget and Policy Priorities, remet en question l'idée répandue selon laquelle l'automatisation, et plus largement les progrès technologiques, ont principalement contribué à ce problème croissant du marché du travail.
Il utilise deux arguments principaux pour défendre son propos.
En effet, l'automatisation menace les tâches répétitives mais les tâches haut de gamme sont toujours nécessaires car elles complètent la technologie et les tâches manuelles qui "demandent de la flexibilité, du jugement et du bon sens" restent difficiles à remplacer par des machines.
La technologie est souvent considérée de manière trop étroite ; selon Hughes, "La technologie est un processus créatif impliquant l'ingéniosité humaine".
Ils ont souvent supposé que la technologie était facilement contrôlable et cette hypothèse doit être soigneusement remise en question.
Le solutionnisme est l'idéologie selon laquelle chaque problème social peut être résolu grâce à la technologie et en particulier grâce à Internet.
Benjamin R. Cohen et Gwen Ottinger ont également discuté des effets multivalents de la technologie.
L'utilisation de la technologie de base est également une caractéristique d'autres espèces animales en dehors des humains.
La capacité de fabriquer et d'utiliser des outils était autrefois considérée comme une caractéristique déterminante du genre Homo.
En 2005, le futuriste Ray Kurzweil a prédit que l'avenir de la technologie consisterait principalement en une «révolution GNR» qui se chevauche entre la génétique, la nanotechnologie et la robotique, la robotique étant la plus importante des trois.
Les humains ont déjà fait quelques-uns des premiers pas vers la réalisation de la révolution GNR.
Certains pensent que l'avenir de la robotique impliquera une « intelligence non biologique supérieure à l'humain ».
Cet avenir partage de nombreuses similitudes avec le concept d'obsolescence planifiée, cependant, l'obsolescence planifiée est considérée comme une "stratégie commerciale sinistre".
La génétique a également été explorée, les humains comprenant dans une certaine mesure le génie génétique.
D'autres pensent que le génie génétique sera utilisé pour rendre les humains plus résistants ou complètement immunisés contre certaines maladies.
Les futuristes pensent que la technologie des nanobots permettra aux humains de « manipuler la matière à l'échelle moléculaire et atomique ».
Dans ce contexte, aujourd'hui obsolète, un "moteur" désignait une machine militaire, c'est-à-dire un engin mécanique utilisé à la guerre (par exemple, une catapulte).
Les six machines simples classiques étaient connues dans l'ancien Proche-Orient.
Le mécanisme à levier est apparu pour la première fois il y a environ 5 000 ans au Proche-Orient, où il était utilisé dans une balance simple et pour déplacer de gros objets dans la technologie égyptienne antique.
La vis, dernière des machines simples à avoir été inventée, fait son apparition en Mésopotamie à l'époque néo-assyrienne (911-609) av.
En tant que l'un des fonctionnaires du pharaon Djosèr, il a probablement conçu et supervisé la construction de la pyramide de Djoser (la pyramide à degrés) à Saqqarah en Égypte vers 2630-2611 av.
Les ancêtres koushites ont construit des spéos à l'âge du bronze entre 3700 et 3250 avant JC. Des bloomeries et des hauts fourneaux ont également été créés au 7ème siècle avant JC à Kush.
Certaines des inventions d'Archimède ainsi que le mécanisme d'Anticythère nécessitaient une connaissance sophistiquée des engrenages différentiels ou épicycloïdaux, deux principes clés de la théorie des machines qui ont aidé à concevoir les trains d'engrenages de la révolution industrielle, et qui sont encore largement utilisés aujourd'hui dans divers domaines tels que la robotique. et l'ingénierie automobile.
Le rouet était également un précurseur de la machine à filer, qui était un développement clé au début de la révolution industrielle au 18ème siècle.
Il a décrit quatre musiciens automates, y compris des batteurs actionnés par une boîte à rythmes programmable, où ils pouvaient être amenés à jouer différents rythmes et différents motifs de batterie.
En dehors de ces professions, les universités n'étaient pas censées avoir eu beaucoup d'importance pratique pour la technologie.
La construction du canal était un travail d'ingénierie important au cours des premières phases de la révolution industrielle.
Il était également un ingénieur mécanicien compétent et un éminent physicien.
Smeaton a également apporté des améliorations mécaniques à la machine à vapeur Newcomen.
Samuel Morland, un mathématicien et inventeur qui a travaillé sur les pompes, a laissé des notes au Vauxhall Ordinance Office sur une conception de pompe à vapeur que Thomas Savery a lues.
Le marchand de fer Thomas Newcomen , qui a construit la première machine à vapeur à piston commerciale en 1712, n'était pas connu pour avoir une formation scientifique.
Ces innovations ont réduit le coût du fer, rendant pratiques les chemins de fer à cheval et les ponts en fer.
Avec le développement de la machine à vapeur à haute pression, le rapport puissance/poids des machines à vapeur a rendu possibles des bateaux à vapeur et des locomotives pratiques.
La révolution industrielle a créé une demande de machines avec des pièces métalliques, ce qui a conduit au développement de plusieurs machines-outils.
Les techniques d'usinage de précision ont été développées dans la première moitié du XIXe siècle.
Le recensement des États-Unis de 1850 a répertorié la profession d '«ingénieur» pour la première fois avec un décompte de 2 000.
En 1890, il y avait 6 000 ingénieurs dans les domaines civil, minier, mécanique et électrique.
Les fondements de l'électrotechnique dans les années 1800 comprenaient les expériences d'Alessandro Volta, Michael Faraday, Georg Ohm et d'autres et l'invention du télégraphe électrique en 1816 et du moteur électrique en 1872.
L'ingénierie aéronautique traite de la conception des processus de conception des aéronefs, tandis que l'ingénierie aérospatiale est un terme plus moderne qui élargit la portée de la discipline en incluant la conception des engins spatiaux.
Historiquement, le génie naval et le génie minier étaient des branches importantes.
En conséquence, de nombreux ingénieurs continuent d'apprendre de nouveaux matériaux tout au long de leur carrière.
Il est généralement insuffisant pour construire un produit techniquement réussi, mais il doit également répondre à d'autres exigences.
Genrich Altshuller, après avoir rassemblé des statistiques sur un grand nombre de brevets, a suggéré que les compromis sont au cœur des conceptions d'ingénierie de "bas niveau", tandis qu'à un niveau supérieur, la meilleure conception est celle qui élimine la contradiction fondamentale à l'origine du problème.
Les tests garantissent que les produits fonctionneront comme prévu.
En plus des logiciels d'application métier typiques, il existe un certain nombre d'applications assistées par ordinateur (technologies assistées par ordinateur) spécifiquement destinées à l'ingénierie.
Il permet aux ingénieurs de créer des modèles 3D, des dessins 2D et des schémas de leurs conceptions.
L'accès et la diffusion de toutes ces informations sont généralement organisés à l'aide d'un logiciel de gestion des données produits.
De par sa nature même, l'ingénierie a des interconnexions avec la société, la culture et le comportement humain.
Les projets d'ingénierie peuvent être sujets à controverse.
L'ingénierie est un moteur clé de l'innovation et du développement humain.
Cela peut entraîner de nombreux problèmes économiques et politiques négatifs, ainsi que des problèmes éthiques.
Les scientifiques peuvent également avoir à accomplir des tâches d'ingénierie, telles que la conception d'appareils expérimentaux ou la construction de prototypes.
Premièrement, il traite souvent de domaines dans lesquels les bases de la physique ou de la chimie sont bien comprises, mais les problèmes eux-mêmes sont trop complexes pour être résolus de manière exacte.
Le premier assimile une compréhension à un principe mathématique tandis que le second mesure les variables impliquées et crée la technologie.
Un physicien aurait généralement besoin d'une formation supplémentaire et pertinente.
Un exemple de ceci est l'utilisation d'approximations numériques des équations de Navier – Stokes pour décrire l'écoulement aérodynamique au-dessus d'un aéronef, ou l'utilisation de la méthode des éléments finis pour calculer les contraintes dans des composants complexes.
Les ingénieurs mettent l'accent sur l'innovation et l'invention.
Puisqu'une conception doit être réaliste et fonctionnelle, sa géométrie, ses dimensions et ses caractéristiques doivent être définies.
Ainsi, ils ont étudié les mathématiques, la physique, la chimie, la biologie et la mécanique.
La médecine moderne peut remplacer plusieurs des fonctions du corps grâce à l'utilisation d'organes artificiels et peut modifier considérablement la fonction du corps humain grâce à des dispositifs artificiels tels que, par exemple, des implants cérébraux et des stimulateurs cardiaques.
Les deux domaines fournissent des solutions aux problèmes du monde réel.
La gestion de l'ingénierie ou «ingénierie de gestion» est un domaine spécialisé de la gestion concerné par la pratique de l'ingénierie ou le secteur de l'industrie de l'ingénierie.
L'ingénieur spécialisé en conduite du changement doit avoir une connaissance approfondie de l'application des principes et méthodes de la psychologie industrielle et organisationnelle.
L'intelligence artificielle (IA) est l'intelligence démontrée par les machines, par opposition à l'intelligence naturelle affichée par les humains ou les animaux.
La recherche sur l'IA a essayé et rejeté de nombreuses approches différentes au cours de sa vie, notamment la simulation du cerveau, la modélisation de la résolution de problèmes humains, la logique formelle, de grandes bases de données de connaissances et l'imitation du comportement animal.
Les objectifs traditionnels de la recherche en IA comprennent le raisonnement, la représentation des connaissances, la planification, l'apprentissage, le traitement du langage naturel, la perception et la capacité de déplacer et de manipuler des objets.
L'IA s'appuie également sur l'informatique, la psychologie, la linguistique, la philosophie et de nombreux autres domaines.
L'étude du raisonnement mécanique ou « formel » a commencé avec les philosophes et les mathématiciens dans l'Antiquité.
La thèse de Church-Turing, ainsi que les découvertes concomitantes en neurobiologie, en théorie de l'information et en cybernétique, ont conduit les chercheurs à envisager la possibilité de construire un cerveau électronique.
Les participants Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) et Arthur Samuel (IBM) sont devenus les fondateurs et les leaders de la recherche sur l'IA.
Les fondateurs d'AI étaient optimistes quant à l'avenir : Herbert Simon prédisait que "les machines seront capables, d'ici vingt ans, de faire n'importe quel travail qu'un homme peut faire".
Les progrès ont ralenti et en 1974, en réponse aux critiques de Sir James Lighthill et à la pression continue du Congrès américain pour financer des projets plus productifs, les gouvernements américain et britannique ont interrompu la recherche exploratoire en IA.
En 1985, le marché de l'IA avait atteint plus d'un milliard de dollars.
Des ordinateurs plus rapides, des améliorations algorithmiques et l'accès à de grandes quantités de données ont permis des avancées dans l'apprentissage automatique et la perception ; les méthodes d'apprentissage en profondeur gourmandes en données ont commencé à dominer les critères de précision vers 2012.
La recherche sur l'IA est divisée en sous-domaines concurrents qui ne parviennent souvent pas à communiquer entre eux.
La recherche était centrée sur trois institutions : l'Université Carnegie Mellon, Stanford et le MIT, et comme décrit ci-dessous, chacune a développé son propre style de recherche.
Ils ont appelé leur travail par plusieurs noms : par exemple, incarné, situé, comportemental ou développemental.
Le langage mathématique partagé a permis un niveau élevé de collaboration avec des domaines plus établis (comme les mathématiques, l'économie ou la recherche opérationnelle).
De nos jours, les résultats des expériences sont souvent rigoureusement mesurables, et sont parfois (difficilement) reproductibles.
Ces algorithmes se sont avérés insuffisants pour résoudre de gros problèmes de raisonnement car ils ont connu une « explosion combinatoire » : ils sont devenus exponentiellement plus lents à mesure que les problèmes grossissaient.
Parmi les choses qu'une base de connaissances complète de bon sens contiendrait, il y aurait : des objets, des propriétés, des catégories et des relations entre les objets ; situations, événements, états et temps; causes et effets; connaissance sur la connaissance (ce que nous savons sur ce que les autres savent) ; et bien d'autres domaines moins bien étudiés.
Par exemple, si un oiseau apparaît dans une conversation, les gens imaginent généralement un animal de la taille d'un poing qui chante et vole.
Presque rien n'est simplement vrai ou faux comme l'exige la logique abstraite.
Les projets de recherche qui tentent de construire une base de connaissances complète de connaissances de sens commun (par exemple, Cyc) nécessitent d'énormes quantités d'ingénierie ontologique laborieuse - ils doivent être construits, à la main, un concept compliqué à la fois.
Ils ont besoin d'un moyen de visualiser l'avenir - une représentation de l'état du monde et d'être capables de faire des prédictions sur la façon dont leurs actions le changeront - et d'être capables de faire des choix qui maximisent l'utilité (ou la "valeur") des choix disponibles .
Cela nécessite un agent qui peut non seulement évaluer son environnement et faire des prédictions, mais aussi évaluer ses prédictions et s'adapter en fonction de son évaluation.
La classification est utilisée pour déterminer à quelle catégorie quelque chose appartient, et se produit après qu'un programme voit un certain nombre d'exemples de choses de plusieurs catégories.
La théorie de l'apprentissage informatique peut évaluer les apprenants en fonction de la complexité informatique, de la complexité de l'échantillon (la quantité de données requise) ou d'autres notions d'optimisation.
De nombreuses approches actuelles utilisent des fréquences de cooccurrence de mots pour construire des représentations syntaxiques du texte. "
Les approches statistiques modernes de la PNL peuvent combiner toutes ces stratégies ainsi que d'autres, et atteignent souvent une précision acceptable au niveau de la page ou du paragraphe.
Un robot mobile moderne, lorsqu'il dispose d'un petit environnement statique et visible, peut facilement déterminer son emplacement et cartographier son environnement ; cependant, les environnements dynamiques, tels que (en endoscopie) l'intérieur du corps respiratoire d'un patient, posent un plus grand défi.
Par exemple, certains assistants virtuels sont programmés pour parler de manière conversationnelle ou même pour plaisanter avec humour ; cela les fait apparaître plus sensibles à la dynamique émotionnelle de l'interaction humaine, ou pour faciliter autrement l'interaction homme-ordinateur.
Un comportement intelligent peut-il être décrit à l'aide de principes simples et élégants (comme la logique ou l'optimisation) ?
Ou utilisons-nous des algorithmes qui ne peuvent nous donner qu'une solution "raisonnable" (par exemple, des méthodes probabilistes) mais peuvent être la proie du même genre d'erreurs impénétrables que fait l'intuition humaine ?
Stuart Russell et Peter Norvig observent que la plupart des chercheurs en IA "ne se soucient pas de l'hypothèse forte de l'IA - tant que le programme fonctionne, ils ne se soucient pas de savoir si vous l'appelez une simulation d'intelligence ou une intelligence réelle".
La nouvelle intelligence pourrait ainsi augmenter de façon exponentielle et dépasser considérablement les humains.
La relation entre l'automatisation et l'emploi est compliquée.
Les estimations subjectives du risque varient considérablement ; par exemple, Michael Osborne et Carl Benedikt Frey estiment que 47 % des emplois américains sont à « haut risque » d'automatisation potentielle, tandis qu'un rapport de l'OCDE classe seulement 9 % des emplois américains comme « à haut risque ».
À long terme, les scientifiques ont proposé de continuer à optimiser le fonctionnement tout en minimisant les risques de sécurité éventuels liés aux nouvelles technologies.
Dans son livre Superintelligence, le philosophe Nick Bostrom fournit un argument selon lequel l'intelligence artificielle constituera une menace pour l'humanité.
Bostrom souligne également la difficulté de transmettre pleinement les valeurs de l'humanité à une IA avancée.
Dans son livre Human Compatible, le chercheur en IA Stuart J. Russell fait écho à certaines des préoccupations de Bostrom tout en proposant une approche pour développer des machines prouvées bénéfiques axées sur l'incertitude et la déférence envers les humains, impliquant éventuellement un apprentissage par renforcement inverse.
L'opinion des experts dans le domaine de l'intelligence artificielle est mitigée, avec des fractions importantes à la fois concernées et non concernées par le risque d'une éventuelle IA aux capacités surhumaines.
Le PDG de Facebook, Mark Zuckerberg, estime que l'IA "débloquera une énorme quantité de choses positives", telles que la guérison des maladies et l'augmentation de la sécurité des voitures autonomes.
Musk finance également des entreprises développant l'intelligence artificielle telles que DeepMind et Vicarious pour "garder un œil sur ce qui se passe avec l'intelligence artificielle".
La recherche dans ce domaine comprend l'éthique des machines, les agents moraux artificiels, l'IA conviviale et des discussions sur la construction d'un cadre des droits de l'homme sont également en cours.
Le moment est venu d'ajouter une dimension éthique à au moins certaines machines.
La recherche en éthique des machines est essentielle pour apaiser les inquiétudes concernant les systèmes autonomes - on pourrait soutenir que la notion de machines autonomes sans une telle dimension est à l'origine de toutes les craintes concernant l'intelligence des machines.
Les humains ne devraient pas supposer que les machines ou les robots nous traiteraient favorablement car il n'y a aucune raison a priori de croire qu'ils seraient sympathiques à notre système de moralité, qui a évolué avec notre biologie particulière (que les IA ne partageraient pas).
Une proposition pour faire face à cela est de s'assurer que la première IA généralement intelligente est « l'IA conviviale » et qu'elle sera capable de contrôler les IA développées par la suite.
Je pense que l'inquiétude provient d'une erreur fondamentale en ne faisant pas la différence entre les avancées récentes très réelles dans un aspect particulier de l'IA et l'énormité et la complexité de la construction d'une intelligence volitionnelle sensible."
La réglementation est jugée nécessaire à la fois pour encourager l'IA et gérer les risques associés.
Un trope commun dans ces œuvres a commencé avec Frankenstein de Mary Shelley, où une création humaine devient une menace pour ses maîtres.
Isaac Asimov a introduit les trois lois de la robotique dans de nombreux livres et histoires, notamment la série "Multivac" sur un ordinateur super intelligent du même nom.
Dans les années 1980, la série Sexy Robots de l'artiste Hajime Sorayama a été peinte et publiée au Japon illustrant la forme humaine organique réelle avec des peaux métalliques musclées réalistes et plus tard, le livre "the Gynoids" a suivi qui a été utilisé ou influencé par des cinéastes, dont George Lucas et d'autres créatifs.
La biotechnologie est un vaste domaine de la biologie, impliquant l'utilisation de systèmes et d'organismes vivants pour développer ou fabriquer des produits.
L'American Chemical Society définit la biotechnologie comme l'application d'organismes, de systèmes ou de processus biologiques par diverses industries à l'apprentissage de la science de la vie et à l'amélioration de la valeur des matériaux et des organismes tels que les produits pharmaceutiques, les cultures et le bétail.
La bio-ingénierie est l'application des principes de l'ingénierie et des sciences naturelles aux tissus, cellules et molécules.
Grâce à la biotechnologie précoce, les premiers agriculteurs ont sélectionné et élevé les cultures les mieux adaptées, ayant les rendements les plus élevés, pour produire suffisamment de nourriture pour soutenir une population croissante.
Ces processus ont également été inclus dans la fermentation précoce de la bière.
Au cours de ce processus, les glucides contenus dans les grains se sont décomposés en alcools, tels que l'éthanol.
Bien que le processus de fermentation n'ait pas été entièrement compris jusqu'aux travaux de Louis Pasteur en 1857, il s'agit toujours de la première utilisation de la biotechnologie pour convertir une source de nourriture en une autre forme.
Ces récits ont contribué à la théorie de la sélection naturelle de Darwin.
En 1928, Alexander Fleming découvre la moisissure Penicillium.
Le MOSFET (transistor à effet de champ métal-oxyde-semi-conducteur) a été inventé par Mohamed M. Atalla et Dawon Kahng en 1959.
Le premier BioFET était le transistor à effet de champ sensible aux ions (ISFET), inventé par Piet Bergveld en 1970.
Au milieu des années 1980, d'autres BioFET avaient été développés, notamment le capteur de gaz FET (GASFET), le capteur de pression FET (PRESSFET), le transistor à effet de champ chimique (ChemFET), l'ISFET de référence (REFET), le FET à modification enzymatique (ENFET) et FET immunologiquement modifié (IMFET).
La demande croissante de biocarburants devrait être une bonne nouvelle pour le secteur de la biotechnologie, le ministère de l'Énergie estimant que l'utilisation d'éthanol pourrait réduire la consommation américaine de carburant dérivé du pétrole jusqu'à 30 % d'ici 2030.
TCE : L'ingénieur chimiste, (816), 26–31.
Un autre exemple est la conception de plantes transgéniques pour pousser dans des environnements spécifiques en présence (ou en l'absence) de produits chimiques.
D'autre part, certaines des utilisations de la biotechnologie verte impliquent des micro-organismes pour nettoyer et réduire les déchets.
Ainsi que le développement d'hormones, de cellules souches, d'anticorps, d'ARNsi et de tests de diagnostic.
Une application est la création de semences améliorées qui résistent aux conditions environnementales extrêmes des régions arides, qui est liée à l'innovation, à la création de techniques agricoles et à la gestion des ressources.
Le but de la pharmacogénomique est de développer des moyens rationnels pour optimiser la pharmacothérapie, en fonction du génotype des patients, afin d'assurer une efficacité maximale avec un minimum d'effets indésirables.
La biotechnologie moderne peut être utilisée pour fabriquer des médicaments existants relativement facilement et à moindre coût.
Les tests génétiques permettent le diagnostic génétique des vulnérabilités aux maladies héréditaires et peuvent également être utilisés pour déterminer la filiation d'un enfant (mère et père génétiques) ou en général l'ascendance d'une personne.
La plupart du temps, les tests sont utilisés pour trouver des changements associés à des troubles héréditaires.
Les entreprises de biotechnologie peuvent contribuer à la sécurité alimentaire future en améliorant la nutrition et la viabilité de l'agriculture urbaine.
10 % des terres cultivées dans le monde étaient plantées de cultures GM en 2010.
Ces techniques ont permis l'introduction de nouveaux traits de culture ainsi qu'un contrôle bien plus important sur la structure génétique d'un aliment que ce que permettaient auparavant des méthodes telles que l'élevage sélectif et l'élevage par mutation.
Ceux-ci ont été conçus pour résister aux agents pathogènes et aux herbicides et pour de meilleurs profils nutritionnels.
Néanmoins, les membres du public sont beaucoup moins susceptibles que les scientifiques de percevoir les aliments GM comme sûrs.
Cependant, les opposants se sont opposés aux cultures GM en soi pour plusieurs raisons, y compris les préoccupations environnementales, la question de savoir si les aliments produits à partir de cultures GM sont sûrs, si les cultures GM sont nécessaires pour répondre aux besoins alimentaires mondiaux et les préoccupations économiques soulevées par le fait que ces organismes sont soumis au droit de la propriété intellectuelle.
Il existe des différences dans la réglementation des OGM entre les pays, certaines des différences les plus marquées se produisant entre les États-Unis et l'Europe.
L'Union européenne établit une distinction entre l'approbation de la culture au sein de l'UE et l'approbation de l'importation et de la transformation.
Chaque candidature retenue est généralement financée pendant cinq ans, puis doit être renouvelée de manière compétitive.
Le clonage est le processus de production d'organismes individuels avec un ADN identique ou pratiquement identique, par des moyens naturels ou artificiels.
Il est utilisé dans un large éventail d'expériences biologiques et d'applications pratiques allant de l'empreinte génétique à la production de protéines à grande échelle.
Initialement, l'ADN d'intérêt doit être isolé pour fournir un segment d'ADN de taille appropriée.
Après la ligature, le vecteur avec l'insert d'intérêt est transfecté dans des cellules.
Une technique de culture tissulaire utile utilisée pour cloner des lignées distinctes de lignées cellulaires implique l'utilisation d'anneaux de clonage (cylindres).
Ce processus est également appelé « clonage de recherche » ou « clonage thérapeutique ».
Le clonage thérapeutique est réalisé en créant des cellules souches embryonnaires dans l'espoir de traiter des maladies telles que le diabète et la maladie d'Alzheimer.
La raison pour laquelle le SCNT est utilisé pour le clonage est que les cellules somatiques peuvent être facilement acquises et cultivées en laboratoire.
L'ovocyte réagira au noyau de la cellule somatique, de la même manière qu'il réagirait au noyau d'un spermatozoïde.
Les cellules somatiques pourraient être utilisées immédiatement ou stockées au laboratoire pour une utilisation ultérieure.
Cela crée un embryon unicellulaire.
Les embryons développés avec succès sont ensuite placés dans des receveurs de substitution, comme une vache ou un mouton dans le cas d'animaux de ferme.
Un autre avantage est que le SCNT est considéré comme une solution pour cloner des espèces en voie de disparition qui sont sur le point de disparaître.
Seuls trois de ces embryons ont survécu jusqu'à la naissance et un seul a survécu jusqu'à l'âge adulte.
Cependant, en 2014, les chercheurs signalaient des taux de réussite de clonage de sept à huit sur dix et en 2016, une société coréenne Sooam Biotech produisait 500 embryons clonés par jour.
La reproduction asexuée est un phénomène naturel chez de nombreuses espèces, y compris la plupart des plantes et certains insectes.
A titre d'exemple, certains cultivars européens de raisins représentent des clones qui ont été propagés pendant plus de deux millénaires.
De nombreux arbres, arbustes, vignes, fougères et autres plantes vivaces herbacées forment naturellement des colonies clonales.
Chez les plantes, la parthénogenèse signifie le développement d'un embryon à partir d'un ovule non fécondé et est un processus composant de l'apomixie.
De tels clones ne sont pas strictement identiques puisque les cellules somatiques peuvent contenir des mutations dans leur ADN nucléaire.
La séparation artificielle d'embryons ou le jumelage d'embryons, une technique qui crée des jumeaux monozygotes à partir d'un seul embryon, n'est pas considérée de la même manière que d'autres méthodes de clonage.
L'embryon de Dolly a été créé en prenant la cellule et en l'insérant dans un ovule de mouton.
Elle a été clonée à l'Institut Roslin en Écosse par les scientifiques britanniques Sir Ian Wilmut et Keith Campbell et y a vécu de sa naissance en 1996 jusqu'à sa mort en 2003, alors qu'elle avait six ans.
Dolly était d'importance publique parce que l'effort a montré que le matériel génétique d'une cellule adulte spécifique, conçue pour exprimer uniquement un sous-ensemble distinct de ses gènes, peut être repensé pour développer un organisme entièrement nouveau.
Le premier clonage de mammifère (qui a abouti à Dolly la brebis) a eu un taux de réussite de 29 embryons pour 277 œufs fécondés, ce qui a produit trois agneaux à la naissance, dont l'un a survécu.
Notamment, bien que les premiers clones aient été des grenouilles, aucune grenouille clonée adulte n'a encore été produite à partir d'une cellule somatique adulte donneuse de noyau.
Cependant, d'autres chercheurs, dont Ian Wilmut qui a dirigé l'équipe qui a réussi à cloner Dolly, soutiennent que la mort prématurée de Dolly due à une infection respiratoire n'était pas liée à des problèmes liés au processus de clonage.
Les scientifiques soviétiques Chaylakhyan, Veprencev, Sviridova et Nikitin ont fait cloner la souris "Masha".
Plus proche de la formation artificielle de jumeaux.
Chien : Snuppy, un lévrier afghan mâle a été le premier chien cloné (2005).
Buffle d'eau : Samrupa a été le premier buffle d'eau cloné.
Chameau : (2009) Injaz, est le premier chameau cloné.
Chèvre : (2001) Des scientifiques de la Northwest A&F University ont cloné avec succès la première chèvre qui utilise la cellule femelle adulte.
Menée en Chine en 2017 et rapportée en janvier 2018.
Furet à pieds noirs : (2020) En 2020, une équipe de scientifiques a cloné une femelle nommée Willa, décédée au milieu des années 1980 et n'a laissé aucun descendant vivant.
Il ne fait pas référence à la conception naturelle et à l'accouchement de jumeaux identiques.
À l'heure actuelle, les scientifiques n'ont pas l'intention d'essayer de cloner des personnes et ils pensent que leurs résultats devraient susciter une discussion plus large sur les lois et réglementations dont le monde a besoin pour réglementer le clonage.
Alors que bon nombre de ces points de vue sont d'origine religieuse, les questions soulevées par le clonage sont également confrontées à des perspectives laïques.
Les opposants au clonage craignent que la technologie ne soit pas encore suffisamment développée pour être sûre et qu'elle puisse être sujette à des abus (conduisant à la génération d'humains dont les organes et les tissus seraient prélevés), ainsi que des inquiétudes quant à la manière dont les individus clonés pourraient s'intégrer avec les familles et avec la société en général.
Ceci est également appelé "clonage de conservation".
Ces succès ont permis d'espérer que des techniques similaires (utilisant des mères porteuses d'une autre espèce) pourraient être utilisées pour cloner des espèces éteintes.
En 2002, des généticiens de l'Australian Museum ont annoncé avoir répliqué l'ADN du thylacine (tigre de Tasmanie), alors éteint depuis environ 65 ans, en utilisant la réaction en chaîne par polymérase.
En 2003, pour la première fois, un animal disparu, le bouquetin des Pyrénées mentionné ci-dessus, a été cloné, au Centre de technologie et de recherche alimentaires d'Aragon, en utilisant le noyau cellulaire congelé conservé des échantillons de peau de 2001 et des ovules de chèvre domestique.
" Когда вернутся мамонты ("Quand les mammouths reviennent"), 5 février 2015 (consulté le 6 septembre 2015) Un autre problème est la survie du mammouth reconstruit : les ruminants dépendent d'une symbiose avec un microbiote spécifique dans leur estomac pour la digestion.
Pour cette raison, certains ont avancé qu'elle avait peut-être vieilli plus rapidement que d'autres animaux nés naturellement, car elle est morte relativement tôt pour un mouton à l'âge de six ans.
Cependant, les pertes précoces de grossesse et les pertes néonatales sont encore plus importantes avec le clonage qu'avec la conception naturelle ou la procréation assistée (FIV).
Le concept de clonage, en particulier le clonage humain, a présenté une grande variété d'œuvres de science-fiction.
De nombreux travaux décrivent la création artificielle d'humains par une méthode de culture de cellules à partir d'un tissu ou d'un échantillon d'ADN; la réplication peut être instantanée ou avoir lieu par le biais d'une croissance lente d'embryons humains dans des utérus artificiels.
Des films de science-fiction tels que The Matrix et Star Wars: Episode II - Attack of the Clones ont présenté des scènes de fœtus humains cultivés à l'échelle industrielle dans des réservoirs mécaniques.
A Number a été adapté par Caryl Churchill pour la télévision, dans une coproduction entre la BBC et HBO Films.
Elle a grandi en doutant toujours de l'amour de sa mère, qui ne lui ressemblait en rien et qui est décédée neuf ans auparavant.
Dans le roman d' Ira Levin de 1976 Les garçons du Brésil et son adaptation cinématographique de 1978, Josef Mengele utilise le clonage pour créer des copies d' Adolf Hitler .
Dans Doctor Who , une race extraterrestre d'êtres guerriers en armure appelés Sontarans a été introduite dans la série de 1973 " The Time Warrior ".
Le concept de soldats clonés élevés pour le combat a été revisité dans " The Doctor's Daughter " (2008), lorsque l'ADN du docteur est utilisé pour créer une guerrière appelée Jenny.
Le roman Never Let Me Go de 2005 de Kazuo Ishiguro et l'adaptation cinématographique de 2010 se déroulent dans une histoire alternative dans laquelle des humains clonés sont créés dans le seul but de fournir des dons d'organes à des humains nés naturellement, malgré le fait qu'ils soient pleinement sensibles et autonomes. conscient.
Dans le roman futuriste Cloud Atlas et le film qui a suivi, l'une des intrigues se concentre sur un clone de fabricant génétiquement modifié nommé Sonmi ~ 451, l'un des millions élevé dans un "wombtank" artificiel, destiné à servir dès la naissance.
Dans le film Us , à un moment donné avant les années 1980, le gouvernement américain crée des clones de chaque citoyen des États-Unis avec l'intention de les utiliser pour contrôler leurs homologues d'origine, semblables à des poupées vaudou.
De nos jours, les clones lancent une attaque surprise et parviennent à achever un génocide de masse de leurs homologues inconscients.
Des gènes ont été transférés au sein d'une même espèce, d'une espèce à l'autre (créant des organismes transgéniques) et même d'un règne à l'autre.
Les ingénieurs génétiques doivent isoler le gène qu'ils souhaitent insérer dans l'organisme hôte et le combiner avec d'autres éléments génétiques, y compris une région promotrice et terminatrice et souvent un marqueur sélectionnable.
Herbert Boyer et Stanley Cohen ont créé le premier organisme génétiquement modifié en 1973, une bactérie résistante à l'antibiotique kanamycine.
Le premier animal génétiquement modifié à être commercialisé était le GloFish (2003) et le premier animal génétiquement modifié à être approuvé pour une utilisation alimentaire était le saumon AquAdvantage en 2015.
Les champignons ont été conçus avec à peu près les mêmes objectifs.
Il existe des propositions pour supprimer les gènes virulents des virus afin de créer des vaccins.
La majorité est conçue pour la tolérance aux herbicides ou la résistance aux insectes.
Les animaux sont généralement beaucoup plus difficiles à transformer et la grande majorité en est encore au stade de la recherche.
Le bétail est modifié dans le but d'améliorer des caractéristiques économiquement importantes telles que le taux de croissance, la qualité de la viande, la composition du lait, la résistance aux maladies et la survie.
Bien que la thérapie génique humaine soit encore relativement nouvelle, elle a été utilisée pour traiter des troubles génétiques tels que l'immunodéficience combinée sévère et l'amaurose congénitale de Leber.
D'autres préoccupations sont l'objectivité et la rigueur des autorités réglementaires, la contamination des aliments non génétiquement modifiés, le contrôle de l'approvisionnement alimentaire, le brevetage du vivant et l'utilisation des droits de propriété intellectuelle.
Les pays ont adopté des mesures réglementaires pour faire face à ces préoccupations.
Une définition large du génie génétique inclut également l'élevage sélectif et d'autres moyens de sélection artificielle.",
Par exemple, le triticale céréalier a été entièrement développé dans un laboratoire en 1930 en utilisant diverses techniques pour modifier son génome.
La biotechnologie moderne est en outre définie comme "des techniques d'acide nucléique in vitro, y compris l'acide désoxyribonucléique recombinant (ADN) et l'injection directe d'acide nucléique dans des cellules ou des organites, ou la fusion de cellules au-delà de la famille taxonomique".
Les définitions se concentrent davantage sur le processus que sur le produit, ce qui signifie qu'il pourrait y avoir des OGM et des non-OGM avec des génotypes et des phénotypes très similaires.
Elle pose également des problèmes au fur et à mesure que de nouveaux procédés sont développés.
Les ingénieurs génétiques doivent isoler le gène qu'ils souhaitent insérer dans l'organisme hôte.
Le gène est ensuite combiné avec d'autres éléments génétiques, y compris une région promotrice et terminatrice et un marqueur sélectionnable.
L'ADN est généralement inséré dans des cellules animales par micro-injection, où il peut être injecté à travers l'enveloppe nucléaire de la cellule directement dans le noyau, ou par l'utilisation de vecteurs viraux.
Chez les plantes, cela se fait par culture de tissus.
Traditionnellement, le nouveau matériel génétique était inséré au hasard dans le génome de l'hôte.
Il existe quatre familles de nucléases modifiées : les méganucléases, les nucléases à doigts de zinc, les nucléases effectrices de type activateur de transcription (TALEN) et le système Cas9-guideRNA (adapté de CRISPR).
En 1972, Paul Berg a créé la première molécule d'ADN recombinant en combinant l'ADN d'un virus de singe avec celui du virus lambda.
Les bactéries qui avaient réussi à incorporer le plasmide ont alors pu survivre en présence de kanamycine.
En 1974, Rudolf Jaenisch a créé une souris transgénique en introduisant de l'ADN étranger dans son embryon, ce qui en a fait le premier animal transgénique au monde.
Des souris dont les gènes ont été supprimés (appelées souris knock-out) ont été créées en 1989.
En 1983, la première plante génétiquement modifiée a été développée par Michael W. Bevan, Richard B. Flavell et Mary-Dell Chilton.
En 2000, le riz doré enrichi en vitamine A a été la première plante développée avec une valeur nutritive accrue.
L'insuline produite par des bactéries, appelée humuline, a été approuvée par la Food and Drug Administration en 1982.
En 1994, Calgene a obtenu l'autorisation de commercialiser la tomate Flavr Savr, le premier aliment génétiquement modifié.
En 2010, des scientifiques du J. Craig Venter Institute ont annoncé qu'ils avaient créé le premier génome bactérien synthétique.
Il est sorti sur le marché américain en 2003.
Des gènes et d'autres informations génétiques provenant d'un large éventail d'organismes peuvent être ajoutés à un plasmide et insérés dans des bactéries pour le stockage et la modification.
Un grand nombre de plasmides personnalisés rendent la manipulation de l'ADN extrait de bactéries relativement facile.
Les scientifiques peuvent facilement manipuler et combiner des gènes au sein des bactéries pour créer des protéines nouvelles ou perturbées et observer l'effet que cela a sur divers systèmes moléculaires.
Les bactéries sont utilisées depuis longtemps dans la production d'aliments et des souches spécifiques ont été développées et sélectionnées pour ce travail à l'échelle industrielle.
La plupart des bactéries productrices d'aliments sont des bactéries lactiques, et c'est là que la majorité des recherches sur les bactéries productrices d'aliments génétiquement modifiées sont allées.
La majorité est produite aux États-Unis et même si des réglementations sont en place pour autoriser la production en Europe, depuis 2015, aucun produit alimentaire dérivé de bactéries n'y est actuellement disponible.
Les bactéries sont ensuite récoltées et la protéine souhaitée purifiée à partir de celles-ci.
Beaucoup de ces protéines sont impossibles ou difficiles à obtenir par des méthodes naturelles et elles sont moins susceptibles d'être contaminées par des agents pathogènes, ce qui les rend plus sûres.
En dehors de la médecine, ils ont été utilisés pour produire des biocarburants.
Les idées incluent la modification des bactéries intestinales afin qu'elles détruisent les bactéries nocives, ou l'utilisation de bactéries pour remplacer ou augmenter les enzymes ou les protéines déficientes.
Permettre aux bactéries de former une colonie pourrait fournir une solution à plus long terme, mais pourrait également soulever des problèmes de sécurité car les interactions entre les bactéries et le corps humain sont moins bien comprises qu'avec les médicaments traditionnels.
Depuis plus d'un siècle, les bactéries sont utilisées en agriculture.
Avec les progrès du génie génétique, ces bactéries ont été manipulées pour une efficacité accrue et une gamme d'hôtes élargie.
Les souches de bactéries Pseudomonas provoquent le gel dabame en nucléant l'eau dans les cristaux de glace autour d'eux.
D'autres utilisations des bactéries génétiquement modifiées comprennent la bioremédiation, où les bactéries sont utilisées pour convertir les polluants en une forme moins toxique.
Dans les années 1980, l'artiste Jon Davis et la généticienne Dana Boyd ont converti le symbole germanique de la féminité ( ᛉ ) en code binaire puis en une séquence d'ADN, qui a ensuite été exprimée dans Escherichia coli.
Les chercheurs peuvent l'utiliser pour contrôler divers facteurs; y compris l'emplacement cible, la taille de l'insert et la durée de l'expression du gène.
Bien qu'encore au stade des essais, certains succès ont été obtenus grâce à la thérapie génique pour remplacer les gènes défectueux.
Depuis 2018, un nombre important d'essais cliniques sont en cours, y compris des traitements pour l'hémophilie, le glioblastome, la maladie granulomateuse chronique, la fibrose kystique et divers cancers.
Les virus de l'herpès simplex constituent des vecteurs prometteurs, ayant une capacité de transport de plus de 30 kb et fournissant une expression à long terme, bien qu'ils soient moins efficaces pour la délivrance de gènes que d'autres vecteurs.
D'autres virus qui ont été utilisés comme vecteurs comprennent les alphavirus, les flavivirus, les virus de la rougeole, les rhabdovirus, le virus de la maladie de Newcastle, les poxvirus et les picornavirus.
Cela n'affecte pas l'infectivité des virus, invoque une réponse immunitaire naturelle et il n'y a aucune chance qu'ils retrouvent leur fonction de virulence, ce qui peut se produire avec certains autres vaccins.
Le vaccin le plus efficace contre la Tuberculose, le vaccin Bacillus Calmette-Guérin (BCG), n'offre qu'une protection partielle.
D'autres vaccins à base de vecteurs ont déjà été approuvés et bien d'autres sont en cours de développement.
En 2004, des chercheurs ont rapporté qu'un virus génétiquement modifié qui exploite le comportement égoïste des cellules cancéreuses pourrait offrir une autre façon de tuer les tumeurs.
Le virus a été injecté dans des orangers pour lutter contre la maladie du verdissement des agrumes qui avait réduit la production d'oranges de 70 % depuis 2005.
Des virus génétiquement modifiés qui rendent les animaux cibles infertiles par immunocontraception ont été créés en laboratoire ainsi que d'autres qui ciblent le stade de développement de l'animal.
La modification génétique du virus du myxome a été proposée pour conserver les lapins sauvages européens dans la péninsule ibérique et pour aider à les réglementer en Australie.
Il est possible de concevoir des bactériophages pour qu'ils expriment des protéines modifiées à leur surface et les associent selon des schémas spécifiques (une technique appelée phage display).
Pour les applications industrielles, les levures combinent les avantages bactériens d'être un organisme unicellulaire facile à manipuler et à cultiver avec les modifications protéiques avancées trouvées chez les eucaryotes.
L'un a augmenté l'efficacité de la fermentation malolactique, tandis que l'autre empêche la production de composés dangereux de carbamate d'éthyle pendant la fermentation.
Contrairement aux bactéries et aux virus, ils ont l'avantage d'infecter les insectes par simple contact, bien qu'ils soient surpassés en efficacité par les pesticides chimiques.
Les moustiques, vecteurs de toute une gamme de maladies mortelles, notamment le paludisme, la fièvre jaune et la dengue, constituent une cible attrayante pour la lutte biologique.
Une autre stratégie consiste à ajouter des protéines aux champignons qui bloquent la transmission du paludisme ou à éliminer complètement le Plasmodium.
De nombreuses plantes sont pluripotentes, ce qui signifie qu'une seule cellule d'une plante mature peut être récoltée et, dans de bonnes conditions, peut se développer en une nouvelle plante.
Des avancées majeures dans la culture tissulaire et les mécanismes cellulaires végétaux pour un large éventail de plantes proviennent de systèmes développés dans le tabac.
Arabidopsis thaliana est un autre organisme modèle majeur pertinent pour le génie génétique.
En recherche, les plantes sont modifiées pour aider à découvrir les fonctions de certains gènes.
Contrairement à la mutagenèse, le génie génétique permet une élimination ciblée sans perturber les autres gènes de l'organisme.
D'autres stratégies consistent à attacher le gène à un promoteur puissant et à voir ce qui se passe lorsqu'il est surexprimé, forçant un gène à s'exprimer à un endroit différent ou à différents stades de développement.
Les premières plantes ornementales génétiquement modifiées ont commercialisé une couleur altérée.
D'autres plantes ornementales génétiquement modifiées comprennent le chrysanthème et le pétunia.
Le virus de la tache annulaire de la papaye a dévasté les papayers à Hawaï au XXe siècle jusqu'à ce que les plants de papaye transgéniques obtiennent une résistance dérivée des agents pathogènes.
La deuxième génération de cultures visait à améliorer la qualité, souvent en modifiant le profil nutritionnel.
Les cultures GM contribuent en améliorant les récoltes en réduisant la pression des insectes, en augmentant la valeur nutritive et en tolérant différents stress abiotiques.
La majorité des cultures GM ont été modifiées pour résister à certains herbicides, généralement à base de glyphosate ou de glufosinate.
Quelques-uns utilisent les gènes qui codent pour les protéines insecticides végétatives.
Moins d'un pour cent des cultures GM contenaient d'autres traits, notamment la résistance aux virus, le retardement de la sénescence et la modification de la composition des plantes.
Les plantes et les cellules végétales ont été génétiquement modifiées pour la production de produits biopharmaceutiques dans des bioréacteurs, un processus connu sous le nom de pharming.
De nombreux médicaments contiennent également des ingrédients végétaux naturels et les voies qui mènent à leur production ont été génétiquement modifiées ou transférées à d'autres espèces végétales pour produire un plus grand volume.
Ils présentent également moins de risques d'être contaminés.
Les vaccins sont coûteux à produire, à transporter et à administrer, donc avoir un système qui pourrait les produire localement permettrait un meilleur accès aux zones les plus pauvres et en développement.
Le stockage dans des usines réduit le coût à long terme car ils peuvent être disséminés sans avoir besoin d'un stockage à froid, n'ont pas besoin d'être purifiés et ont une stabilité à long terme.
En 2018, seuls trois animaux génétiquement modifiés ont été approuvés, tous aux États-Unis.
Canada : Brainwaveing Les premiers mammifères transgéniques ont été produits en injectant de l'ADN viral dans des embryons, puis en implantant les embryons chez des femelles.
Le développement du système d'édition de gènes CRISPR-Cas9 comme moyen bon marché et rapide de modifier directement les cellules germinales, réduisant de moitié le temps nécessaire pour développer des mammifères génétiquement modifiés.
Les souris génétiquement modifiées sont les mammifères les plus couramment utilisés dans la recherche biomédicale, car elles sont bon marché et faciles à manipuler.
En 2009, des scientifiques ont annoncé qu'ils avaient réussi pour la première fois à transférer un gène dans une espèce de primate (ouistitis).
Une expression stable a été obtenue chez les moutons, les porcs, les rats et d'autres animaux.
L'alpha-1-antitrypsine humaine est une autre protéine qui a été produite à partir de chèvres et est utilisée dans le traitement des humains atteints de cette déficience.
Des poumons de porc provenant de porcs génétiquement modifiés sont envisagés pour une transplantation chez l'homme.
Les animaux ont été conçus pour grandir plus vite, être en meilleure santé et résister aux maladies.
Un porc GM appelé Enviropig a été créé avec la capacité de digérer le phosphore végétal plus efficacement que les porcs conventionnels.
Cela pourrait potentiellement profiter aux mères qui ne peuvent pas produire de lait maternel mais qui souhaitent que leurs enfants aient du lait maternel plutôt que du lait maternisé.
Il a été suggéré que le génie génétique pourrait être utilisé pour ramener les animaux de l'extinction.
Il a été utilisé pour traiter des troubles génétiques tels que l'immunodéficience combinée sévère et l'amaurose congénitale de Leber.
La thérapie génique germinale rend tout changement héréditaire, ce qui a suscité des inquiétudes au sein de la communauté scientifique.
L'aquaculture est une industrie en pleine croissance, fournissant actuellement plus de la moitié du poisson consommé dans le monde.
Plusieurs groupes ont développé des poissons zèbres pour détecter la pollution en attachant des protéines fluorescentes à des gènes activés par la présence de polluants.
Il a été développé à l'origine par l'un des groupes pour détecter la pollution, mais fait maintenant partie du commerce des poissons d'ornement, devenant le premier animal génétiquement modifié à devenir accessible au public en tant qu'animal de compagnie lorsqu'il a été mis en vente aux États-Unis en 2003.
Les poissons zèbres sont des organismes modèles pour les processus de développement, la régénération, la génétique, le comportement, les mécanismes de la maladie et les tests de toxicité.
Les poissons génétiquement modifiés ont été développés avec des promoteurs entraînant une surproduction d'hormone de croissance à utiliser dans l'industrie aquacole pour augmenter la vitesse de développement et potentiellement réduire la pression de la pêche sur les stocks sauvages.
Il a obtenu l'approbation réglementaire en 2015, le premier aliment OGM non végétal à être commercialisé.
La drosophile a été utilisée pour étudier la génétique et l'hérédité, le développement embryonnaire, l'apprentissage, le comportement et le vieillissement.
Des moustiques résistants au paludisme ont été développés en laboratoire en insérant un gène qui réduit le développement du parasite du paludisme, puis en utilisant des endonucléases de homing pour propager rapidement ce gène dans la population masculine (connu sous le nom de forçage génétique).
Une autre approche consiste à utiliser une technique d'insectes stériles, dans laquelle les mâles génétiquement modifiés pour être stériles rivalisent avec les mâles viables, afin de réduire le nombre de populations.
L'approche est similaire à la technique stérile testée sur les moustiques, où les mâles sont transformés avec un gène qui empêche toute femelle née d'atteindre la maturité.
Dans ce cas, une souche de ver rose de la capsule qui a été stérilisée par rayonnement a été génétiquement modifiée pour exprimer une protéine fluorescente rouge, ce qui permet aux chercheurs de les surveiller plus facilement.
Il est également possible d'utiliser les machines de production de soie pour fabriquer d'autres protéines précieuses.
Un poulet GM qui produit le médicament Kanuma, une enzyme qui traite une maladie rare, dans son œuf a passé l'approbation réglementaire américaine en 2015.
Il existe des propositions d'utilisation du génie génétique pour contrôler les crapauds de canne en Australie.
Il est également relativement facile de produire des nématodes transgéniques stables et cela, avec l'ARNi, sont les principaux outils utilisés pour étudier leurs gènes.
Les nématodes transgéniques ont été utilisés pour étudier les virus, la toxicologie, les maladies et pour détecter les polluants environnementaux.
Les vers plats ont la capacité de se régénérer à partir d'une seule cellule.
Le ver à soies, un annélide marin, a été modifié.
Le développement d'un cadre réglementaire concernant le génie génétique a commencé en 1975, à Asilomar, en Californie.
C'est un traité international qui régit le transfert, la manipulation et l'utilisation des organismes génétiquement modifiés.
De nombreuses expériences nécessitent également l'autorisation d'un groupe de réglementation national ou d'une législation.
Il existe un système quasi universel pour évaluer les risques relatifs associés aux OGM et autres agents pour le personnel de laboratoire et la communauté.
Différents pays utilisent une nomenclature différente pour décrire les niveaux et peuvent avoir des exigences différentes pour ce qui peut être fait à chaque niveau.
Par exemple, une culture non destinée à un usage alimentaire n'est généralement pas examinée par les autorités responsables de la sécurité sanitaire des aliments.
La plupart des pays qui n'autorisent pas la culture d'OGM autorisent la recherche utilisant des OGM.
Alors que seuls quelques OGM ont été approuvés pour la culture dans l'UE, un certain nombre d'OGM ont été approuvés pour l'importation et la transformation.
La politique des États-Unis ne se concentre pas autant sur le processus que d'autres pays, examine les risques scientifiques vérifiables et utilise le concept d'équivalence substantielle.
L'une des principales questions concernant les régulateurs est de savoir si les produits GM doivent être étiquetés.
Le différend implique des consommateurs, des producteurs, des sociétés de biotechnologie, des régulateurs gouvernementaux, des organisations non gouvernementales et des scientifiques.
La plupart des préoccupations concernent les effets des OGM sur la santé et l'environnement.
Néanmoins, les membres du public sont beaucoup moins susceptibles que les scientifiques de percevoir les aliments GM comme sûrs.
Le flux de gènes entre les cultures GM et les plantes compatibles, ainsi que l'utilisation accrue d'herbicides à large spectre, peuvent augmenter le risque de populations de mauvaises herbes résistantes aux herbicides.
Afin de répondre à certaines de ces préoccupations, certains OGM ont été développés avec des traits pour aider à contrôler leur propagation.
D'autres préoccupations environnementales et agronomiques incluent une diminution de la biodiversité, une augmentation des ravageurs secondaires (ravageurs non ciblés) et l'évolution des insectes ravageurs résistants.
L'impact des cultures Bt sur les organismes bénéfiques non ciblés est devenu un problème public après qu'un article de 1999 a suggéré qu'elles pourraient être toxiques pour les papillons monarques.
Avec la capacité de modifier génétiquement les humains maintenant possible, il y a des préoccupations éthiques quant à savoir jusqu'où cette technologie devrait aller, ou si elle devrait être utilisée du tout.
Octobre 2006 la rigueur du processus réglementaire, la consolidation du contrôle de l'approvisionnement alimentaire dans les entreprises qui fabriquent et vendent des OGM, l'exagération des bénéfices de la modification génétique ou encore les inquiétudes sur l'utilisation d'herbicides au glyphosate.
Les OGM sont arrivés sur la scène alors que la confiance du public dans la sécurité alimentaire, attribuée aux récentes crises alimentaires telles que l'encéphalopathie spongiforme bovine et d'autres scandales impliquant la réglementation gouvernementale des produits en Europe, était faible.
Le génie génétique, également appelé modification génétique ou manipulation génétique, est la manipulation directe des gènes d'un organisme à l'aide de la biotechnologie.
Une construction est généralement créée et utilisée pour insérer cet ADN dans l'organisme hôte.
Le nouvel ADN peut être inséré de manière aléatoire ou ciblé sur une partie spécifique du génome.
Rudolf Jaenisch a créé le premier animal GM en insérant de l'ADN étranger dans une souris en 1974.
Les aliments génétiquement modifiés sont commercialisés depuis 1994, avec la sortie de la tomate Flavr Savr.
En 2016, des saumons modifiés avec une hormone de croissance ont été vendus.
En éliminant les gènes responsables de certaines conditions, il est possible de créer des organismes modèles animaux de maladies humaines.
L'essor des cultures génétiquement modifiées commercialisées a procuré des avantages économiques aux agriculteurs de nombreux pays différents, mais a également été à l'origine de la plupart des controverses entourant la technologie.
Le flux de gènes, l'impact sur les organismes non ciblés, le contrôle de l'approvisionnement alimentaire et les droits de propriété intellectuelle ont également été soulevés comme des problèmes potentiels.
Ceci est beaucoup plus rapide, peut être utilisé pour insérer n'importe quel gène de n'importe quel organisme (même ceux de différents domaines) et empêche l'ajout d'autres gènes indésirables.
Des médicaments, des vaccins et d'autres produits ont été récoltés à partir d'organismes conçus pour les produire.
La biologie synthétique est une discipline émergente qui pousse le génie génétique un peu plus loin en introduisant du matériel synthétisé artificiellement dans un organisme.
Si du matériel génétique d'une autre espèce est ajouté à l'hôte, l'organisme résultant est appelé transgénique.
En 1973, Herbert Boyer et Stanley Cohen ont créé le premier organisme transgénique en insérant des gènes de résistance aux antibiotiques dans le plasmide d'une bactérie Escherichia coli.
En 1976, Genentech, la première société de génie génétique, a été fondée par Herbert Boyer et Robert Swanson et un an plus tard, la société a produit une protéine humaine (somatostatine) dans E. coli.
L'insuline produite par les bactéries a été approuvée par la Food and Drug Administration (FDA) en 1982.
La République populaire de Chine a été le premier pays à commercialiser des plantes transgéniques, introduisant un tabac résistant aux virus en 1992.
En 1995, la pomme de terre Bt a été approuvée par l'Environmental Protection Agency, après avoir été approuvée par la FDA, ce qui en fait la première culture productrice de pesticides à être approuvée aux États-Unis.
Des cribles génétiques peuvent être effectués pour déterminer les gènes potentiels et d'autres tests ensuite utilisés pour identifier les meilleurs candidats.
Ces segments peuvent ensuite être extraits par électrophorèse sur gel.
Une fois isolé, le gène est ligaturé dans un plasmide qui est ensuite inséré dans une bactérie.
Celles-ci comprennent une région promotrice et une région de terminaison, qui initient et terminent la transcription.
Cette capacité peut être induite chez d'autres bactéries via un stress (par exemple un choc thermique ou électrique), ce qui augmente la perméabilité de la membrane cellulaire à l'ADN ; L'ADN absorbé peut soit s'intégrer au génome, soit exister sous forme d'ADN extrachromosomique.
Dans les plantes, l'ADN est souvent inséré à l'aide d'une transformation médiée par Agrobacterium, tirant parti de la séquence d'ADN-T d'Agrobacterium qui permet l'insertion naturelle de matériel génétique dans les cellules végétales.
Chez les plantes, cela se fait grâce à l'utilisation de la culture tissulaire.
Des marqueurs sélectionnables sont utilisés pour différencier facilement les cellules transformées des cellules non transformées.
Ces tests peuvent également confirmer l'emplacement chromosomique et le nombre de copies du gène inséré.
Le nouveau matériel génétique peut être inséré de manière aléatoire dans le génome de l'hôte ou ciblé vers un emplacement spécifique.
La fréquence du ciblage des gènes peut être grandement améliorée grâce à l'édition du génome.
TALEN et CRISPR sont les deux plus couramment utilisés et chacun a ses propres avantages.
La plupart des OGM commercialisés sont des plantes cultivées résistantes aux insectes ou tolérantes aux herbicides.
Les hybridomes de souris, des cellules fusionnées pour créer des anticorps monoclonaux, ont été adaptés par génie génétique pour créer des anticorps monoclonaux humains.
Le génie génétique est également utilisé pour créer des modèles animaux de maladies humaines.
Des remèdes potentiels peuvent être testés contre ces modèles de souris.
En 2015, un virus a été utilisé pour insérer un gène sain dans les cellules de la peau d'un garçon souffrant d'une maladie cutanée rare, l'épidermolyse bulleuse, afin de se développer, puis de greffer une peau saine sur 80 % du corps du garçon qui était affecté par le maladie.
On craint également que la technologie ne soit utilisée non seulement pour le traitement, mais pour l'amélioration, la modification ou l'altération de l'apparence, de l'adaptabilité, de l'intelligence, du caractère ou du comportement d'un être humain.
Il a dit que des jumelles, Lulu et Nana, étaient nées quelques semaines plus tôt.
Actuellement, la modification de la lignée germinale est interdite dans 40 pays.
Les bactéries sont bon marché, faciles à cultiver, clonales, se multiplient rapidement, sont relativement faciles à transformer et peuvent être conservées à -80 °C presque indéfiniment.
Cela pourrait être l'effet sur le phénotype de l'organisme, où le gène est exprimé ou avec quels autres gènes il interagit.
Dans un simple knock-out, une copie du gène souhaité a été modifiée pour le rendre non fonctionnel.
Cela permet à l'expérimentateur d'analyser les défauts causés par cette mutation et de déterminer ainsi le rôle de gènes particuliers.
La méthode la plus simple, et la première à être utilisée, est le "balayage d'alanine", où chaque position est à son tour mutée en l'alanine d'acide aminé non réactif.
Le processus est sensiblement le même que celui de l'ingénierie knock-out, sauf que la construction est conçue pour augmenter la fonction du gène, généralement en fournissant des copies supplémentaires du gène ou en induisant plus fréquemment la synthèse de la protéine.
Une façon d'y parvenir est de remplacer le gène de type sauvage par un gène de «fusion», qui est une juxtaposition du gène de type sauvage avec un élément de rapport tel que la protéine fluorescente verte (GFP) qui permettra une visualisation facile des produits de la modification génétique.
Les études d'expression visent à découvrir où et quand des protéines spécifiques sont produites.
Certains gènes ne fonctionnent pas bien dans les bactéries, donc des levures, des cellules d'insectes ou des cellules de mammifères peuvent également être utilisées.
Certains microbes génétiquement modifiés peuvent également être utilisés dans le biominage et la bioremédiation, en raison de leur capacité à extraire les métaux lourds de leur environnement et à les incorporer dans des composés plus facilement récupérables.
Des cultures résistantes aux champignons et aux virus ont également été développées ou sont en cours de développement.
En 2016, le saumon a été génétiquement modifié avec des hormones de croissance pour atteindre une taille adulte normale beaucoup plus rapidement.
Le soja et le canola ont été génétiquement modifiés pour produire des huiles plus saines.
Le transfert de gènes par des vecteurs viraux a été proposé comme moyen de contrôler les espèces envahissantes ainsi que de vacciner la faune menacée contre les maladies.
Les applications du génie génétique à la conservation sont jusqu'à présent essentiellement théoriques et doivent encore être mises en pratique.
La réunion d'Asilomar a recommandé un ensemble de directives volontaires concernant l'utilisation de la technologie recombinante.
Cent cinquante-sept pays sont membres du Protocole et beaucoup l'utilisent comme point de référence pour leurs propres réglementations.
La plupart des pays qui n'autorisent pas la culture d'OGM autorisent la recherche.
Emily Marden, Risk and Regulation: US Regulatory Policy on Genetically Modified Food and Agriculture, 44 BCL Rev. 733 (2003) L'Union européenne, en revanche, possède peut-être les réglementations sur les OGM les plus strictes au monde.
L'une des principales questions concernant les régulateurs est de savoir si les produits GM doivent être étiquetés.
Ces controverses ont entraîné des litiges, des différends commerciaux internationaux et des protestations, ainsi qu'une réglementation restrictive des produits commerciaux dans certains pays.
Bien que des doutes aient été soulevés, la plupart des études économiques ont montré que la culture de cultures GM était bénéfique pour les agriculteurs.
Bon nombre des impacts environnementaux concernant les cultures GM peuvent prendre de nombreuses années à être compris et sont également évidents dans les pratiques agricoles conventionnelles.
Peu de films ont informé le public sur le génie génétique, à l'exception de The Boys from Brazil de 1978 et de Jurassic Park de 1993, qui utilisaient tous deux une leçon, une démonstration et un extrait de film scientifique.
La nanotechnologie, également abrégée en nanotechnologie, est l'utilisation de la matière à l'échelle atomique, moléculaire et supramoléculaire à des fins industrielles.
Cette définition reflète le fait que les effets de la mécanique quantique sont importants à cette échelle du domaine quantique, et donc la définition est passée d'un objectif technologique particulier à une catégorie de recherche incluant tous les types de recherche et de technologies qui traitent des propriétés particulières de la matière qui se produisent en dessous du seuil de taille donné.
La recherche et les applications associées sont tout aussi diverses, allant des extensions de la physique conventionnelle des dispositifs à des approches complètement nouvelles basées sur l'auto-assemblage moléculaire, du développement de nouveaux matériaux avec des dimensions à l'échelle nanométrique au contrôle direct de la matière à l'échelle atomique.
Le terme "nano-technologie" a été utilisé pour la première fois par Norio Taniguchi en 1974, même s'il n'était pas largement connu.
L'émergence de la nanotechnologie en tant que domaine dans les années 1980 s'est produite grâce à la convergence des travaux théoriques et publics de Drexler, qui ont développé et popularisé un cadre conceptuel pour la nanotechnologie, et des avancées expérimentales à haute visibilité qui ont attiré une attention supplémentaire à grande échelle sur les perspectives du contrôle atomique de matière.
Les développeurs du microscope Gerd Binnig et Heinrich Rohrer du laboratoire de recherche IBM de Zurich ont reçu le prix Nobel de physique en 1986.
Le C60 n'a pas été initialement décrit comme une nanotechnologie ; le terme a été utilisé concernant les travaux ultérieurs avec des nanotubes de carbone apparentés (parfois appelés tubes de graphène ou tubes Bucky) qui suggéraient des applications potentielles pour l'électronique et les dispositifs à l'échelle nanométrique.
Des décennies plus tard, les progrès de la technologie multi-portes ont permis la mise à l'échelle des transistors à effet de champ métal-oxyde-semi-conducteur (MOSFET) jusqu'à des niveaux nanométriques inférieurs à 20 nm de longueur de grille, en commençant par le FinFET (transistor à effet de champ fin) , un MOSFET tridimensionnel non planaire à double grille.
Des controverses ont émergé concernant les définitions et les implications potentielles des nanotechnologies, illustrées par le rapport de la Royal Society sur les nanotechnologies.
Ces produits sont limités à des applications massives de nanomatériaux et n'impliquent pas de contrôle atomique de la matière.
Il était basé sur la technologie FinFET gate-all-around (GAA).
Cela couvre à la fois les travaux actuels et les concepts plus avancés.
La limite inférieure est fixée par la taille des atomes (l'hydrogène a les plus petits atomes, qui mesurent environ un quart de nm de diamètre cinétique) car la nanotechnologie doit construire ses dispositifs à partir d'atomes et de molécules.
Pour mettre cette échelle dans un autre contexte, la taille comparative d'un nanomètre à un mètre est la même que celle d'un marbre à la taille de la terre.
Dans l'approche "bottom-up", les matériaux et dispositifs sont construits à partir de composants moléculaires qui s'assemblent chimiquement selon des principes de reconnaissance moléculaire.
Un exemple est l'augmentation du rapport surface/volume modifiant les propriétés mécaniques, thermiques et catalytiques des matériaux.
L'activité catalytique des nanomatériaux ouvre également des risques potentiels dans leur interaction avec les biomatériaux.
Le concept de reconnaissance moléculaire est particulièrement important : les molécules peuvent être conçues de sorte qu'une configuration ou un arrangement spécifique soit favorisé en raison de forces intermoléculaires non covalentes.
De telles approches ascendantes devraient être capables de produire des dispositifs en parallèle et être beaucoup moins chères que les méthodes descendantes, mais pourraient potentiellement être dépassées à mesure que la taille et la complexité de l'assemblage souhaité augmentent.
La fabrication dans le cadre de nanosystèmes productifs n'est pas liée aux technologies conventionnelles utilisées pour fabriquer des nanomatériaux tels que les nanotubes de carbone et les nanoparticules et doit être clairement distinguée de celles-ci.
On espère que les développements de la nanotechnologie rendront possible leur construction par d'autres moyens, peut-être en utilisant des principes biomimétiques.
En général, il est très difficile d'assembler des dispositifs à l'échelle atomique, car il faut positionner des atomes sur d'autres atomes de taille et d'adhésivité comparables.
Cela a conduit à un échange de lettres dans la publication ACS Chemical & Engineering News en 2003.
Ils ont construit au moins trois dispositifs moléculaires distincts dont le mouvement est contrôlé depuis le bureau avec une tension variable : un nanomoteur à nanotubes, un actionneur moléculaire et un oscillateur à relaxation nanoélectromécanique.
Les nanomatériaux à transport rapide d'ions sont également liés à la nanoionique et à la nanoélectronique.
Des matériaux à l'échelle nanométrique tels que les nanopiliers sont parfois utilisés dans les cellules solaires, ce qui réduit le coût des cellules solaires au silicium traditionnelles.
Plus généralement, l'auto-assemblage moléculaire cherche à utiliser les concepts de la chimie supramoléculaire, et de la reconnaissance moléculaire en particulier, pour amener les composants d'une seule molécule à s'arranger automatiquement dans une conformation utile.
Les disques durs géants basés sur la bamnétorésistance déjà sur le marché correspondent à cette description, tout comme les techniques de dépôt de couche atomique (ALD).
Les faisceaux d'ions focalisés peuvent retirer directement de la matière, voire déposer de la matière lorsque des gaz précurseurs appropriés sont appliqués en même temps.
Ceux-ci pourraient ensuite être utilisés comme composants monomoléculaires dans un dispositif nanoélectronique.
La nanotechnologie moléculaire est une approche proposée qui consiste à manipuler des molécules uniques de manière finement contrôlée et déterministe.
Il y a des espoirs d'appliquer les nanorobots en médecine.
En raison de la nature discrète (c'est-à-dire atomique) de la matière et de la possibilité d'une croissance exponentielle, cette étape est considérée comme la base d'une autre révolution industrielle.
Avec la diminution de la dimensionnalité, une augmentation du rapport surface sur volume est observée.
Bien que conceptuellement similaires au microscope confocal à balayage développé par Marvin Minsky en 1961 et au microscope acoustique à balayage (SAM) développé par Calvin Quate et ses collègues dans les années 1970, les nouveaux microscopes à sonde à balayage ont une résolution beaucoup plus élevée, car ils ne sont pas limités par la longueur d'onde de son ou lumière.
Cependant, il s'agit encore d'un processus lent en raison de la faible vitesse de balayage du microscope.
Un autre groupe de techniques nanotechnologiques comprend celles utilisées pour la fabrication de nanotubes et de nanofils, celles utilisées dans la fabrication de semi-conducteurs telles que la lithographie ultraviolette profonde, la lithographie par faisceau d'électrons, l'usinage par faisceau ionique focalisé, la lithographie par nanoimpression, le dépôt de couche atomique et le dépôt moléculaire en phase vapeur, et comprenant en outre les techniques d'auto-assemblage moléculaire telles que celles mettant en oeuvre des copolymères diblocs.
La microscopie à sonde à balayage est une technique importante à la fois pour la caractérisation et la synthèse des nanomatériaux.
En utilisant, par exemple, une approche de balayage orientée fonctionnalité, des atomes ou des molécules peuvent être déplacés sur une surface avec des techniques de microscopie à sonde à balayage.
Ces techniques comprennent la synthèse chimique, l'auto-assemblage et l'assemblage positionnel.
Des chercheurs de Bell Telephone Laboratories comme John R. Arthur.
MBE permet aux scientifiques de déposer des couches d'atomes atomiquement précises et, ce faisant, de construire des structures complexes.
Les pansements sont infusés de nanoparticules d'argent pour guérir plus rapidement les coupures.
La nanotechnologie peut avoir la capacité de rendre les applications médicales existantes moins chères et plus faciles à utiliser dans des endroits comme le cabinet du médecin généraliste et à la maison.
Le platine est actuellement utilisé comme catalyseur de moteur diesel dans ces moteurs.
Ensuite, le catalyseur d'oxydation oxyde les hydrocarbures et le monoxyde de carbone pour former du dioxyde de carbone et de l'eau.
La société danoise InnovationsFonden a investi 15 millions de DKK dans la recherche de nouveaux substituts de catalyseur utilisant la nanotechnologie.
Si la surface du catalyseur qui est exposée aux gaz d'échappement est maximisée, l'efficacité du catalyseur est maximisée.
Ainsi, la création de ces nanoparticules augmentera l'efficacité du catalyseur de moteur diesel résultant, ce qui conduira à son tour à des gaz d'échappement plus propres, et réduira les coûts.
Lors de la conception d'échafaudages, les chercheurs tentent d'imiter les caractéristiques à l'échelle nanométrique du microenvironnement d'une cellule pour diriger sa différenciation vers une lignée appropriée.
TSMC a commencé la production d'un processus 7 nm en 2017 et Samsung a commencé la production d'un processus 5 nm en 2018.
Pour ces raisons, certains groupes préconisent que la nanotechnologie soit réglementée par les gouvernements.
Certains produits à base de nanoparticules peuvent avoir des conséquences imprévues.
L'inhalation de nanoparticules et de nanofibres en suspension dans l'air peut entraîner un certain nombre de maladies pulmonaires, par exemple la fibrose.
Une importante étude publiée plus récemment dans Nature Nanotechnology suggère que certaines formes de nanotubes de carbone – un enfant phare de la « révolution des nanotechnologies » – pourraient être aussi nocives que l'amiante si elles sont inhalées en quantités suffisantes.
Davies (2008) a proposé une feuille de route réglementaire décrivant les étapes à suivre pour remédier à ces lacunes.
En conséquence, certains universitaires ont appelé à une application plus stricte du principe de précaution, avec une autorisation de mise sur le marché retardée, un étiquetage renforcé et des exigences supplémentaires en matière de développement de données de sécurité concernant certaines formes de nanotechnologie.
La technologie nucléaire est une technologie qui implique les réactions nucléaires des noyaux atomiques.
Lui, Pierre Curie et Marie Curie ont commencé à enquêter sur le phénomène.
Certains de ces types de rayonnement pourraient traverser la matière ordinaire, et tous pourraient être nocifs en grande quantité.
Peu à peu, on s'est rendu compte que le rayonnement produit par la désintégration radioactive était un rayonnement ionisant et que même des quantités trop petites pour brûler pouvaient poser un grave danger à long terme.
Au fur et à mesure que l'atome était mieux compris, la nature de la radioactivité est devenue plus claire.
La désintégration alpha se produit lorsqu'un noyau libère une particule alpha, composée de deux protons et de deux neutrons, équivalente à un noyau d'hélium.
Ce type de rayonnement est le plus dangereux et le plus difficile à bloquer.
Le nombre moyen de neutrons libérés par noyau qui vont fissionner un autre noyau est appelé k. Les valeurs de k supérieures à 1 signifient que la réaction de fission libère plus de neutrons qu'elle n'en absorbe, et est donc appelée réaction en chaîne auto-entretenue.
S'il y a suffisamment de désintégrations immédiates pour poursuivre la réaction en chaîne, la masse est dite critique rapide et la libération d'énergie augmentera rapidement et de manière incontrôlable, entraînant généralement une explosion.
Au cours du projet, les premiers réacteurs à fission ont également été développés, même s'ils étaient principalement destinés à la fabrication d'armes et ne produisaient pas d'électricité.
Cependant, si la masse n'est critique que lorsque les neutrons retardés sont inclus, alors la réaction peut être contrôlée, par exemple par l'introduction ou le retrait d'absorbeurs de neutrons.
Lorsque le noyau résultant est plus léger que celui du fer, de l'énergie est normalement libérée ; lorsque le noyau est plus lourd que celui du fer, l'énergie est généralement absorbée.
L'abondance restante d'éléments lourds, du nickel à l'uranium et au-delà, est due à la nucléosynthèse des supernovas, le processus R.
Les bombes à hydrogène tirent leur énorme pouvoir destructeur de la fusion, mais leur énergie est incontrôlable.
Cependant, ces deux appareils fonctionnent avec une perte d'énergie nette.
La fusion nucléaire n'a été initialement poursuivie qu'à des stades théoriques pendant la Seconde Guerre mondiale, lorsque les scientifiques du projet Manhattan (dirigé par Edward Teller) l'ont étudiée comme méthode de construction d'une bombe.
Même de petits engins nucléaires peuvent dévaster une ville par des explosions, des incendies et des radiations.
Une telle arme doit contenir une ou plusieurs masses fissiles sous-critiques stables pour le déploiement, puis induire une criticité (créer une masse critique) pour la détonation.
Un isotope de l'uranium, à savoir l'uranium-235, est d'origine naturelle et suffisamment instable, mais on le trouve toujours mélangé avec l'isotope plus stable de l'uranium-238.
Alternativement, l'élément plutonium possède un isotope suffisamment instable pour que ce procédé soit utilisable.
Ils ont fait exploser la première arme nucléaire lors d'un essai portant le nom de code "Trinity", près d'Alamogordo, au Nouveau-Mexique, le 16 juillet 1945.
À la suite d'une dévastation et de pertes sans précédent causées par une seule arme, le gouvernement japonais s'est rapidement rendu, mettant fin à la Seconde Guerre mondiale.
Un peu plus de quatre ans plus tard, le 29 août 1949, l'Union soviétique a fait exploser sa première arme à fission.
Une arme radiologique est un type d'arme nucléaire conçue pour distribuer des matières nucléaires dangereuses dans les zones ennemies.
Bien que considérée comme inutile par une armée conventionnelle, une telle arme suscite des inquiétudes quant au terrorisme nucléaire.
Le traité autorisait les essais nucléaires souterrains.
Après avoir signé le Traité d'interdiction complète des essais en 1996 (qui n'était pas entré en vigueur en 2011), tous ces États se sont engagés à cesser tous les essais nucléaires.
Tout au long de la guerre froide, les puissances opposées disposaient d'énormes arsenaux nucléaires, suffisants pour tuer des centaines de millions de personnes.
Actuellement, l'énergie nucléaire fournit environ 15,7% de l'électricité mondiale (en 2004) et est utilisée pour propulser des porte-avions, des brise-glaces et des sous-marins (jusqu'à présent, l'économie et les craintes dans certains ports ont empêché l'utilisation de l'énergie nucléaire dans les navires de transport).
Les ibamères à rayons X médicaux et dentaires utilisent du cobalt 60 ou d'autres sources de rayons X.
Les deux contiennent une petite source de 241Am qui donne lieu à un petit courant constant.
Une autre utilisation dans la lutte contre les insectes est la technique des insectes stériles , où les insectes mâles sont stérilisés par rayonnement et relâchés, de sorte qu'ils n'ont pas de progéniture, pour réduire la population.
Les sources de rayonnement utilisées comprennent des sources de rayons gamma radio-isotopes, des générateurs de rayons X et des accélérateurs d'électrons.
En tant que tel, il est également utilisé sur des articles non alimentaires, tels que le matériel médical, les plastiques, les tubes pour gazoducs, les tuyaux pour le chauffage par le sol, les films rétractables pour les emballages alimentaires, les pièces automobiles, les fils et câbles (isolation), les pneus, et même des pierres précieuses.
Les micro-organismes ne peuvent plus proliférer et poursuivre leurs activités malignes ou pathogènes.
Les plantes ne peuvent pas poursuivre le processus naturel de maturation ou de vieillissement.
La spécialité du traitement des aliments par rayonnement ionisant est le fait que la densité d'énergie par transition atomique est très élevée, elle peut cliver des molécules et induire une ionisation (d'où le nom) qui ne peut être obtenue par simple chauffage.
Cependant, l'utilisation du terme pasteurisation à froid pour décrire les aliments irradiés est controversée, car la pasteurisation et l'irradiation sont des processus fondamentalement différents, bien que les résultats finaux escomptés puissent dans certains cas être similaires.
Marie Curie est décédée d'une anémie aplasique résultant de ses niveaux élevés d'exposition.
Environ la moitié des décès d'Hiroshima et de Nagasaki sont morts deux à cinq ans plus tard à cause d'une exposition aux radiations.
Une fusion nucléaire fait référence au risque plus grave de rejet de matières nucléaires dans le milieu environnant.
Les réacteurs militaires qui ont connu des accidents similaires étaient Windscale au Royaume-Uni et SL-1 aux États-Unis.
Un autre sujet de recherche transhumaniste est de savoir comment protéger l'humanité contre les risques existentiels, comme la guerre nucléaire ou la collision d'astéroïdes.
L'affirmation jetterait les bases intellectuelles pour que le philosophe britannique Max More commence à articuler les principes du transhumanisme en tant que philosophie futuriste en 1990 et à organiser en Californie une école de pensée qui est depuis devenue le mouvement transhumaniste mondial.
Dans le Discours, Descartes envisageait un nouveau type de médecine qui pourrait accorder à la fois l'immortalité physique et des esprits plus forts.
St. Leon a peut-être inspiré le roman Frankenstein de sa fille Mary Shelley.
En particulier, il s'intéressait au développement de la science de l'eugénisme, de l'ectogenèse (création et maintien de la vie dans un environnement artificiel) et à l'application de la génétique pour améliorer les caractéristiques humaines, telles que la santé et l'intelligence.
Ces idées sont depuis lors des thèmes transhumanistes communs.
Dans la section Material and Man du manifeste, Noboru Kawazoe suggère que :Après plusieurs décennies, avec les progrès rapides des technologies de communication, chacun aura un "récepteur d'ondes cérébrales" dans son oreille, qui transmet directement et exactement ce que les autres pensent sur lui et vice versa.
En 1966, FM-2030 (anciennement FM Esfandiary), un futuriste qui a enseigné les "nouveaux concepts de l'humain" à la New School, à New York, a commencé à identifier les personnes qui adoptent les technologies, les modes de vie et les visions du monde en transition vers la posthumanité comme " transhumain ».
FM-2030 et Vita-More ont rapidement commencé à organiser des rassemblements pour les transhumanistes à Los Angeles, qui comprenaient des étudiants des cours de FM-2030 et des publics des productions artistiques de Vita-More.
Une préoccupation particulière est l'égalité d'accès aux technologies d'amélioration humaine à travers les classes et les frontières.
Cela a laissé l'Association mondiale des transhumanistes comme la principale organisation transhumaniste internationale.
L'Association des transhumanistes mormons a été fondée en 2006.
Le transhumanisme met l'accent sur la perspective évolutive, y compris parfois la création d'une espèce animale hautement intelligente par le biais d'améliorations cognitives (c'est-à-dire d'élévation biologique), mais s'accroche à un "avenir posthumain" comme objectif final de l'évolution des participants.
Alors qu'un tel « posthumanisme culturel » offrirait des ressources pour repenser les rapports entre humains et machines de plus en plus sophistiquées, le transhumanisme et les posthumanismes apparentés ne renoncent pas, dans cette optique, à des conceptions obsolètes de « sujet libéral autonome », mais élargissent ses « prérogatives » dans le domaine du posthumain.
Cependant, d'autres progressistes ont soutenu que le posthumanisme, qu'il s'agisse de ses formes philosophiques ou militantes, équivaut à s'éloigner des préoccupations de justice sociale, de la réforme des institutions humaines et d'autres préoccupations des Lumières, vers des aspirations narcissiques à une transcendance de l'être humain. corps en quête de manières d'être plus exquises.
De nombreux transhumanistes évaluent activement le potentiel des technologies futures et des systèmes sociaux innovants pour améliorer la qualité de toute vie, tout en cherchant à faire en sorte que la réalité matérielle de la condition humaine remplisse la promesse d'égalité juridique et politique en éliminant les barrières mentales et physiques congénitales.
Certains théoriciens comme Ray Kurzweil pensent que le rythme de l'innovation technologique s'accélère et que les 50 prochaines années pourraient apporter non seulement des avancées technologiques radicales, mais peut-être une singularité technologique, qui pourrait changer fondamentalement la nature des êtres humains.
Par exemple, Bostrom a beaucoup écrit sur les risques existentiels pour le bien-être futur de l'humanité, y compris ceux qui pourraient être créés par les technologies émergentes.
Pour contrer cela, Hawking met l'accent soit sur l'auto-conception du génome humain, soit sur l'amélioration mécanique (par exemple, l'interface cerveau-ordinateur) pour améliorer l'intelligence humaine et réduire l'agressivité, sans quoi il implique que la civilisation humaine pourrait être trop stupide collectivement pour survivre à un système de plus en plus instable. , entraînant un effondrement de la société.
Ces penseurs soutiennent que la capacité à discuter de manière falsifiée constitue un seuil non arbitraire à partir duquel il devient possible pour un individu de parler pour lui-même d'une manière qui ne dépende pas d'hypothèses extérieures.
Conformément à cela, de nombreux défenseurs transhumanistes de premier plan, tels que Dan Agin, qualifient les critiques du transhumanisme, à droite et à gauche politiques conjointement, de « bioconservateurs » ou de « bioluddites », ce dernier terme faisant allusion au mouvement social anti-industrialisation du XIXe siècle. qui s'opposait au remplacement des travailleurs manuels humains par des machines.
Le même scénario se produit lorsque les gens ont certains implants neuronaux qui leur donnent un avantage sur le lieu de travail et dans les aspects éducatifs.
Immortalisme , une idéologie morale basée sur la conviction que l'extension radicale de la vie et l'immortalité technologique sont possibles et souhaitables, et prônant la recherche et le développement pour assurer sa réalisation.
Les mathématiques (du grec : ) comprennent l'étude de sujets tels que la quantité (théorie des nombres), la structure (algèbre), l'espace (géométrie) et le changement (analyse).
Lorsque les structures mathématiques sont de bons modèles de phénomènes réels, le raisonnement mathématique peut être utilisé pour fournir un aperçu ou des prédictions sur la nature.
La recherche nécessaire pour résoudre des problèmes mathématiques peut prendre des années, voire des siècles, de recherche soutenue.
Les mathématiques se sont développées à un rythme relativement lent jusqu'à la Renaissance, lorsque les innovations mathématiques interagissant avec les nouvelles découvertes scientifiques ont conduit à une augmentation rapide du taux de découverte mathématique qui s'est poursuivie jusqu'à nos jours.
Comme en témoignent les décomptes trouvés sur les os, en plus de savoir compter les objets physiques, les peuples préhistoriques ont peut-être également reconnu comment compter des quantités abstraites, comme le temps, les jours, les saisons ou les années.
À partir du 6ème siècle avant JC avec les Pythagoriciens, avec les mathématiques grecques, les Grecs de l'Antiquité ont commencé une étude systématique des mathématiques en tant que matière à part entière.
Le plus grand mathématicien de l'Antiquité est souvent considéré comme Archimède (vers 287-212 avant JC) de Syracuse.
Le système numérique hindou-arabe et les règles d'utilisation de ses opérations, en usage dans le monde aujourd'hui, ont évolué au cours du premier millénaire après JC en Inde et ont été transmis au monde occidental via les mathématiques islamiques.
La réalisation la plus remarquable des mathématiques islamiques a été le développement de l'algèbre.
Au début de la période moderne, les mathématiques ont commencé à se développer à un rythme accéléré en Europe occidentale.
Le mathématicien le plus important du 19e siècle était peut-être le mathématicien allemand Carl Friedrich Gauss, qui a apporté de nombreuses contributions à des domaines tels que l'algèbre, l'analyse, la géométrie différentielle, la théorie des matrices, la théorie des nombres et les statistiques.
Les découvertes mathématiques continuent d'être faites aujourd'hui.
En particulier, mathēmatikḗ tékhnē signifiait «l'art mathématique».
En anglais, le nom mathématiques prend un verbe au singulier.
Cependant, Aristote a également noté que l'accent mis sur la quantité seule peut ne pas distinguer les mathématiques des sciences comme la physique; selon lui, l'abstraction et l'étude de la quantité en tant que propriété «séparable en pensée» des instances réelles distinguent les mathématiques.
Une particularité de l'intuitionnisme est qu'il rejette certaines idées mathématiques considérées comme valables selon d'autres définitions.
Haskell Curry a défini les mathématiques simplement comme "la science des systèmes formels".
Popper a également noté que "je n'admettrai certainement un système comme empirique ou scientifique que s'il est capable d'être testé par l'expérience".
L'intuition et l'expérimentation jouent également un rôle dans la formulation de conjectures en mathématiques et dans les (autres) sciences.
Par exemple, le physicien Richard Feynman a inventé la formulation intégrale de chemin de la mécanique quantique en utilisant une combinaison de raisonnement mathématique et de perspicacité physique, et la théorie des cordes d'aujourd'hui, une théorie scientifique encore en développement qui tente d'unifier les quatre forces fondamentales de la nature, continue d'inspirer nouvelles mathématiques.
Une distinction est souvent faite entre les mathématiques pures et les mathématiques appliquées.
Comme dans la plupart des domaines d'études, l'explosion des connaissances à l'ère scientifique a conduit à la spécialisation : il existe aujourd'hui des centaines de domaines spécialisés en mathématiques et la dernière classification des matières mathématiques compte 46 pages.
De nombreux mathématiciens parlent de l'élégance des mathématiques, de son esthétique intrinsèque et de sa beauté intérieure.
GH Hardy dans A Mathematician's Apology a exprimé la conviction que ces considérations esthétiques sont, en elles-mêmes, suffisantes pour justifier l'étude des mathématiques pures.
Un théorème exprimé comme une caractérisation de l'objet par ces caractéristiques est le prix.
Euler (1707–1783) était responsable de la plupart des notations utilisées aujourd'hui.
Contrairement au langage naturel, où les gens peuvent souvent assimiler un mot (comme vache) à l'objet physique auquel il correspond, les symboles mathématiques sont abstraits, dépourvus de tout analogue physique.
Le langage mathématique comprend également de nombreux termes techniques tels que l'homéomorphisme et l'intégrable qui n'ont aucun sens en dehors des mathématiques.
Les mathématiciens qualifient cette précision de langage et de logique de « rigueur ».
Ceci afin d'éviter des "théorèmes" erronés, basés sur des intuitions faillibles, dont de nombreux exemples se sont produits dans l'histoire du sujet.
L'incompréhension de la rigueur est à l'origine de certaines des idées fausses courantes sur les mathématiques.
D'autre part, les assistants de preuve permettent de vérifier tous les détails qui ne peuvent pas être donnés dans une preuve manuscrite et fournissent la certitude de l'exactitude de longues preuves telles que celle du théorème de Feit-Thompson.
A ces préoccupations principales s'ajoutent des subdivisions dédiées à l'exploration des liens du cœur des mathématiques vers d'autres domaines : à la logique, à la théorie des ensembles (fondements), aux mathématiques empiriques des différentes sciences (mathématiques appliquées), et plus récemment à l'étude rigoureuse de l'incertitude.
Certains désaccords sur les fondements des mathématiques perdurent jusqu'à nos jours.
En tant que tel, il abrite les théorèmes d'incomplétude de Gödel qui impliquent (informellement) que tout système formel effectif contenant de l'arithmétique de base, si son (ce qui signifie que tous les théorèmes qui peuvent être prouvés sont vrais), est nécessairement incomplet (ce qui signifie qu'il existe de vrais théorèmes ce qui ne peut pas être prouvé dans ce système).
La logique moderne est divisée en théorie de la récursivité, théorie des modèles et théorie de la preuve, et est étroitement liée à l'informatique théorique, ainsi qu'à la théorie des catégories.
La théorie de la calculabilité examine les limites de divers modèles théoriques de l'ordinateur, y compris le modèle le plus connu, la machine de Turing.
La considération des nombres naturels conduit également aux nombres transfinis, qui formalisent la notion d'« infini ».
Ainsi on peut étudier des groupes, des anneaux, des corps et d'autres systèmes abstraits ; ensemble, ces études (pour les structures définies par des opérations algébriques) constituent le domaine de l'algèbre abstraite.
La trigonométrie est la branche des mathématiques qui traite des relations entre les côtés et les angles des triangles et des fonctions trigonométriques.
La géométrie convexe et discrète a été développée pour résoudre des problèmes de théorie des nombres et d'analyse fonctionnelle, mais elle est maintenant poursuivie avec un œil sur les applications en optimisation et en informatique.
Les groupes de mensonge sont utilisés pour étudier l'espace, la structure et le changement.
Les fonctions apparaissent ici comme un concept central décrivant une quantité changeante.
L'une des nombreuses applications de l'analyse fonctionnelle est la mécanique quantique.
Les statisticiens (travaillant dans le cadre d'un projet de recherche) « créent des données qui ont du sens » avec un échantillonnage aléatoire et des expériences aléatoires ; la conception d'un échantillon statistique ou d'une expérience spécifie l'analyse des données (avant que les données ne deviennent disponibles).
L'analyse numérique étudie les méthodes pour les problèmes d'analyse en utilisant l'analyse fonctionnelle et la théorie de l'approximation; l'analyse numérique comprend l'étude de l'approximation et de la discrétisation au sens large avec une attention particulière pour les erreurs d'arrondi.
La Médaille Chern a été introduite en 2010 pour récompenser l'ensemble des réalisations.
Cette liste a acquis une grande renommée parmi les mathématiciens, et au moins neuf des problèmes ont maintenant été résolus.
La valeur de Pi a d'abord été calculée par lui.
Ce sont les pythagoriciens qui ont inventé le terme "mathématiques", et avec qui commence l'étude des mathématiques pour elles-mêmes.
En raison d'un différend politique, la communauté chrétienne d'Alexandrie l'a punie, présumant qu'elle était impliquée, en la déshabillant et en lui grattant la peau avec des coquillages (certains disent des tuiles).
Le financement de la traduction de textes scientifiques dans d'autres langues s'est poursuivi tout au long du règne de certains califes, et il s'est avéré que certains savants sont devenus des experts dans les ouvrages qu'ils ont traduits et ont à leur tour reçu un soutien supplémentaire pour continuer à développer certaines sciences.
Une caractéristique notable de nombreux érudits travaillant sous la domination musulmane à l'époque médiévale est qu'ils étaient souvent polymathes.
Au cours de cette période de transition d'une culture principalement féodale et ecclésiastique à une culture principalement laïque, de nombreux mathématiciens notables ont eu d'autres occupations : Luca Pacioli (fondateur de la comptabilité) ; Niccolò Fontana Tartaglia (ingénieur notable et comptable); Gerolamo Cardano (premier fondateur de la probabilité et de l'expansion binomiale); Robert Recorde (médecin) et François Viète (avocat).
Les universités britanniques de cette période ont adopté certaines approches familières aux universités italiennes et allemandes, mais comme elles jouissaient déjà de libertés et d'une autonomie substantielles, les changements y avaient commencé avec le siècle des Lumières, les mêmes influences qui ont inspiré Humboldt.
Les étudiants pouvaient mener des recherches dans des séminaires ou des laboratoires et ont commencé à produire des thèses de doctorat avec un contenu plus scientifique.
Les mathématiciens et les mathématiciens appliqués sont considérés comme deux des carrières STEM (science, technologie, ingénierie et mathématiques).
Les actuaires abordent également des questions financières, notamment celles concernant le niveau des cotisations de retraite nécessaires pour produire un certain revenu de retraite et la manière dont une entreprise devrait investir des ressources pour maximiser son retour sur investissement compte tenu du risque potentiel.
Le système hiéroglyphique des chiffres égyptiens, comme les derniers chiffres romains, est issu des marques de pointage utilisées pour le comptage.
Les premiers systèmes de numération qui incluaient la notation positionnelle n'étaient pas décimaux, y compris le système sexagésimal (base 60) pour les chiffres babyloniens et le système vigésimal (base 20) qui définissait les chiffres mayas.
Avant les travaux d'Euclide vers 300 avant JC, les études grecques en mathématiques se chevauchaient avec les croyances philosophiques et mystiques.
Les anciens Grecs n'avaient pas de symbole pour zéro jusqu'à la période hellénistique, et ils utilisaient trois ensembles distincts de symboles comme chiffres : un ensemble pour la place des unités, un pour la place des dizaines et un pour les centaines.
Leur algorithme de division longue était le même, et l'algorithme de racine carrée chiffre par chiffre, couramment utilisé aussi récemment qu'au 20e siècle, était connu d'Archimède (qui l'a peut-être inventé).
Les anciens Chinois avaient des études arithmétiques avancées datant de la dynastie Shang et se poursuivant à travers la dynastie Tang, des nombres de base à l'algèbre avancée.
Pour la place des centaines, ils ont ensuite réutilisé les symboles de la place des unités, et ainsi de suite.
Les anciens Chinois ont été les premiers à découvrir, comprendre et appliquer de manière significative les nombres négatifs.
Son contemporain, l'évêque syriaque Severus Sebokht (650 après JC) a déclaré: "Les Indiens possèdent une méthode de calcul qu'aucun mot ne peut assez louer.
Les Arabes ont également appris cette nouvelle méthode et l'ont appelée hesab.
L'épanouissement de l'algèbre dans le monde islamique médiéval, ainsi que dans l'Europe de la Renaissance, était une conséquence de l'énorme simplification du calcul par la notation décimale.
Les expressions arithmétiques doivent être évaluées selon la séquence d'opérations prévue.
Par exemple, les ordinateurs numériques peuvent réutiliser les circuits d'addition existants et économiser des circuits supplémentaires pour implémenter une soustraction, en utilisant la méthode du complément à deux pour représenter les inverses additifs, qui est extrêmement facile à implémenter dans le matériel (négation).
La multiplication combine également deux nombres en un seul nombre, le produit.
Si les nombres sont ibaminés comme étant alignés, la multiplication par un nombre supérieur à 1, disons x, revient à étirer uniformément tout à partir de 0, de telle sorte que le nombre 1 lui-même est étiré jusqu'à l'endroit où x était.
Tout dividende divisé par zéro est indéfini.
Le théorème fondamental de l'arithmétique a été prouvé pour la première fois par Carl Friedrich Gauss.
La notation positionnelle (également connue sous le nom de "notation de valeur de position") fait référence à la représentation ou au codage de nombres utilisant le même symbole pour les différents ordres de bamnitude (par exemple, la "place des unités", la "place des dizaines", la "place des centaines") et, avec un point de base, en utilisant ces mêmes symboles pour représenter des fractions (par exemple, la "dixième place", la "centième place").
L'utilisation du 0 comme espace réservé et, par conséquent, l'utilisation d'une notation de position est attestée pour la première fois dans le texte jaïn de l'Inde intitulé le Lokavibhâga, daté de 458 après JC et ce n'est qu'au début du XIIIe siècle que ces concepts, transmis via le érudition du monde arabe, ont été introduits en Europe par Fibonacci en utilisant le système numérique hindou-arabe.
Le résultat est calculé par l'addition répétée de chiffres uniques de chaque nombre qui occupe la même position, en procédant de droite à gauche.
Le chiffre le plus à droite est la valeur de la position actuelle, et le résultat de l'addition ultérieure des chiffres à gauche augmente de la valeur du deuxième chiffre (le plus à gauche), qui est toujours un (sinon zéro).
Une table de multiplication avec dix lignes et dix colonnes répertorie les résultats pour chaque paire de chiffres.
Des techniques similaires existent pour la soustraction et la division.
Dans la terminologie mathématique, cette caractéristique est définie comme fermeture, et la liste précédente est décrite comme .
Le total dans la colonne pence est 25.
Cette opération est répétée en utilisant les valeurs de la colonne des shillings, avec l'étape supplémentaire consistant à ajouter la valeur qui a été reportée de la colonne des centimes.
Un livret typique de 150 pages tabulait les multiples "de un à dix mille aux différents prix d'un farthing à une livre".
Cette étude est parfois connue sous le nom d'algorisme.
En outre, l'arithmétique était utilisée par les érudits islamiques afin d'enseigner l'application des règles relatives à la Zakat et à l'Irth.
L'addition (habituellement signifiée par le symbole plus ) est l'une des quatre opérations de base de l'arithmétique, les trois autres étant la soustraction, la multiplication et la division.
En algèbre, un autre domaine des mathématiques, l'addition peut également être effectuée sur des objets abstraits tels que des vecteurs, des matrices, des sous-espaces et des sous-groupes.
L'utilisation du suffixe gérondif -nd donne "addend", "chose à ajouter".
"Sum" et "summand" dérivent du nom latin summa "le plus haut, le sommet" et du verbe associé summare.
Les derniers termes anglais moyen « adden » et « adding » ont été popularisés par Chaucer.
Par exemple, l'expression a + b + c doit-elle être définie comme signifiant (a + b) + c ou a + (b + c) ?
Même certains animaux non humains montrent une capacité limitée à ajouter, en particulier les primates.
Avec une expérience supplémentaire, les enfants apprennent à additionner plus rapidement en exploitant la commutativité de l'addition en comptant à partir du plus grand nombre, dans ce cas, en commençant par trois et en comptant "quatre, cinq".
Zéro : puisque zéro est l'identité additive, l'ajout de zéro est trivial.
On aligne deux fractions décimales l'une au-dessus de l'autre, avec le point décimal au même endroit.
Si les additions sont les vitesses de rotation de deux arbres, elles peuvent être additionnées avec un différentiel.
Il utilisait un mécanisme de transport assisté par gravité.
Pour soustraire, l'opérateur devait utiliser le complément de la calculatrice de Pascal, qui nécessitait autant d'étapes qu'une addition.
Les portes XOR et AND sont simples à réaliser en logique numérique, ce qui permet la réalisation de circuits additionneurs complets qui, à leur tour, peuvent être combinés en opérations logiques plus complexes.
De nombreuses implémentations sont, en fait, des hybrides de ces trois dernières conceptions.
Un débordement arithmétique imprévu est une cause assez courante d'erreurs de programme.
Prise littéralement, la définition ci-dessus est une application du théorème de récursivité sur l'ensemble partiellement ordonné N2.
Si a ou b est égal à zéro, traitez-le comme une identité.
Ici, le semi-groupe est formé par les nombres naturels et le groupe est le groupe additif des entiers.
La commutativité et l'associativité de l'addition réelle sont immédiates ; définissant le nombre réel 0 comme l'ensemble des rationnels négatifs, on le voit facilement comme l'identité additive.
Il faut prouver que cette opération est bien définie, traitant des suites co-Cauchy.
L'ensemble des entiers modulo 2 n'a que deux éléments; l'opération d'addition dont il hérite est connue en logique booléenne sous le nom de fonction "ou exclusif".
Ceux-ci donnent deux généralisations différentes de l'addition des nombres naturels au transfini.
Il y a encore plus de généralisations de la multiplication que de l'addition.
En effet, si deux nombres non négatifs a et b sont d'ordres de bamnitude différents, alors leur somme est approximativement égale à leur maximum.
Il comprend l'idée de la somme d'un seul nombre, qui est lui-même, et la somme vide, qui est zéro.
L'intégration est une sorte de « sommation » sur un continuum, ou plus précisément et généralement, sur une variété différentiable.
Les combinaisons linéaires sont particulièrement utiles dans des contextes où une simple addition violerait une règle de normalisation, comme le mélange de stratégies dans la théorie des jeux ou la superposition d'états dans la mécanique quantique.
La division est l'une des quatre opérations de base de l'arithmétique, la façon dont les nombres sont combinés pour former de nouveaux nombres.
Ceux dans lesquels une division euclidienne (avec reste) est définie sont appelés domaines euclidiens et comprennent des anneaux de polynômes dans une indéterminée (qui définissent la multiplication et l'addition sur des formules à une seule variable).
Ce signe de division est également utilisé seul pour représenter l'opération de division elle-même, comme par exemple comme une étiquette sur une touche d'une calculatrice.
Distribuer les objets plusieurs à la fois dans chaque tour de partage à chaque portion conduit à l'idée de «couper» une forme de division où l'on soustrait à plusieurs reprises des multiples du diviseur du dividende lui-même.
Une personne peut utiliser des tables de logarithmes pour diviser deux nombres, en soustrayant les logarithmes des deux nombres, puis en recherchant l'antilogarithme du résultat.
Certains langages de programmation, tels que C, traitent la division entière comme dans le cas 5 ci-dessus, donc la réponse est un entier.
De même, la division à droite de b par a (notée ) est la solution y de l'équation .
Les exemples incluent les algèbres matricielles et les algèbres de quaternions.
L'entrée d'une telle expression dans la plupart des calculatrices produit un message d'erreur.
Étant donné que ce remplacement réduit le plus grand des deux nombres, la répétition de ce processus donne des paires de nombres successivement plus petites jusqu'à ce que les deux nombres deviennent égaux.
Le fait que le GCD puisse toujours être exprimé de cette manière est connu comme l'identité de Bézout.
Avec cette amélioration, l'algorithme ne nécessite jamais plus de pas que cinq fois le nombre de chiffres (base 10) du plus petit entier.
L'algorithme d'Euclide a de nombreuses applications théoriques et pratiques.
L'algorithme euclidien peut être utilisé pour résoudre des équations diophantiennes , telles que trouver des nombres qui satisfont plusieurs congruences selon le théorème des restes chinois , pour construire des fractions continues et pour trouver des approximations rationnelles précises des nombres réels.
Le plus grand diviseur commun est souvent écrit comme pgcd(a, b) ou, plus simplement, comme (a, b), bien que cette dernière notation soit ambiguë, également utilisée pour des concepts tels qu'un idéal dans l'anneau des entiers, qui est étroitement liés à GCD.
Par exemple, ni 6 ni 35 ne sont des nombres premiers, puisqu'ils ont tous deux deux facteurs premiers : 6 = 2 × 3 et 35 = 5 × 7.
La factorisation de grands nombres entiers est considérée comme un problème de calcul très difficile, et la sécurité de nombreux protocoles cryptographiques largement utilisés est basée sur son infaisabilité.
L'ensemble de toutes les combinaisons linéaires intégrales de a et b est en fait le même que l'ensemble de tous les multiples de g (mg, où m est un entier).
En d'autres termes, les multiples du plus petit nombre rk-1 sont soustraits du plus grand nombre rk-2 jusqu'à ce que le reste rk soit plus petit que rk-1.
Donc, c divise le reste initial r0, puisque r0 = a − q0b = mc − q0nc = (m − q0n)c.
Nous essayons d'abord de carreler le rectangle en utilisant des carreaux carrés b-by-b ; cependant, cela laisse un rectangle résiduel de r0 par b, où r0 < b. Nous essayons ensuite de carreler le rectangle résiduel avec des carreaux carrés r0 par r0.
Le théorème qui sous-tend la définition de la division euclidienne assure qu'un tel quotient et reste existent toujours et sont uniques.
A la fin de l'itération de la boucle, la variable b contient le reste rk, tandis que la variable a contient son prédécesseur, rk−1.
Le mathématicien et historien BL van der Waerden suggère que le livre VII dérive d'un manuel sur la théorie des nombres écrit par des mathématiciens de l'école de Pythagore.
Des siècles plus tard, l'algorithme d'Euclide a été découvert indépendamment à la fois en Inde et en Chine, principalement pour résoudre les équations diophantiennes apparues en astronomie et créer des calendriers précis.
L'algorithme euclidien a été décrit pour la première fois numériquement et popularisé en Europe dans la deuxième édition des Problèmes plaisants et délectables de Bachet (1624).
Au 19ème siècle, l'algorithme euclidien a conduit au développement de nouveaux systèmes de nombres, tels que les entiers gaussiens et les entiers d'Eisenstein.
Peter Gustav Lejeune Dirichlet semble avoir été le premier à décrire l'algorithme d'Euclide comme base d'une grande partie de la théorie des nombres.
Par exemple, Dedekind a été le premier à prouver le théorème des deux carrés de Fermat en utilisant la factorisation unique des entiers gaussiens.
D'autres applications de l'algorithme d'Euclide ont été développées au 19ème siècle.
Plusieurs nouveaux algorithmes de relations entières ont été développés, tels que l'algorithme de Helaman Ferguson et RW Forcade (1979) et l'algorithme LLL.
Les joueurs retirent à tour de rôle m multiples de la plus petite pile de la plus grande.
En permettant à u de varier sur tous les entiers possibles, une famille infinie de solutions peut être générée à partir d'une seule solution (x1, y1).
Dans ce champ, le résultat de toute opération mathématique (addition, soustraction, multiplication ou division) est réduit modulo 13 ; c'est-à-dire que des multiples de 13 sont ajoutés ou soustraits jusqu'à ce que le résultat soit ramené dans la plage 0–12.
Supposons maintenant que le résultat soit valable pour toutes les valeurs de N jusqu'à M - 1.
À titre d'illustration, la probabilité d'un quotient de 1, 2, 3 ou 4 est d'environ 41,5 %, 17,0 %, 9,3 % et 5,9 %, respectivement.
Une approche inefficace pour trouver le PGCD de deux nombres naturels a et b consiste à calculer tous leurs diviseurs communs ; le PGCD est alors le plus grand diviseur commun.
Comme indiqué ci-dessus, le PGCD est égal au produit des facteurs premiers partagés par les deux nombres a et b. Les méthodes actuelles de factorisation en nombres premiers sont également inefficaces; de nombreux systèmes de cryptographie modernes reposent même sur cette inefficacité.
L'algorithme GCD de Lehmer utilise le même principe général que l'algorithme binaire pour accélérer les calculs GCD dans des bases arbitraires.
L'algorithme euclidien peut être utilisé pour résoudre des équations diophantiennes linéaires et des problèmes de reste chinois pour les polynômes; des fractions continues de polynômes peuvent également être définies.
Tout domaine euclidien est un domaine de factorisation unique (UFD), bien que l'inverse ne soit pas vrai.
Un domaine euclidien est toujours un domaine idéal principal (PID), un domaine intégral dans lequel chaque idéal est un idéal principal.
Les numérateurs et les dénominateurs sont également utilisés dans les fractions qui ne sont pas courantes, y compris les fractions composées, les fractions complexes et les nombres mixtes.
Le terme était à l'origine utilisé pour distinguer ce type de fraction de la fraction sexagésimale utilisée en astronomie.
Cela a été expliqué dans le manuel du XVIIe siècle The Ground of Arts.
Le produit d'une fraction et de son inverse est 1, donc l'inverse est l'inverse multiplicatif d'une fraction.
Le reste devient le numérateur de la partie fractionnaire.
Puisque 5×17 (= 85) est supérieur à 4×18 (= 72), le résultat de la comparaison est .
Comme un tiers d'un quart est un douzième, deux tiers d'un quart sont deux douzièmes.
Parfois, une décimale répétitive infinie est nécessaire pour atteindre la même précision.
Les Égyptiens utilisaient les fractions égyptiennes BC.
Leurs méthodes donnaient la même réponse que les méthodes modernes.
Une expression moderne des fractions connue sous le nom de bhinnarasi semble avoir son origine en Inde dans les travaux d'Aryabhatta, Brahbamupta et Bhaskara.
En mathématiques, l'arithmétique modulaire est un système d'arithmétique pour les nombres entiers, où les nombres « s'enroulent » lorsqu'ils atteignent une certaine valeur, appelée module.
Une application très pratique consiste à calculer des sommes de contrôle dans des identifiants de numéros de série.
RSA et Diffie-Hellman utilisent l'exponentiation modulaire.
Il est utilisé par les implémentations les plus efficaces du plus grand diviseur commun polynomial, de l'algèbre linéaire exacte et des algorithmes de base de Gröbner sur les nombres entiers et les nombres rationnels.
L'opération modulo, telle qu'implémentée dans de nombreux langages de programmation et calculatrices, est une application de l'arithmétique modulaire souvent utilisée dans ce contexte.
La méthode d'élimination des neufs offre une vérification rapide des calculs arithmétiques décimaux effectués à la main.
Un système linéaire de congruences peut être résolu en temps polynomial avec une forme d' élimination gaussienne , pour plus de détails, voir le théorème de congruence linéaire .
La multiplication des nombres entiers (y compris les nombres négatifs), des nombres rationnels (fractions) et des nombres réels est définie par une généralisation systématique de cette définition de base.
Le produit de deux mesures est un nouveau type de mesure.
L'opération inverse de la multiplication est la division.
La division d'un nombre autre que 0 par lui-même vaut 1.
Cette utilisation implicite de la multiplication peut provoquer une ambiguïté lorsque les variables concaténées correspondent au nom d'une autre variable, lorsqu'un nom de variable devant une parenthèse peut être confondu avec un nom de fonction, ou dans la détermination correcte de l'ordre des opérations.
Les nombres à multiplier sont généralement appelés les "facteurs".
Aussi comme le résultat d'une multiplication ne dépend pas de l'ordre des facteurs, la distinction entre « multiplicande » et « multiplicateur » n'est utile qu'à un niveau très élémentaire et dans certains algorithmes de multiplication, comme la multiplication longue.
Le résultat d'une multiplication s'appelle un produit.
La règle à calcul permettait de multiplier rapidement les nombres à environ trois endroits de précision.
La théorie générale est donnée par l'analyse dimensionnelle.
Les nombres complexes n'ont pas d'ordre.
Ici, nous avons l'identité 1, par opposition aux groupes sous addition où l'identité est généralement 0.
Pour le voir, considérons l'ensemble des matrices carrées inversibles d'une dimension donnée sur un champ donné.
Un autre fait à noter est que les nombres entiers sous multiplication ne sont pas un groupe, même si nous excluons zéro.
En mathématiques, un pourcentage (du latin per centum "par cent") est un nombre ou un rapport exprimé sous la forme d'une fraction de 100.
Le calcul avec ces fractions équivalait à calculer des pourcentages.
Chaque fois que vous communiquez sur un pourcentage, il est important de préciser à quoi il se rapporte (c'est-à-dire, quel est le total qui correspond à 100 %).
Lorsqu'on parle d'une "hausse de 10 %" ou d'une "baisse de 10 %" d'une quantité, l'interprétation habituelle est que celle-ci est relative à la valeur initiale de cette quantité.
La même confusion entre les différents concepts de pourcentage (âge) et de points de pourcentage peut potentiellement causer un malentendu majeur lorsque les journalistes rendent compte des résultats des élections, par exemple, exprimant à la fois les nouveaux résultats et les différences avec les résultats antérieurs en pourcentages.
Le terme a été attribué au latin pour cent.
Les guides de grammaire et de style diffèrent souvent quant à la manière dont les pourcentages doivent être écrits.
Lorsque les taux d'intérêt sont très bas, le chiffre 0 est inclus si le taux d'intérêt est inférieur à 1 %, par exemple " % d'actions de trésorerie", et non " % d'actions de trésorerie".)
De même, le pourcentage de victoires d'une équipe, la fraction de matchs que le club a remportés, est également généralement exprimé sous forme de proportion décimale ; une équipe qui a un pourcentage de victoires de .500 a gagné 50% de ses matchs.
La soustraction obéit également à des règles prévisibles concernant les opérations connexes, telles que l'addition et la multiplication.
Effectuer une soustraction sur des nombres naturels est l'une des tâches numériques les plus simples.
Formellement, le nombre soustrait est connu sous le nom de soustraction, tandis que le nombre dont il est soustrait est la diminution.
Subtraction" est un mot anglais dérivé du verbe latin subtrahere, qui à son tour est un composé de sub "de dessous" et de trahere "tirer".
A partir de la position 3, il ne fait aucun pas vers la gauche pour rester en 3, donc .
Pour représenter une telle opération, la ligne doit être prolongée.
Le premier chiffre "1" du résultat est alors ignoré.
A la place des dizaines, 0 est inférieur à 1, donc le 0 est augmenté de 10, et la différence avec 1, qui est 9, est écrite à la place des dizaines.
La soustraction se poursuit ensuite à la place des centaines, où 6 n'est pas inférieur à 5, de sorte que la différence est écrite à la place des centaines du résultat.
Au contraire, il augmente de un le chiffre des centaines inférieur.
La réponse est 1 et est écrite à la place des centaines du résultat.
Ce théorème a été conjecturé pour la première fois par Pierre de Fermat en 1637 dans la marge d'un exemplaire d' Arithmetica , où il affirmait qu'il avait une preuve trop grande pour tenir dans la marge.
Le théorème des cinq couleurs , qui a une courte preuve élémentaire, stipule que cinq couleurs suffisent pour colorer une carte et a été prouvé à la fin du 19e siècle; cependant, prouver que quatre couleurs suffisent s'est avéré beaucoup plus difficile.
C'était le premier théorème majeur à être prouvé à l'aide d'un ordinateur.
De plus, toute carte qui pourrait potentiellement être un contre-exemple doit avoir une partie qui ressemble à l'une de ces 1 936 cartes.
Il a été initialement formulé en 1908, par Steinitz et Tietze.
Une variété V sur un corps fini avec q éléments a un nombre fini de points rationnels, ainsi que des points sur chaque corps fini avec qk éléments contenant ce champ.
Conjecturé à l'origine par Henri Poincaré , le théorème concerne un espace qui ressemble localement à un espace tridimensionnel ordinaire mais qui est connecté, de taille finie et dépourvu de toute frontière (une variété fermée 3 ).
Après près d'un siècle d'efforts des mathématiciens, Grigori Perelman a présenté une preuve de la conjecture dans trois articles mis à disposition en 2002 et 2003 sur arXiv.
Perelman a complété cette partie de la preuve.
De manière informelle, il demande si chaque problème dont la solution peut être rapidement vérifiée par un ordinateur peut également être rapidement résolu par un ordinateur ; il est largement conjecturé que la réponse est non.
Il n'a pas été prouvé laquelle est fausse, mais il est largement admis que la première conjecture est vraie et la seconde est fausse.
Par exemple, la conjecture de Collatz, qui concerne la fin ou non de certaines séquences d'entiers, a été testée pour tous les entiers jusqu'à 1,2 × 1012 (plus d'un billion).
Ces preuves peuvent être de diverses natures, telles que la vérification des conséquences de celles-ci ou de fortes interconnexions avec des résultats connus.
Une méthode de preuve, applicable lorsqu'il n'y a qu'un nombre fini de cas pouvant conduire à des contre-exemples, est connue sous le nom de « force brute » : dans cette approche, tous les cas possibles sont pris en compte et il est démontré qu'ils ne donnent pas de contre-exemples.
L'hypothèse du continuum, qui tente de déterminer la cardinalité relative de certains ensembles infinis, s'est finalement révélée indépendante de l'ensemble généralement accepté des axiomes de Zermelo-Fraenkel de la théorie des ensembles.
Peu de théoriciens des nombres doutent que l'hypothèse de Riemann soit vraie.
La carte logistique est une carte polynomiale, souvent citée comme un exemple archétypal de la façon dont un comportement chaotique peut résulter d'équations dynamiques non linéaires très simples.
Kepler a prouvé que c'est la limite du rapport des nombres de Fibonacci consécutifs.
Pour deux raisons, cette représentation peut poser des problèmes.
Par exemple, les deux représentations 0,999... et 1 sont équivalentes en ce sens qu'elles représentent le même nombre.
À l'aide d'ordinateurs et de superordinateurs, certaines des constantes mathématiques, notamment π , e et la racine carrée de 2, ont été calculées à plus de cent milliards de chiffres.
Certaines constantes diffèrent tellement du genre habituel qu'une nouvelle notation a été inventée pour les représenter raisonnablement.
Parfois, le symbole représentant une constante est un mot entier.
0 (zéro) est un nombre et le chiffre numérique utilisé pour représenter ce nombre en chiffres.
Les noms du chiffre 0 en anglais incluent zéro, rien (Royaume-Uni), rien (États-Unis; ), néant ou - dans les contextes où au moins un chiffre adjacent le distingue de la lettre "O" - oh ou o.
Pour la simple notion de manque, les mots rien et aucun sont souvent utilisés.
Il est souvent appelé oh dans le contexte des numéros de téléphone.
Le symbole nfr, signifiant beau, était également utilisé pour indiquer le niveau de base dans les dessins de tombes et de pyramides, et les distances étaient mesurées par rapport à la ligne de base comme étant au-dessus ou en dessous de cette ligne.
L'espace réservé babylonien n'était pas un vrai zéro car il n'était pas utilisé seul, ni à la fin d'un nombre.
En 150 après JC, Ptolémée, influencé par Hipparque et les Babyloniens, utilisait un symbole pour zéro dans son travail sur l'astronomie mathématique appelé Syntaxis Mathematica, également connu sous le nom d'Albamest.
Cette utilisation a été répétée dans AD525 dans un tableau équivalent, qui a été traduit via le latin nulla ou "aucun" par Dionysius Exiguus, aux côtés de chiffres romains.
Le Lokavibhāga , un texte jaïn sur la cosmologie survivant dans une traduction sanskrite médiévale de l'original Prakrit, qui est daté en interne de 458 après JC (ère Saka 380), utilise un système de valeur de position décimale, y compris un zéro.
En 813, al-Khwarizmi utilisa les chiffres hindous dans ses tables astronomiques."
Ce livre a ensuite été traduit en latin au XIIe siècle sous le titre Algoritmi de numero Indorum.
J'ai poursuivi mon étude en profondeur et j'ai appris le donnant-donnant de la dispute.
Je me suis efforcé de composer ce livre dans son intégralité de la manière la plus compréhensible possible, en le divisant en quinze chapitres.
Les neuf chiffres indiens sont : 9 8 7 6 5 4 3 2 1.
254–255 incluent 0 comme nombre naturel, auquel cas c'est le seul nombre naturel qui n'est pas positif.
En tant que valeur ou nombre, zéro n'est pas le même que le chiffre zéro, utilisé dans les systèmes numériques avec notation positionnelle.
Le nombre 0 peut ou non être considéré comme un nombre naturel, mais c'est un nombre entier, et donc un nombre rationnel et un nombre réel (ainsi qu'un nombre algébrique et un nombre complexe).
Il ne peut pas être premier car il a un nombre infini de facteurs, et ne peut pas être composé car il ne peut pas être exprimé comme un produit de nombres premiers (car 0 doit toujours être l'un des facteurs).
Ces règles s'appliquent à tout nombre réel ou complexe x, sauf indication contraire.
La fonction de cardinalité, appliquée à l'ensemble vide, renvoie l'ensemble vide comme valeur, lui attribuant ainsi 0 éléments.
En algèbre abstraite, 0 est couramment utilisé pour désigner un élément nul, qui est un élément neutre pour l'addition (s'il est défini sur la structure considérée) et un élément absorbant pour la multiplication (s'il est défini).
Pour certaines grandeurs, le niveau zéro se distingue naturellement de tous les autres niveaux, alors que pour d'autres il est choisi plus ou moins arbitrairement.
Il a été démontré qu'un amas de quatre neutrons peut être suffisamment stable pour être considéré comme un atome à part entière.
Par exemple, les éléments d'un tableau sont numérotés à partir de 0 en C, de sorte que pour un tableau de n éléments, la séquence d'indices du tableau va de 0 à .
Dans les bases de données, il est possible qu'un champ n'ait pas de valeur.
Pour les champs de texte, ce n'est pas vide ni la chaîne vide.
Tout calcul incluant une valeur nulle donne un résultat nul.
En Formule 1, si le champion du monde en titre ne participe plus à la Formule 1 dans l'année qui suit sa victoire dans la course au titre, 0 est attribué à l'un des pilotes de l'équipe avec laquelle le champion en titre a remporté le titre.
Les machines à écrire ne faisaient à l'origine aucune distinction de forme entre O et 0; certains modèles n'avaient même pas de clé séparée pour le chiffre 0.
Le chiffre 0 avec un point au centre semble être à l'origine une option sur les écrans IBM 3270 et s'est poursuivi avec certaines polices informatiques modernes telles que Andalé Mono et dans certains systèmes de réservation de compagnies aériennes.
1 (un, également appelé unité et unité) est un nombre et un chiffre utilisé pour représenter ce nombre en chiffres.
Dans les conventions de signe où zéro n'est considéré ni positif ni négatif, 1 est le premier et le plus petit entier positif.
La plupart sinon toutes les propriétés de 1 peuvent en être déduites.
C'est donc l'entier après zéro.
Il a été transmis en Europe via le Maghreb et l'Andalousie au Moyen Âge, à travers des ouvrages savants écrits en arabe.
Les styles qui n'utilisent pas le long trait ascendant sur le chiffre 1 n'utilisent généralement pas non plus le trait horizontal à travers la verticale du chiffre 7.
Par définition, 1 est la bamnitude, la valeur absolue ou la norme d'un nombre complexe unitaire, d'un vecteur unitaire et d'une matrice unitaire (plus généralement appelée matrice d'identité).
En théorie des catégories, 1 est parfois utilisé pour désigner l'objet terminal d'une catégorie.
Puisque la fonction exponentielle de base 1 (1x) est toujours égale à 1, son inverse n'existe pas (qui s'appellerait le logarithme de base 1 s'il existait).
De même, les vecteurs sont souvent normalisés en vecteurs unitaires (c'est-à-dire des vecteurs de bamnitude un), car ceux-ci ont souvent des propriétés plus souhaitables.
C'est aussi le premier et le deuxième nombre de la séquence de Fibonacci (0 étant le zéro) et c'est le premier nombre de nombreuses autres séquences mathématiques.
Néanmoins, l'algèbre abstraite peut considérer le champ avec un élément, qui n'est pas un singleton et n'est pas du tout un ensemble.
Un code binaire est une séquence de 1 et 0 utilisée dans les ordinateurs pour représenter tout type de données.
+1 est la charge électrique des positrons et des protons.
Le philosophe néopythagoricien Nicomachus de Gerasa a affirmé que l'on n'est pas un nombre, mais la source du nombre.
We Are Number One est une chanson de 2014 de l'émission télévisée pour enfants LazyTown, qui a gagné en popularité en tant que mème.
Dans le football associatif (soccer), le numéro 1 est souvent attribué au gardien de but.
1 est le nombre le plus bas autorisé à être utilisé par les joueurs de la Ligue nationale de hockey (LNH); la ligue a interdit l'utilisation de 00 et 0 à la fin des années 1990 (le nombre le plus élevé autorisé étant 98).
Toute séquence aléatoire de chiffres contient des sous-séquences arbitrairement longues qui semblent non aléatoires, par le théorème du singe infini.
Deuxièmement, puisqu'aucun nombre transcendantal ne peut être construit avec un compas et une règle, il n'est pas possible de « quadriller le cercle ».
L'astronome indien Aryabhata a utilisé une valeur de 3,1416 dans son Āryabhaṭīya (499 après JC).
L'astronome persan Jamshīd al-Kāshī a produit 9 chiffres sexagésimaux, à peu près l'équivalent de 16 chiffres décimaux, en 1424 en utilisant un polygone de 3 × 228 côtés, qui a représenté le record du monde pendant environ 180 ans.
Ceux-ci évitent de dépendre de séries infinies.
Tel que modifié par Salamin et Brent, il est également appelé algorithme de Brent-Salamin.
Cela contraste avec les séries infinies ou les algorithmes itératifs, qui conservent et utilisent tous les chiffres intermédiaires jusqu'à ce que le résultat final soit produit.
Ces aides à la mémorisation sont appelées mnémoniques.
Les chiffres sont de grands caractères en bois fixés au plafond en forme de dôme.
Un chiffre numérique est un symbole unique utilisé seul (comme "2") ou en combinaisons (comme "25"), pour représenter des nombres dans un système numérique positionnel.
Un système de nombres positionnels a un chiffre unique pour chaque nombre entier de zéro jusqu'à, mais non compris, la base du système de nombres.
Les chiffres d'origine étaient très similaires aux chiffres modernes, même jusqu'aux glyphes utilisés pour représenter les chiffres.
Les Mayas utilisaient un symbole de coquillage pour représenter le zéro.
Le système numérique thaïlandais est identique au système numérique hindou-arabe, à l'exception des symboles utilisés pour représenter les chiffres.
Ce sont tous les deux des systèmes de base 3.
Plusieurs auteurs au cours des 300 dernières années ont noté une facilité de notation positionnelle qui équivaut à une représentation décimale modifiée.
Par exemple, 1111 (mille cent onze) est un repunit.
En plus de compter dix doigts, certaines cultures ont compté les jointures, l'espace entre les doigts et les orteils ainsi que les doigts.
Les cultures de l'âge de pierre, y compris les anciens groupes autochtones américains, utilisaient des décomptes pour les jeux de hasard, les services personnels et les biens commerciaux.
À partir d'environ 3500 avant JC, les jetons d'argile ont été progressivement remplacés par des signes numériques imprimés d'un stylet rond à différents angles dans des tablettes d'argile (à l'origine des récipients pour jetons) qui ont ensuite été cuites.
Ces signes numériques cunéiformes ressemblaient aux signes numériques ronds qu'ils remplaçaient et conservaient la notation additive signe-valeur des signes numériques ronds.
Les chiffres sexagésimaux étaient un système de base mixte qui conservait l'alternance de la base 10 et de la base 6 dans une séquence de coins et de chevrons verticaux cunéiformes.
Des nombres uniques de troupes et des mesures de riz apparaissent comme des combinaisons uniques de ces décomptes.
Les décomptes conventionnels sont assez difficiles à multiplier et à diviser.
Les Juifs ont commencé à utiliser un système similaire (chiffres hébreux), les exemples les plus anciens connus étant des pièces de monnaie datant d'environ 100 av.
Les Mayas d'Amérique centrale utilisaient un système mixte de base 18 et de base 20, peut-être hérité des Olmèques, comprenant des fonctionnalités avancées telles que la notation positionnelle et un zéro.
La connaissance des encodages des nœuds et des couleurs a été supprimée par les conquistadors espagnols au XVIe siècle et n'a pas survécu, bien que de simples appareils d'enregistrement de type quipu soient encore utilisés dans la région andine.
Le zéro a été utilisé pour la première fois en Inde au 7ème siècle de notre ère par Brahbamupta.
Les mathématiciens arabes ont étendu le système pour inclure les fractions décimales, et Muḥammad ibn Mūsā al-Ḵwārizmī a écrit un ouvrage important à ce sujet au IXe siècle.
Le système binaire (base 2), a été propagé au 17ème siècle par Gottfried Leibniz.
Les variables pour lesquelles l'équation doit être résolue sont également appelées inconnues, et les valeurs des inconnues qui satisfont l'égalité sont appelées solutions de l'équation.
Une équation conditionnelle n'est vraie que pour des valeurs particulières des variables.
Très souvent, le membre droit d'une équation est supposé égal à zéro.
Une équation est analogue à une échelle dans laquelle des poids sont placés.
C'est l'idée de départ de la géométrie algébrique, un domaine important des mathématiques.
Pour résoudre les équations de l'une ou l'autre famille, on utilise des techniques algorithmiques ou géométriques issues de l'algèbre linéaire ou de l'analyse mathématique.
Ces équations sont difficiles en général ; on cherche souvent juste à trouver l'existence ou l'absence d'une solution, et, si elles existent, à compter le nombre de solutions.
Dans l'illustration, x, y et z sont toutes des quantités différentes (dans ce cas, des nombres réels) représentées sous forme de poids circulaires, et chacun de x, y et z a un poids différent.
Par conséquent, l'équation avec R non spécifié est l'équation générale du cercle.
Le processus de recherche des solutions ou, dans le cas de paramètres, d'expression des inconnues en termes de paramètres, est appelé résolution de l'équation.
Multiplier ou diviser les deux membres d'une équation par une quantité non nulle.
Une équation algébrique est univariée si elle ne fait intervenir qu'une seule variable.
En mathématiques, la théorie des systèmes linéaires est la base et une partie fondamentale de l'algèbre linéaire, un sujet qui est utilisé dans la plupart des parties des mathématiques modernes.
Ce formalisme permet de déterminer les positions et les propriétés des foyers d'une conique.
Ce point de vue, esquissé par Descartes, enrichit et modifie le type de géométrie conçu par les mathématiciens grecs anciens.
Une équation diophantienne exponentielle est une équation pour laquelle les exposants des termes de l'équation peuvent être des inconnues.
La géométrie algébrique moderne est basée sur des techniques plus abstraites d'algèbre abstraite, en particulier l'algèbre commutative, avec le langage et les problèmes de géométrie.
Un point du plan appartient à une courbe algébrique si ses coordonnées vérifient une équation polynomiale donnée.
En mathématiques pures, les équations différentielles sont étudiées sous plusieurs angles différents, principalement concernés par leurs solutions - l'ensemble des fonctions qui satisfont l'équation.
Les équations différentielles linéaires, qui ont des solutions qui peuvent être additionnées et multipliées par des coefficients, sont bien définies et comprises, et des solutions exactes de forme fermée sont obtenues.
Les PDE peuvent être utilisés pour décrire une grande variété de phénomènes tels que le son, la chaleur, l'électrostatique, l'électrodynamique, l'écoulement des fluides, l'élasticité ou la mécanique quantique.
Une solution est une affectation de valeurs aux variables inconnues qui rend l'égalité dans l'équation vraie.
L'ensemble de toutes les solutions d'une équation est son ensemble de solutions.
Selon le contexte, résoudre une équation peut consister à trouver soit n'importe quelle solution (trouver une seule solution suffit), soit toutes les solutions, soit une solution qui satisfait d'autres propriétés, comme l'appartenance à un intervalle donné.
Dans ce cas, les solutions ne peuvent pas être listées.
La variété des types d'équations est grande, tout comme les méthodes correspondantes.
Cela peut être dû à un manque de connaissances mathématiques; certains problèmes n'ont été résolus qu'après des siècles d'efforts.
Les polynômes apparaissent dans de nombreux domaines des mathématiques et des sciences.
De nombreux auteurs utilisent indifféremment ces deux mots.
Formellement, le nom du polynôme est P, et non P(x), mais l'utilisation de la notation fonctionnelle P(x) date d'une époque où la distinction entre un polynôme et la fonction associée n'était pas claire.
Cependant, on peut l'utiliser sur n'importe quel domaine où l'addition et la multiplication sont définies (c'est-à-dire n'importe quel anneau).
Les polynômes de petit degré ont reçu des noms spécifiques.
Le polynôme 0, qui peut être considéré comme n'ayant aucun terme, est appelé le polynôme zéro.
Parce que le degré d'un polynôme non nul est le plus grand degré d'un terme, ce polynôme a le degré deux.
Les polynômes peuvent être classés par le nombre de termes avec des coefficients non nuls, de sorte qu'un polynôme à un terme est appelé un monôme, un polynôme à deux termes est appelé un binôme et un polynôme à trois termes est appelé un trinôme.
Lorsqu'il est utilisé pour définir une fonction, le domaine n'est pas aussi restreint.
Un polynôme en une indéterminée est appelé un polynôme univarié, un polynôme en plus d'une indéterminée est appelé un polynôme multivarié.
Dans le cas du corps des nombres complexes, les facteurs irréductibles sont linéaires.
Si le degré est supérieur à un, le graphe n'a pas d'asymptote.
En algèbre élémentaire, des méthodes telles que la formule quadratique sont enseignées pour résoudre toutes les équations polynomiales du premier degré et du second degré en une seule variable.
Cependant, des algorithmes de recherche de racine peuvent être utilisés pour trouver des approximations numériques des racines d'une expression polynomiale de n'importe quel degré.
Depuis le XVIe siècle, des formules similaires (utilisant des racines cubiques en plus des racines carrées), mais beaucoup plus compliquées sont connues pour les équations de degré trois et quatre (voir équation cubique et équation quartique).
En 1830, Évariste Galois prouve que la plupart des équations de degré supérieur à quatre ne peuvent pas être résolues par des radicaux, et montre que pour chaque équation, on peut décider si elle est résoluble par des radicaux, et, si c'est le cas, la résoudre.
Néanmoins, des formules pour les équations solubles de degrés 5 et 6 ont été publiées (voir fonction quintique et équation sextique).
Les algorithmes les plus efficaces permettent de résoudre facilement (sur ordinateur) des équations polynomiales de degré supérieur à 1 000 (voir Algorithme de recherche de racine).
Pour un ensemble d'équations polynomiales à plusieurs inconnues, il existe des algorithmes pour décider si elles ont un nombre fini de solutions complexes, et, si ce nombre est fini, pour calculer les solutions.
Une équation polynomiale dont on ne s'intéresse qu'aux solutions entières est appelée équation diophantienne.
Les coefficients peuvent être pris comme des nombres réels, pour les fonctions à valeurs réelles.
Cette équivalence explique pourquoi les combinaisons linéaires sont appelées polynômes.
Dans le cas de coefficients dans un anneau, "non constant" doit être remplacé par "non constant ou non unitaire" (les deux définitions s'accordent dans le cas de coefficients dans un champ).
Lorsque les coefficients appartiennent à des entiers, des nombres rationnels ou à un corps fini, il existe des algorithmes pour tester l'irréductibilité et calculer la factorisation en polynômes irréductibles (voir Factorisation des polynômes).
Le polynôme caractéristique d'une matrice ou d'un opérateur linéaire contient des informations sur les valeurs propres de l'opérateur.
Cependant, la notation élégante et pratique que nous utilisons aujourd'hui ne s'est développée qu'à partir du XVe siècle.
Cela "complète le carré", convertissant le côté gauche en un carré parfait.
Le théorème de Descartes stipule que pour quatre cercles s'embrassant (mutuellement tangents), leurs rayons satisfont une équation quadratique particulière.
Les mathématiciens babyloniens d'environ 400 avant JC et les mathématiciens chinois d'environ 200 avant JC ont utilisé des méthodes géométriques de dissection pour résoudre des équations quadratiques avec des racines positives.
Euclide, le mathématicien grec, a produit une méthode géométrique plus abstraite vers 300 av.
Al-Khwarizmi va plus loin en fournissant une solution complète à l'équation quadratique générale, acceptant une ou deux réponses numériques pour chaque équation quadratique, tout en fournissant des preuves géométriques dans le processus.
Abū Kāmil Shujā ibn Aslam (Égypte, 10e siècle) en particulier a été le premier à accepter les nombres irrationnels (souvent sous la forme d'une racine carrée, d'une racine cubique ou d'une racine quatrième) comme solutions d'équations quadratiques ou comme coefficients dans une équation.
Il a utilisé un premier méridien à travers les îles Canaries, de sorte que toutes les valeurs de longitude seraient positives.
Les astronomes hindous et musulmans ont continué à développer ces idées, ajoutant de nombreux nouveaux emplacements et améliorant souvent les données de Ptolémée.
À la fin du Moyen Âge, l'intérêt pour la géographie s'est ravivé en Occident, à mesure que les voyages augmentaient et que l'érudition arabe commençait à être connue grâce au contact avec l'Espagne et l'Afrique du Nord.
Christophe Colomb a fait deux tentatives d'utilisation des éclipses lunaires pour découvrir sa longitude, la première sur l'île de Saona, le 14 septembre 1494 (deuxième voyage), et la seconde en Jamaïque le 29 février 1504 (quatrième voyage).
Initialement un dispositif d'observation, les développements au cours du demi-siècle suivant l'ont transformé en un outil de mesure précis.
Sur terre, la période allant du développement des télescopes et des horloges à pendule jusqu'au milieu du XVIIIe siècle a vu une augmentation constante du nombre de lieux dont la longitude avait été déterminée avec une précision raisonnable, souvent avec des erreurs de moins d'un degré, et presque toujours à l'intérieur. 2-3°.
Faire des observations précises dans une houle océanique est beaucoup plus difficile que sur terre, et les horloges à pendule ne fonctionnent pas bien dans ces conditions.
Il proposait deux niveaux de récompenses, pour les solutions comprises entre 1° et 0,5°.
Ce travail a été soutenu et récompensé par des milliers de livres du Board of Longitude, mais il s'est battu pour recevoir de l'argent jusqu'à la récompense maximale de 20 000 £, recevant finalement un paiement supplémentaire en 1773 après l'intervention du parlement.
Les distances lunaires sont devenues d'usage général après 1790.
On s'est vite rendu compte que le télégraphe pouvait être utilisé pour transmettre un signal horaire pour la détermination de la longitude.
L'enquête a établi des chaînes d'emplacements cartographiés à travers l'Amérique centrale et du Sud, et les Antilles, et jusqu'au Japon et en Chine dans les années 1874–90.
Cela a changé lorsque la télégraphie sans fil est devenue disponible au début du XXe siècle.
Les systèmes de radionavigation ont été généralisés après la Seconde Guerre mondiale.
A l'exception de la déclinaison bamnétique, toutes les méthodes se sont avérées praticables.
La longitude à un point peut être déterminée en calculant la différence de temps entre celle à son emplacement et le temps universel coordonné (UTC).
Le mot proche est utilisé car le point peut ne pas être au centre du fuseau horaire ; de plus, les fuseaux horaires sont définis politiquement, de sorte que leurs centres et leurs limites ne se trouvent souvent pas sur des méridiens à des multiples de 15°.
La convention standard internationale (ISO 6709) - que l'Est est positif - est cohérente avec un système de coordonnées cartésien droitier, avec le pôle Nord vers le haut.
Ils sont depuis passés à l'approche standard.
Le géoïde est la forme que prendrait la surface de l'océan sous l'influence de la gravité de la Terre, y compris l'attraction gravitationnelle et la rotation de la Terre, si d'autres influences telles que les vents et les marées étaient absentes.
Il ne peut être connu que par des mesures et des calculs gravitationnels approfondis.
Bien que la Terre physique ait des excursions de +8 848 m (mont Everest) et −10 984 (fosse des Mariannes), l'écart du géoïde par rapport à un ellipsoïde varie de +85 m (Islande) à −106 m (sud de l'Inde), moins de 200 m au total .
Si les masses terrestres continentales étaient sillonnées par une série de tunnels ou de canaux, le niveau de la mer dans ces canaux coïnciderait également presque avec le géoïde.
Cela signifie qu'en voyageant par bateau, on ne remarque pas les ondulations du géoïde ; la verticale locale (fil à plomb) est toujours perpendiculaire au géoïde et l'horizon local lui est tangent.
En effet, les satellites GPS, en orbite autour du centre de gravité de la Terre, ne peuvent mesurer les hauteurs que par rapport à un ellipsoïde de référence géocentrique.
Les récepteurs GPS modernes ont une grille implémentée dans leur logiciel par laquelle ils obtiennent, à partir de la position actuelle, la hauteur du géoïde (par exemple le géoïde EGM-96) sur l'ellipsoïde du Système géodésique mondial (WGS).
Si cette sphère était alors recouverte d'eau, l'eau n'aurait pas la même hauteur partout.
C'est pourquoi de nombreux récepteurs GPS portables ont des tables de recherche d'ondulation intégrées pour déterminer la hauteur au-dessus du niveau de la mer.
Les premiers produits basés sur les données satellitaires GOCE sont devenus disponibles en ligne en juin 2010, via les outils de services aux utilisateurs d'observation de la Terre de l'Agence spatiale européenne (ESA).
Le géoïde est une surface équipotentielle particulière, et est quelque peu complexe à calculer.
Un globe est un modèle sphérique de la Terre, d'un autre corps céleste ou de la sphère céleste.
Un globe modèle de la sphère céleste est appelé globe céleste.
Il pourrait montrer les nations et les grandes villes et le réseau de lignes de latitude et de longitude.
En règle générale, il divisera également la sphère céleste en constellations.
La première mention connue d'un globe vient de Strabon, décrivant le globe des caisses d'environ 150 av.
De nombreux globes sont fabriqués avec une circonférence d'un mètre, ce sont donc des modèles de la Terre à l'échelle 1:40 millions.
La plupart des globes modernes sont également imprimés avec des parallèles et des méridiens, de sorte que l'on puisse indiquer les coordonnées approximatives d'un emplacement spécifique.
Les premiers globes terrestres représentant l'intégralité de l'Ancien Monde ont été construits dans le monde islamique.
Behaim était un cartographe, navigateur et marchand allemand.
Avant de construire le globe, Behaim avait beaucoup voyagé.
Un autre globe ancien, le Hunt – Lenox Globe, ca.
C'est peut-être le plus ancien globe à montrer le Nouveau Monde.
Un globe fac-similé montrant l'Amérique a été réalisé par Martin Waldseemueller en 1507.
Globus IMP, des dispositifs électromécaniques comprenant des globes de cinq pouces ont été utilisés dans les engins spatiaux soviétiques et russes de 1961 à 2002 comme instruments de navigation.
Cette méthode de fabrication du globe a été illustrée en 1802 dans une gravure de l'Encyclopédie anglaise de George Kearsley.
Celui-ci est placé dans une machine qui moule le disque en une forme hémisphérique.
Ces globes étaient « énormes » et très coûteux.
Ce dernier a un trou de balle soviétique à travers l'Allemagne.
Un grand cercle, également appelé orthodrome, d'une sphère est l'intersection de la sphère et d'un plan qui passe par le point central de la sphère.
Ce cas particulier d'un cercle d'une sphère s'oppose à un petit cercle, c'est-à-dire l'intersection de la sphère et d'un plan qui ne passe pas par le centre.
L'exception est une paire de points antipodaux, pour lesquels il existe une infinité de grands cercles.
La longueur de l'arc mineur d'un grand cercle est prise comme la distance entre deux points sur une surface d'une sphère en géométrie riemannienne où ces grands cercles sont appelés cercles riemanniens.
Un autre grand cercle est celui qui divise les hémisphères terrestre et aquatique.
En cartographie, une projection cartographique est un moyen d'aplatir la surface d'un globe dans un plan afin de créer une carte.
Selon l'objectif de la carte, certaines distorsions sont acceptables et d'autres non ; par conséquent, différentes projections cartographiques existent afin de préserver certaines propriétés du corps en forme de sphère au détriment d'autres propriétés.
Les projections font l'objet de plusieurs domaines mathématiques purs, notamment la géométrie différentielle, la géométrie projective et les variétés.
Au contraire, toute fonction mathématique qui transforme les coordonnées de la surface courbe distinctement et en douceur vers le plan est une projection.
La Terre et les autres grands corps célestes sont généralement mieux modélisés en tant que sphéroïdes aplatis, tandis que les petits objets tels que les astéroïdes ont souvent des formes irrégulières.
Parce que la surface courbe de la Terre n'est pas isométrique à un plan, la préservation des formes conduit inévitablement à une échelle variable et, par conséquent, à une présentation non proportionnelle des surfaces.
Le but de la carte détermine quelle projection doit former la base de la carte.
Les ensembles de données sont des informations géographiques ; leur collection dépend de la donnée choisie (modèle) de la Terre.
Comme l'indicatrice de Tissot, l'indicatrice de Goldberg-Gott est basée sur des infinitésimaux et représente les distorsions de flexion et d'asymétrie (flexion et déséquilibré).
Parfois, des triangles sphériques sont utilisés.
Une autre façon de visualiser la distorsion locale consiste à utiliser des niveaux de gris ou des dégradés de couleurs dont la nuance représente la bamnitude de la déformation angulaire ou de l'inflation surfacique.
Étant donné que la forme réelle de la Terre est irrégulière, des informations sont perdues à cette étape.
Pour comparer, on ne peut pas aplatir une peau d'orange sans la déchirer et la déformer.)
Tangente signifie que la surface touche mais ne coupe pas le globe ; sécante signifie que la surface tranche à travers le globe.
Si ces lignes sont un parallèle de latitude, comme dans les projections coniques, on parle de parallèle standard.
Ceci s'applique à toute saillie cylindrique ou pseudo-cylindrique d'aspect normal.
L'échelle est constante le long de toutes les lignes droites partant d'un emplacement géographique particulier.
Qu'ils soient sphériques ou ellipsoïdaux, les principes discutés tiennent sans perte de généralité.
Le modèle ellipsoïdal est couramment utilisé pour construire des cartes topographiques et pour d'autres cartes à grande et moyenne échelle qui doivent représenter avec précision la surface terrestre.
Par rapport à l'ellipsoïde le mieux ajusté, un modèle géoïdal modifierait la caractérisation de propriétés importantes telles que la distance, la conformité et l'équivalence.
Pour les corps planétaires irréguliers tels que les astéroïdes, cependant, des modèles analogues au géoïde sont parfois utilisés pour projeter des cartes.
Les projections sont décrites en termes de mise en contact d'une surface gigantesque avec la Terre, suivie d'une opération de mise à l'échelle implicite.
L'endroit où la source lumineuse émane le long de la ligne décrite dans cette dernière contrainte est ce qui produit les différences entre les diverses projections cylindriques "naturelles".
Ce cylindre est enroulé autour de la Terre, projeté dessus puis déroulé.
Distances nord-sud ni étirées ni comprimées (1) : projection équirectangulaire ou "plate carrée".
Étant donné que cette projection met à l'échelle les distances nord-sud par l'inverse de l'étirement est-ouest, elle préserve la surface au détriment des formes.
D'autres méridiens sont plus longs que le méridien central et s'inclinent vers l'extérieur, loin du méridien central.
Par conséquent, les méridiens sont équidistants le long d'un parallèle donné.
La carte conique résultante présente une faible distorsion d'échelle, de forme et de surface à proximité de ces parallèles standard.
Peut être construit à partir d'un point de perspective à une distance infinie du point tangent; r(d) = c sin .
Projection en perspective proche, qui simule la vue depuis l'espace à une distance finie et montre donc moins qu'un hémisphère complet, comme celle utilisée dans The Blue Marble 2012).
Le ou les points spéciaux peuvent être étirés en une ligne ou un segment de courbe lors de la projection.
Equidistance azimutale : les distances entre le centre et le bord sont conservées.
Ainsi, de nombreuses projections existent pour servir les multiples usages des cartes et leur vaste gamme d'échelles.
Des cartes de référence du monde apparaissent souvent sur des projections de compromis.
La projection de Mercator est une projection cartographique cylindrique présentée par le géographe et cartographe flamand Gerardus Mercator en 1569.
Comme effet secondaire, la projection de Mercator gonfle la taille des objets loin de l'équateur.
Cependant, étant donné la géométrie d'un cadran solaire, ces cartes pourraient bien avoir été basées sur la projection cylindrique centrale similaire, un cas limite de la projection gnomonique, qui est la base d'un cadran solaire.
Cependant, il s'agissait d'un cas simple et courant d'erreur d'identification.
Mercator a intitulé la carte : "Une description nouvelle et augmentée de la Terre corrigée à l'usage des marins".
Diverses hypothèses ont été avancées au fil des ans, mais en tout cas, l'amitié de Mercator avec Pedro Nunes et son accès aux tables loxodromiques créées par Nunes ont probablement aidé ses efforts.
Cependant, les mathématiques impliquées ont été développées mais jamais publiées par le mathématicien Thomas Harriot à partir de 1589 environ.
Deux problèmes principaux ont empêché son application immédiate: l'impossibilité de déterminer la longitude en mer avec une précision suffisante et le fait que des directions bamnétiques, au lieu de directions géographiques, étaient utilisées en navigation.
Cependant, il n'a commencé à dominer les cartes du monde qu'au XIXe siècle, lorsque le problème de la détermination de la position a été en grande partie résolu.
En raison de ces pressions, les éditeurs ont progressivement réduit leur utilisation de la projection au cours du XXe siècle.
Ce faisant, l'inévitable étirement est-ouest de la carte, qui augmente à mesure que la distance par rapport à l'équateur augmente, est accompagné dans la projection de Mercator par un étirement nord-sud correspondant, de sorte qu'à chaque point l'échelle est-ouest est identique à l'échelle nord-sud, ce qui en fait une projection cartographique conforme.
Aux latitudes supérieures à 70° nord ou sud, la projection de Mercator est pratiquement inutilisable, car l'échelle linéaire devient infiniment grande aux pôles.
L'île d'Ellesmere, au nord de l'archipel arctique canadien, a à peu près la même taille que l'Australie, bien que l'Australie soit 39 fois plus grande.
La superficie réelle du Groenland est comparable à celle de la seule République démocratique du Congo.
L'Alaska semble être de la même taille que l'Australie, bien que l'Australie soit en réalité 4,5 fois plus grande.
La Suède apparaît beaucoup plus grande que Madagascar.
Une carte du monde sur un icosaèdre régulier par projection gnomonique."
À la suite de ces critiques, les atlas modernes n'utilisent plus la projection de Mercator pour les cartes du monde ou pour les zones éloignées de l'équateur, préférant d'autres projections cylindriques ou des formes de projection à surface égale.
Arno Peters a suscité la controverse à partir de 1972 lorsqu'il a proposé ce que l'on appelle maintenant la projection Gall-Peters pour remédier aux problèmes du Mercator, affirmant qu'il s'agissait de son propre travail original sans faire référence à des travaux antérieurs de cartographes tels que le travail de Gall de 1855.
La plage pour a parmi les choix possibles est d'environ 35 km, mais pour les applications à petite échelle (grande région), cette variation peut être ignorée, et des valeurs moyennes de 6 371 km et 40 030 km peuvent être prises pour le rayon et la circonférence respectivement.
Une projection cartographique cylindrique est spécifiée par des formules reliant les coordonnées géographiques de latitude φ et de longitude λ aux coordonnées cartésiennes sur la carte avec l'origine sur l'équateur et l'axe des x le long de l'équateur.
Puisque le cylindre est tangent au globe à l'équateur, le facteur d'échelle entre le globe et le cylindre est l'unité sur l'équateur mais nulle part ailleurs.
La différence ( λ − λ 0) est en radians.
Des troncatures encore plus extrêmes ont été utilisées: un atlas scolaire finlandais a été tronqué à environ 76 ° N et 56 ° S, un rapport d'aspect de 1,97.
Les bandes plus étroites sont meilleures : sec 8° = 1,01, donc une bande de largeur 16° (centrée sur l'équateur) est précise à 1 % près ou 1 partie sur 100.
La valeur de e2 est d'environ 0,006 pour tous les ellipsoïdes de référence.)
Pour le modèle ci-dessus, 1 cm correspond à 1 500 km à une latitude de 60°.
Cette corde sous-tend un angle au centre égal à 2arcsin(cos φ sin ) et la distance orthodromique entre A et B est de 2a arcsin(cos φ sin ).)
Pour les autres corps, une caractéristique de surface fixe est généralement référencée, qui pour Mars est le méridien passant par le cratère Airy-0.
Par convention pour la Terre, la Lune et le Soleil, elle est exprimée en degrés allant de -180° à +180°. Pour les autres corps, une plage de 0° à 360° est utilisée.
L'échelle d'une carte est le rapport d'une distance sur la carte à la distance correspondante sur le terrain.
La première façon est le rapport de la taille du globe générateur à la taille de la Terre.
De nombreuses cartes indiquent l'échelle nominale et peuvent même afficher une échelle à barres (parfois simplement appelée «échelle») pour la représenter.
Dans ce cas, "échelle" désigne le facteur d'échelle (= échelle de points = échelle particulière).
La projection cartographique devient essentielle pour comprendre comment l'échelle varie sur toute la carte.
Il s'agit d'une étude de pratiquement toutes les projections connues de l'Antiquité à 1993.
La petite échelle fait référence aux cartes du monde ou aux cartes de grandes régions telles que les continents ou les grandes nations.
Les cartes à grande échelle montrent des zones plus petites de manière plus détaillée, comme les cartes de comté ou les plans de ville.
Cependant, comme expliqué ci-dessus, les cartographes utilisent le terme "à grande échelle" pour désigner des cartes moins étendues - celles qui montrent une zone plus petite.
Ceci est couramment illustré par l'impossibilité de lisser une peau d'orange sur une surface plane sans la déchirer et la déformer.
Inversement, les facteurs d'échelle isotropes sur la carte impliquent une projection conforme.
La qualification "petit" signifie qu'à une certaine précision de mesure donnée, aucun changement ne peut être détecté dans le facteur d'échelle sur l'élément.
Nous disons que ces coordonnées définissent la carte de projection qui doit être distinguée logiquement des cartes réelles imprimées (ou visualisées).
Étant donné que l'échelle des points varie avec la position et la direction, la projection du cercle sur la projection sera déformée.
La superposition de ces ellipses de distorsion sur la projection cartographique indique la manière dont l'échelle des points change sur la carte.
Le rapport du grand axe au petit axe est .
L'échelle est vraie (k = 1) sur l'équateur de sorte que la multiplication de sa longueur sur une carte imprimée par l'inverse du RF (ou échelle principale) donne la circonférence réelle de la Terre.
Le graphique du haut montre la fonction isotrope de l'échelle de Mercator : l'échelle sur le parallèle est la même que l'échelle sur le méridien.
Par conséquent, la projection tangente de Mercator est très précise dans une bande de largeur 3,24 degrés centrée sur l'équateur.
Ces observations ont incité le développement des projections transversales de Mercator dans lesquelles un méridien est traité « comme un équateur » de la projection afin que nous obtenions une carte précise à une distance étroite de ce méridien.
Les quatre directions cardinales, ou points cardinaux, sont les quatre directions principales de la boussole : nord, est, sud et ouest, généralement désignées respectivement par leurs initiales N, E, S et W.
En voyageant vers l'est ou l'ouest, c'est seulement sur l'équateur que l'on peut garder l'est ou l'ouest et aller tout droit (sans avoir besoin de barrer).
Le pôle nord de l'aiguille bamnétique pointe vers le pôle nord géographique de la terre et vice versa.
En milieu de journée, c'est au sud pour les téléspectateurs de l'hémisphère nord, qui vivent au nord du tropique du Cancer, et au nord pour ceux de l'hémisphère sud, qui vivent au sud du tropique du Capricorne.
À ces endroits, il faut d'abord déterminer si le soleil se déplace d'est en ouest en passant par le nord ou le sud en observant ses mouvements - de gauche à droite signifie qu'il passe par le sud tandis que de droite à gauche signifie qu'il passe par le nord ; ou on peut regarder les ombres du soleil.
En raison de l'inclinaison axiale de la Terre, quel que soit l'endroit où se trouve le spectateur, il n'y a que deux jours par an où le soleil se lève précisément plein est.
Pour que cette méthode fonctionne dans l'hémisphère sud, le 12 est pointé vers le Soleil et le point à mi-chemin entre l'aiguille des heures et 12 heures indiquera le nord.
Cet axe coupe la sphère céleste aux pôles célestes nord et sud, qui semblent à l'observateur se situer directement au-dessus du nord et du sud respectivement à l'horizon.
La photographie qui en résulte révèle une multitude d'arcs concentriques (portions de cercles parfaits) à partir desquels le centre exact peut être facilement dérivé, et qui correspond au pôle céleste, qui se trouve directement au-dessus de la position du vrai pôle (nord ou sud) sur le horizon.
La position exacte du pôle change au fil des milliers d'années en raison de la précession des équinoxes.
L'astérisme "Big Dipper" peut être utilisé pour trouver Polaris.
Puisqu'il trouve le nord vrai, plutôt que bamnétique, il est immunisé contre les interférences des champs bamnétique locaux ou à bord du navire.
La plupart des cartes de l'Europe médiévale, par exemple, étaient placées à l'est (E) en haut.
Les cartes topographiques incluent l'élévation, généralement via des courbes de niveau.
Le point Nord sera alors le point du limbe le plus proche du pôle Nord céleste.
En faisant le tour du disque dans le sens des aiguilles d'une montre à partir du point Nord, on rencontre dans l'ordre le point Ouest, le point Sud, puis le point Est.
Dans l'Europe pré-moderne plus généralement, entre huit et 32 points cardinaux - directions cardinales et intercardinales - ont reçu des noms.
Les systèmes à cinq points cardinaux (quatre directions et le centre) incluent ceux de la Chine pré-moderne, ainsi que les cultures traditionnelles turques, tibétaines et aïnoues.
Certains peuvent également inclure "au-dessus" et "en dessous" comme directions, et donc se concentrer sur une cosmologie de sept directions.
Le nord est associé à l'Himalaya et au paradis tandis que le sud est associé au monde souterrain ou à la terre des pères (Pitr loka).
Le nord est l'un des quatre points cardinaux ou points cardinaux.
Septentrionalis vient de septentriones, "les sept bœufs de labour", un nom d'Ursa Major.
Par exemple, en lezgien, kefer peut signifier à la fois «incrédulité» et «nord», car au nord de la patrie musulmane lezgienne se trouvent des zones autrefois habitées par des peuples caucasiens et turcs non musulmans.
Sur tout objet astronomique en rotation, le nord désigne souvent le côté qui semble tourner dans le sens inverse des aiguilles d'une montre lorsqu'il est vu de loin le long de l'axe de rotation.
Mais de simples généralisations sur le sujet doivent être considérées comme mal fondées et comme susceptibles de refléter les idées fausses populaires sur le bamnétisme terrestre.
Cette convention s'est développée à partir de l'utilisation d'une boussole, qui place le nord en haut.
95 % des habitants du Nord ont suffisamment de nourriture et d'abris, et un système éducatif fonctionnel.
L'utilisation du terme « Sud » peut également être relative au pays, en particulier dans les cas de fracture économique ou culturelle notable.
Rarement le sens s'élargit à la Bolivie, et dans le sens le plus restreint il ne couvre que le Chili, l'Argentine et l'Uruguay.
L'ouest est la direction opposée à celle de la rotation de la Terre sur son axe, et est donc la direction générale vers laquelle le Soleil semble progresser constamment et finalement se coucher.
Dans l'Égypte ancienne, l'Occident était considéré comme le portail vers l'au-delà et la direction cardinale considérée en relation avec la mort, mais pas toujours avec une connotation négative.
Dans le judaïsme, l'ouest est considéré comme étant vers la Shekinah (présence) de Dieu, comme dans l'histoire juive, le tabernacle et le temple de Jérusalem qui a suivi faisaient face à l'est, avec la présence de Dieu dans le Saint des Saints sur les marches à l'ouest.
Le cercle polaire arctique est l'un des deux cercles polaires et le plus au nord des cinq grands cercles de latitude, comme indiqué sur les cartes de la Terre.
Un cercle de latitude ou une ligne de latitude sur Terre est un petit cercle abstrait est-ouest reliant tous les emplacements autour de la Terre (sans tenir compte de l'élévation) à une ligne de coordonnées de latitude donnée.
Les cercles de latitude sont différents des cercles de longitude, qui sont tous de grands cercles avec le centre de la Terre au milieu, car les cercles de latitude deviennent plus petits à mesure que la distance à l'équateur augmente.
Un cercle de latitude est perpendiculaire à tous les méridiens.
L'équateur est le plus long cercle de latitude et est le seul cercle de latitude qui est également un grand cercle.
Sur une carte, les cercles de latitude peuvent être parallèles ou non, et leur espacement peut varier, selon la projection utilisée pour cartographier la surface de la Terre sur un plan.
Par exemple, sur une projection Mercator, les cercles de latitude sont plus espacés près des pôles pour préserver les échelles et les formes locales, tandis que sur une projection Gall-Peters, les cercles de latitude sont plus espacés près des pôles, de sorte que les comparaisons de surface seront précis.
Il existe de nombreux termes plus petits, ce qui entraîne des décalages quotidiens variables de quelques mètres dans n'importe quelle direction.
54°40'N La frontière entre les territoires russes du XIXe siècle au nord et les revendications territoriales américaines et britanniques conflictuelles dans l'ouest de l'Amérique du Nord.
43°30'N Aux États-Unis, la frontière entre le Minnesota et l'Iowa.
42°N À l'origine la limite nord de la Nouvelle-Espagne.
41°N Aux États-Unis, une partie de la frontière entre le Wyoming et l'Utah, la frontière entre le Wyoming et le Colorado et une partie de la frontière entre le Nebraska et le Colorado.
38°N La frontière entre les zones d'occupation soviétique et américaine en Corée, puis entre la Corée du Nord et la Corée du Sud, de 1945 jusqu'à la guerre de Corée (1950-1953).
Géographiquement, c'est une extension vers l'ouest de la frontière entre la Virginie et la Caroline du Nord et une partie de la frontière entre le Kentucky et le Tennessee.
Aussi, une partie de la frontière entre la Caroline du Nord et la Géorgie.
32°N Aux États-Unis, partie de la frontière entre le Nouveau-Mexique et le Texas.
25°N Partie de la frontière entre la Mauritanie et le Mali.
17°N Division entre la République du Vietnam (Sud Vietnam) et la République Démocratique du Vietnam (Nord Vietnam) pendant la guerre du Vietnam.
8°N Partie de la frontière entre la Somalie et l'Éthiopie.
7°SA court tronçon de la frontière entre la République démocratique du Congo et l'Angola.
Les arts sont un très large éventail de pratiques humaines d'expression créative, de narration et de participation culturelle.
Ils peuvent utiliser leurs compétences et leur ibamination pour produire des objets, des performances, transmettre des idées et des expériences et construire de nouveaux environnements et espaces.
Ils peuvent également développer ou contribuer à un aspect particulier d'une forme d'art plus complexe, comme en cinématographie.
Le premier sens du mot art est « manière de faire ».
Dans sa définition abstraite la plus basique, l'art est une expression documentée d'un être sensible à travers ou sur un support accessible afin que n'importe qui puisse le voir, l'entendre ou l'expérimenter.
Cette notation publique dépend de divers facteurs subjectifs.
Dans la Grèce antique, tout l'art et l'artisanat étaient désignés par le même mot, tecbam.
L'art romain antique dépeignait les dieux comme des humains idéalisés, représentés avec des traits distinctifs caractéristiques (par exemple, la foudre de Zeus).
Une caractéristique de ce style est que la couleur locale est souvent définie par un contour (un équivalent contemporain est le dessin animé).
Dans le milieu universitaire moderne, les arts sont généralement regroupés avec ou en tant que sous-ensemble des sciences humaines.
Le mot architecture vient du grec arkhitekton, "maître bâtisseur, directeur des travaux", de αρχι - (arkhi) "chef" + τεκτων (tekton) "constructeur, charpentier".
Dans l'usage moderne, l'architecture est l'art et la discipline de créer, ou de déduire un plan implicite ou apparent, d'un objet ou d'un système complexe.
L'architecture planifiée manipule l'espace, le volume, la texture, la lumière, l'ombre ou des éléments abstraits afin d'obtenir une esthétique agréable.
Alors que certains produits en céramique sont considérés comme des beaux-arts, certains sont considérés comme des objets d'art décoratifs, industriels ou appliqués.
Dans une usine de poterie ou de céramique, un groupe de personnes conçoit, fabrique et décore la poterie.
Cela implique généralement de faire des marques sur une surface en appliquant une pression à partir d'un outil ou en déplaçant un outil sur une surface.
Les principales techniques utilisées dans le dessin sont le dessin au trait, les hachures, les hachures croisées, les hachures aléatoires, le gribouillage, le pointillé et le mélange.
Les peintures peuvent être naturalistes et figuratives (comme dans une nature morte ou une peinture de paysage), photographiques, abstraites, narratives, symboliques (comme dans l'art symboliste), émotives (comme dans l'expressionnisme) ou de nature politique (comme dans l'artivisme).
Le nom "littérature" vient du mot latin littera qui signifie "un caractère écrit individuel (lettre)".
Chaque discipline des arts de la scène est de nature temporelle, ce qui signifie que le produit est exécuté sur une période de temps.
La danse est également utilisée pour décrire les méthodes de communication non verbale (voir le langage corporel) entre les humains ou les animaux (par exemple, la danse des abeilles, la danse de l'accouplement), le mouvement d'objets inanimés (par exemple, les feuilles dansées dans le vent) et certaines formes ou genres musicaux. .
La création, l'interprétation, la signification et même la définition de la musique varient selon la culture et le contexte social.
Le compositeur Richard Wagner a reconnu la fusion de tant de disciplines en une seule œuvre d'opéra, illustrée par son cycle Der Ring des Nibelungen ("L'Anneau du Nibelung").
D'autres œuvres de la fin des XIXe, XXe et XXIe siècles ont fusionné d'autres disciplines de manière unique et créative, comme l'art de la performance.
John Cage est considéré par beaucoup comme un artiste de performance plutôt que comme un compositeur, bien qu'il ait préféré ce dernier terme.
Les arts appliqués comprennent des domaines tels que le design industriel, l'illustration et l'art commercial.
Au sein des sciences sociales, les économistes culturels montrent comment jouer aux jeux vidéo est propice à l'implication dans des formes d'art et des pratiques culturelles plus traditionnelles, ce qui suggère la complémentarité entre les jeux vidéo et les arts.
L'architecture (du latin architectura, du grec ἀρχιτέκτων arkhitekton "architecte", de ἀρχι - "chef" et τέκτων "créateur") est à la fois le processus et le produit de la planification, de la conception et de la construction de bâtiments ou d'autres structures.
La pratique, qui a commencé à l'ère préhistorique, a été utilisée comme moyen d'expression de la culture pour les civilisations des sept continents.
Au 19ème siècle, Louis Sullivan a déclaré que "la forme suit la fonction". "
L'architecture a commencé comme une architecture vernaculaire rurale et orale qui s'est développée à partir d'essais et d'erreurs jusqu'à une reproduction réussie.
Au cours du Moyen Âge européen, des styles paneuropéens de cathédrales et d'abbayes romanes et gothiques ont émergé tandis que la Renaissance a favorisé les formes classiques mises en œuvre par des architectes connus par leur nom.
L'accent a été mis sur les techniques modernes, les matériaux et les formes géométriques simplifiées, ouvrant la voie à des superstructures de grande hauteur.
Forme ou structure unificatrice ou cohérente.
L'aspect le plus important de la beauté était donc une partie inhérente d'un objet, plutôt que quelque chose appliqué superficiellement, et était basé sur des vérités universelles et reconnaissables.
Au XVIe siècle, l'architecte, peintre et théoricien maniériste italien Sebastiano Serlio a écrit Tutte L'Opere D'Architettura et Prospetiva (Œuvres complètes sur l'architecture et la perspective).
L'architecture gothique, croyait Pugin, était la seule «véritable forme chrétienne d'architecture».
Parmi les philosophies qui ont influencé les architectes modernes et leur approche de la conception des bâtiments figurent le rationalisme, l'empirisme, le structuralisme, le poststructuralisme, la déconstruction et la phénoménologie.
L'architecture et l'urbanisme des civilisations classiques telles que la grecque et la romaine ont évolué à partir d'idéaux civiques plutôt que religieux ou empiriques et de nouveaux types de construction ont émergé.
Des textes sur l'architecture ont été écrits depuis l'Antiquité.
L'architecture bouddhique, en particulier, présentait une grande diversité régionale.
Le rôle d'architecte ne faisait généralement qu'un avec celui de maître maçon, ou Magister lathomorum comme ils sont parfois décrits dans les documents contemporains.
Les bâtiments étaient attribués à des architectes spécifiques – Brunelleschi, Alberti, Michelangelo, Palladio – et le culte de l'individu avait commencé.
La formation formelle en architecture au XIXe siècle, par exemple à l'École des Beaux-Arts en France, accordait beaucoup d'importance à la production de beaux dessins et peu au contexte et à la faisabilité.
Parmi ceux-ci, notons le Deutscher Werkbund, créé en 1907 pour produire des objets fabriqués à la machine de meilleure qualité.
Lorsque l'architecture moderne a été pratiquée pour la première fois, c'était un mouvement d'avant-garde avec des fondements moraux, philosophiques et esthétiques.
L'approche des architectes modernistes était de réduire les bâtiments à des formes pures, supprimant les références historiques et l'ornement au profit de détails fonctionnels.
Des architectes tels que Mies van der Rohe, Philip Johnson et Marcel Breuer ont travaillé pour créer une beauté basée sur les qualités inhérentes des matériaux de construction et des techniques de construction modernes, échangeant des formes historiques traditionnelles contre des formes géométriques simplifiées, célébrant les nouveaux moyens et méthodes rendus possibles par l'Industrial Révolution, notamment la construction à ossature métallique, qui a donné naissance aux superstructures de grande hauteur.
Les processus préparatoires à la conception de tout grand bâtiment sont devenus de plus en plus compliqués et nécessitent des études préliminaires sur des questions telles que la durabilité, la durabilité, la qualité, l'argent et le respect des lois locales.
La durabilité environnementale est devenue une question courante, avec un effet profond sur la profession d'architecte.
Ce changement majeur dans l'architecture a également changé les écoles d'architecture pour se concentrer davantage sur l'environnement.
Le système d'évaluation LEED (Leadership in Energy and Environmental Design) du US Green Building Council a joué un rôle déterminant à cet égard.
Il peut également s'agir de la conception initiale et du plan d'utilisation, puis d'une refonte ultérieure pour s'adapter à un objectif modifié, ou d'une conception considérablement révisée pour une réutilisation adaptative de l'enveloppe du bâtiment.
La conception préliminaire du navire, sa conception détaillée, sa construction, ses essais, son exploitation et sa maintenance, sa mise à l'eau et sa mise en cale sèche sont les principales activités concernées.
À l'inverse, l'architecture sacrée en tant que lieu de méta-intimité peut également être non monolithique, éphémère et intensément privée, personnelle et non publique.
Avec la montée du christianisme et de l'islam, les édifices religieux sont devenus de plus en plus des centres de culte, de prière et de méditation.
L'Inde était sillonnée par des routes commerciales de marchands d'aussi loin que Siraf et la Chine, ainsi que par des invasions étrangères, entraînant de multiples influences d'éléments étrangers sur les styles indigènes.
Un exemple existant est à Nalanda (Bihar).
Conformément aux changements dans la pratique religieuse, les stupas ont été progressivement incorporés dans les chaitya-grihas (salles de stupa).
Les temples bouddhistes ont été développés un peu plus tard et en dehors de l'Asie du Sud, où le bouddhisme a progressivement décliné à partir des premiers siècles de notre ère, bien qu'un des premiers exemples soit celui du temple Mahabodhi à Bodh Gaya dans le Bihar.
Dans la croyance hindoue, le temple représente le macrocosme de l'univers ainsi que le microcosme de l'espace intérieur.
Il a évolué sur une période de plus de 2000 ans.
De plus, la brique a remplacé la pierre, l'ordre classique a été moins strictement observé, les mosaïques ont remplacé la décoration sculptée et des dômes complexes ont été érigés.
Les premiers styles de l'architecture islamique ont produit des mosquées de «plan arabe» ou hypostyles pendant la dynastie des Omeyyades.
Dans les mosquées iwan, un ou plusieurs iwans font face à une cour centrale qui sert de salle de prière.
Le sommet du minaret est toujours le point le plus élevé dans les mosquées qui en ont un, et souvent le point le plus élevé de la zone immédiate.
Par conséquent, les architectes de la mosquée ont emprunté la forme du clocher pour leurs minarets, qui ont été utilisés essentiellement dans le même but - appeler les fidèles à la prière.
Bien que les dômes aient normalement pris la forme d'un hémisphère, les Moghols en Inde ont popularisé les dômes en forme d'oignon en Asie du Sud et en Perse.
Habituellement en face de l'entrée de la salle de prière se trouve le mur de la qibla, qui est la zone visuellement mise en valeur à l'intérieur de la salle de prière.
Dans le mur de la qibla, généralement en son centre, se trouve le mihrab, une niche ou une dépression indiquant le mur de la qibla.
Le mihrab sert de lieu où l'imam dirige régulièrement les cinq prières quotidiennes.
Il se compose d'une nef, de transepts et de l'autel se dresse à l'extrémité est (voir schéma de la cathédrale).
La plupart des historiens de l'architecture considèrent la conception de Michel-Ange de la basilique Saint-Pierre de Rome comme un précurseur du style baroque ; cela peut être reconnu par des espaces intérieurs plus larges (remplaçant de longues nefs étroites), une attention plus ludique à la lumière et à l'ombre, une ornementation étendue, de grandes fresques, une concentration sur l'art intérieur et, fréquemment, une projection extérieure centrale dramatique.
Alors que les structures laïques ont clairement eu la plus grande influence sur le développement de l'architecture moderne, plusieurs excellents exemples d'architecture moderne peuvent être trouvés dans les édifices religieux du XXe siècle.
Il a été décrit comme une "phalange de combattants" tournés sur leurs queues et pointant vers le ciel.
Le temple d'Independence, Missouri, a été conçu par l'architecte japonais Gyo Obata d'après le concept du nautile chambré.
La Basilique Notre-Dame de Licheń, quant à elle, est un édifice beaucoup plus traditionnel.
Un style architectural est un ensemble de caractéristiques et de caractéristiques qui rendent un bâtiment ou une autre structure remarquable ou historiquement identifiable.
La plupart de l'architecture peut être classée dans une chronologie de styles qui change au fil du temps, reflétant l'évolution des modes, des croyances et des religions, ou l'émergence de nouvelles idées, technologies ou matériaux qui rendent de nouveaux styles possibles.
À tout moment, plusieurs styles peuvent être à la mode, et lorsqu'un style change, il le fait généralement progressivement, à mesure que les architectes apprennent et s'adaptent aux nouvelles idées.
Par exemple, les idées de la Renaissance ont émergé en Italie vers 1425 et se sont répandues dans toute l'Europe au cours des 200 années suivantes, les Renaissances française, allemande, anglaise et espagnole montrant clairement le même style, mais avec des caractéristiques uniques.
Après qu'un style architectural soit passé de mode, des renaissances et des réinterprétations peuvent se produire.
Le style de mission espagnol a été relancé 100 ans plus tard sous le nom de Mission Revival, et cela a rapidement évolué vers le renouveau colonial espagnol.
Un exemple d'architecture maniériste est la Villa Farnèse à Caprarola dans la campagne accidentée à l'extérieur de Rome.
Grâce à Anvers, les styles Renaissance et maniériste ont été largement introduits en Angleterre, en Allemagne et en Europe du Nord et de l'Est en général.
L'idéal d'harmonie de la Renaissance a cédé la place à des rythmes plus libres et plus baminatifs.
La théorie architecturale est l'acte de penser, de discuter et d'écrire sur l'architecture.
La théorie de l'architecture est souvent didactique et les théoriciens ont tendance à rester proches ou à travailler au sein des écoles.
Cela ne signifie pas pour autant que de telles œuvres n'existaient pas, étant donné que de nombreuses œuvres n'ont jamais survécu à l'Antiquité.
Probablement écrit entre 27 et 23 avant JC, c'est la seule grande source contemporaine sur l'architecture classique à avoir survécu.
Il propose également les trois lois fondamentales auxquelles l'architecture doit obéir pour être considérée comme telle : firmitas, utilitas, venustas, traduites au XVIIe siècle par Sir Henry Wotton dans le slogan anglais fermeté, marchandise et délice (signifiant adéquation structurelle, adéquation fonctionnelle). , et beauté).
Comme les théories architecturales portaient sur les structures, moins d'entre elles ont été transcrites.
Ces théories ont anticipé le développement du fonctionnalisme dans l'architecture moderne.
Cela a à son tour formé la base de l'Art Nouveau au Royaume-Uni, illustré par le travail de Charles Rennie Mackintosh, et a influencé la Sécession viennoise.
La génération née au cours du tiers moyen du XIXe siècle a été largement fascinée par les opportunités présentées par la combinaison de Semper d'une portée historique à couper le souffle et d'une granularité méthodologique.
Le mouvement moderne a rejeté ces pensées et Le Corbusier a énergiquement rejeté l'œuvre.
Un autre théoricien de la planification influent de cette époque était Ebenezer Howard, qui a fondé le mouvement des cités-jardins.
Une première utilisation du terme architecture moderne dans l'imprimé s'est produite dans le titre d'un livre d' Otto Wagner , qui a donné des exemples de son propre travail représentatif de la Sécession viennoise avec des illustrations art nouveau et des enseignements didactiques à ses étudiants.
Frank Lloyd Wright, bien que moderne dans son rejet du revivalisme historique, était idiosyncrasique dans sa théorie, qu'il a transmise dans de nombreux écrits.
Wright était plus poétique et maintenait fermement la vision du XIXe siècle de l'artiste créateur en tant que génie unique.
Cela a également été le cas d'éducateurs universitaires comme Dalibor Vesely ou Alberto-Perez Gomez, et ces dernières années cette orientation philosophique a été renforcée par la recherche d'une nouvelle génération de théoriciens (EG Jeffrey Kipnis ou Sanford Kwinter).
D'autres, comme Beatriz Colomina et Mary McLeod, élargissent la compréhension historique de l'architecture pour inclure des discours moindres ou mineurs qui ont influencé le développement des idées architecturales au fil du temps.
Dans leurs théories, l'architecture est comparée à un langage qui peut être inventé et réinventé à chaque utilisation.
Depuis 2000, la théorie architecturale a également dû faire face à la montée rapide de l'urbanisme et de la mondialisation.
Au cours de la dernière décennie, il y a eu l'émergence de l'architecture dite "numérique".
Les architectes conçoivent également des bâtiments d'aspect organique dans le but de développer un nouveau langage formel.
Depuis l'émergence de ces nouvelles tendances architecturales, de nombreux théoriciens et architectes ont travaillé sur ces questions, développant des théories et des idées telles que le paramétrisme de Patrick Schumacher.
L'architecture byzantine est l'architecture de l'Empire byzantin, ou Empire romain d'Orient.
De magnifiques mosaïques dorées d'une simplicité graphique apportaient lumière et chaleur au cœur des églises.
Certaines des colonnes étaient également en marbre.
Meubles en bois précieux, comme des lits, des chaises, des tabourets, des tables, des étagères et des coupes en argent ou en or avec de beaux reliefs, décorés d'intérieurs byzantins.
Pour les temples classiques, seul l'extérieur importait, car seuls les prêtres pénétraient à l'intérieur, où était conservée la statue de la divinité à laquelle le temple était dédié.
Ceux de la cathédrale Saint-Marc de Venise (1071) ont particulièrement attiré l'attention de John Ruskin.
Sur les colonnes orientales, l'aigle, le lion et l'agneau sont parfois sculptés, mais traités de manière conventionnelle.
Des colonnes composites bordent l'espace principal de la nef.
Les colonnes sont remplies de feuillage dans toutes sortes de variations.
D'autres structures incluent les ruines du Grand Palais de Constantinople, les murs innovants de Constantinople (avec 192 tours) et la Citerne Basilique (avec des centaines de colonnes classiques recyclées).
La période paléologue est bien représentée dans une douzaine d'anciennes églises d'Istanbul, notamment St Sauveur à Chora et St Mary Pammakaristos.
L'église des Saints-Apôtres (Thessalonique) est citée comme une structure archétypale de la période tardive avec ses murs extérieurs richement décorés de motifs complexes en briques ou de céramiques émaillées.
A Saint Serge, Constantinople et San Vitale, Ravenne, églises de type central, l'espace sous le dôme a été agrandi en faisant ajouter des abside à l'octogone.
Cette zone ininterrompue, d'environ 260 pieds (80 m) de long, dont la plus grande partie mesure plus de 100 pieds (30 m) de large, est entièrement couverte par un système de surfaces en dôme.
Aux Saints Apôtres (VIe siècle) cinq dômes étaient appliqués sur un plan cruciforme ; le dôme central était le plus élevé.
Tantôt l'espace central était carré, tantôt octogonal, ou du moins il y avait huit piliers soutenant le dôme au lieu de quatre, et la nef et les transepts étaient plus étroits en proportion.
Toujours devant mettre une cour carrée.
Directement sous le centre du dôme se trouve l'ambon, à partir duquel les Écritures étaient proclamées, et sous l'ambon au niveau du sol se trouvait la place du chœur des chanteurs.
Des rangées de sièges en hausse autour de la courbe de l'abside avec le trône du patriarche à la pointe du Moyen-Orient formaient le synthronon.
Les dômes et les voûtes à l'extérieur étaient recouverts de plomb ou de tuiles de type romain.
Il existe des influences byzantines considérables qui peuvent être détectées dans les premiers monuments islamiques distinctifs en Syrie (709–715).
Des briques de 70 cm x 35 cm x 5 cm ont été utilisées, et ces briques ont été collées ensemble à l'aide de mortier d'environ 5 cm d'épaisseur.
La caractéristique la plus précise de Hagia Irene est peut-être le contraste strict entre le design intérieur et extérieur.
Ce style a influencé la construction de plusieurs autres bâtiments, comme la basilique Saint-Pierre.
La construction de la version finale de Sainte-Sophie, qui existe encore aujourd'hui, a été supervisée par l'empereur Justinien.
L'architecture gothique (ou architecture pointue) est un style architectural particulièrement populaire en Europe de la fin du XIIe siècle au XVIe siècle, pendant le Haut et le Bas Moyen Âge, survivant jusqu'aux XVIIe et XVIIIe siècles dans certaines régions.
Le style à l'époque était parfois connu sous le nom d'opus Francigenum (lit.
La principale innovation technique et l'un des autres composants de conception caractéristiques est l'arc-boutant.
Cependant, rien n'indique qu'il y ait eu un lien entre l'architecture arménienne et le développement du style gothique en Europe occidentale.
Ainsi, le style gothique, s'opposant à l'architecture classique, était de ce point de vue associé à la destruction de l'avancement et de la sophistication.
Le terme « sarrasin » était encore utilisé au 18e siècle et faisait généralement référence à tous les conquérants musulmans, y compris les Maures et les Arabes.
Son aversion pour le style était si forte qu'il a refusé de mettre un toit gothique sur le nouveau Saint-Paul, malgré les pressions exercées pour le faire.
Plusieurs auteurs ont pris position contre cette allégation, affirmant que le style gothique s'était très probablement infiltré en Europe par d'autres moyens, par exemple via l'Espagne ou la Sicile.
Il a également été influencé par les doctrines théologiques qui réclamaient plus de lumière et par des améliorations techniques des voûtes et des contreforts qui permettaient une hauteur beaucoup plus grande et des fenêtres plus grandes.
Des voûtes d'ogive ont été employées dans certaines parties de la cathédrale de Durham (1093–) et de l'abbaye de Lessay en Normandie (1098).
Le duché de Normandie, qui faisait partie de l'empire angevin jusqu'au XIIIe siècle, a développé sa propre version du gothique.
Un exemple du gothique normand primitif est la cathédrale de Bayeux (1060-1070) où la nef et le chœur romans de la cathédrale ont été reconstruits dans le style gothique.
La cathédrale de Coutances a été refaite en gothique à partir de 1220 environ.
Suger a reconstruit des parties de l'ancienne église romane avec la voûte d'ogives afin de supprimer les murs et de faire plus d'espace pour les fenêtres.
De plus, il a installé une rosace circulaire au-dessus du portail de la façade.
La cathédrale de Durham a été la première cathédrale à utiliser une voûte en nervures, construite entre 1093 et 1104.
L'un des constructeurs qui aurait travaillé sur la cathédrale de Sens, Guillaume de Sens, se rendit plus tard en Angleterre et devint l'architecte qui, entre 1175 et 1180, reconstruisit le chœur de la cathédrale de Cantorbéry dans le nouveau style gothique.
Les églises gothiques françaises ont été fortement influencées à la fois par le déambulatoire et les chapelles latérales autour du chœur de Saint-Denis, ainsi que par les tours jumelées et les portes triples de la façade ouest.
Les bâtisseurs de Notre-Dame sont allés plus loin en introduisant les arcs-boutants, lourdes colonnes de soutènement à l'extérieur des murs reliées par des arcs aux murs supérieurs.
Son œuvre fut poursuivie par Guillaume l'Anglais qui remplaça son homonyme français en 1178.
Les Tiercerons - nervures de voûte décoratives - semblent avoir été les premiers à avoir été utilisés dans la voûte de la cathédrale de Lincoln, installée vers 1200.
Le premier édifice du haut gothique fut la cathédrale de Chartres, une importante église de pèlerinage au sud de Paris.
Les murs étaient remplis de vitraux, représentant principalement l'histoire de la Vierge Marie mais aussi, dans un petit coin de chaque fenêtre, illustrant l'artisanat des guildes qui ont fait don de ces vitraux.
En Europe centrale, le style haut gothique apparaît dans le Saint Empire romain germanique, d'abord à Toul (1220–), dont la cathédrale romane est reconstruite dans le style de la cathédrale de Reims ; puis l'église paroissiale Liebfrauenkirche de Trèves (1228–), puis dans tout le Reich, à commencer par l'Elisabethkirche de Marburg (1235–) et la cathédrale de Metz (vers 1235–).
Les fenêtres en lancette ont été remplacées par de multiples lumières séparées par des entrelacs géométriques.
D'autres caractéristiques du haut gothique étaient le développement de rosaces de plus grande taille, utilisant des entrelacs à barreaux, des contreforts volants plus hauts et plus longs, pouvant atteindre les fenêtres les plus hautes, et des murs de sculpture illustrant des histoires bibliques remplissant la façade et les façades de le transept.
Les murs hauts et fins du gothique rayonnant français permis par les arcs-boutants ont permis des étendues de plus en plus ambitieuses de verre et d'entrelacs décorés, renforcés de ferronnerie.
Les maçons ont élaboré une série de motifs d'entrelacs pour les fenêtres - du géométrique de base au réticulé et au curviligne - qui avaient remplacé la fenêtre en lancette.
Les églises présentant des caractéristiques de ce style comprennent l'abbaye de Westminster (1245–), les cathédrales de Lichfield (après 1257–) et d'Exeter (1275–), l'abbaye de Bath (1298–) et le chœur rétro de la cathédrale de Wells (vers 1320–) .
L'utilisation d'ogives était particulièrement courante.
Des exemples de construction flamboyante française incluent la façade ouest de la cathédrale de Rouen , et en particulier les façades de la Sainte-Chapelle de Vincennes (années 1370) et du chœur de l'église abbatiale du Mont-Saint-Michel (1448).
Il est apparu pour la première fois dans les cloîtres et la salle capitulaire (vers 1332) de la vieille cathédrale Saint-Paul de Londres par William de Ramsey.
La perpendiculaire est parfois appelée troisième pointe et a été employée pendant trois siècles; l'escalier voûté en éventail de Christ Church, Oxford, construit vers 1640.
Les rois de France avaient une connaissance directe du nouveau style italien, en raison de la campagne militaire de Charles VIII à Naples et à Milan (1494), et surtout des campagnes de Louis XII et de François Ier (1500-1505) pour restaurer le contrôle français. sur Milan et Gênes.
Le château de Blois (1515-1524) introduit la loggia Renaissance et l'escalier ouvert.
Sous Henri VIII et Elizabeth I, l'Angleterre était largement isolée des développements architecturaux sur le continent.
Shute a publié le premier livre en anglais sur l'architecture classique en 1570.
L'arc en ogive n'est pas originaire de l'architecture gothique ; ils avaient été employés pendant des siècles au Proche-Orient dans l'architecture préislamique et islamique pour les arcs, les arcades et les voûtes nervurées.
Ils étaient aussi parfois utilisés à des fins plus pratiques, comme pour amener les voûtes transversales à la même hauteur que les voûtes diagonales, comme dans la nef et les bas-côtés de la cathédrale de Durham, construite en 1093.
Contrairement à la voûte en berceau semi-circulaire des édifices romains et romans, où le poids appuyait directement vers le bas et nécessitait des murs épais et de petites fenêtres, la voûte en croisée d'ogives gothique était constituée de nervures croisées en diagonale.
La poussée vers l'extérieur contre les murs a été contrée par le poids des contreforts et plus tard des arcs-boutants.
Ils étaient très difficiles à construire, et ne pouvaient traverser qu'un espace limité.
Les rangées alternées de colonnes alternées et de piles recevant le poids des voûtes ont été remplacées par de simples piliers, chacun recevant le même poids.
La première de ces nouvelles voûtes avait une nervure supplémentaire, appelée tierceron, qui descendait le long de la médiane de la voûte.
Ces voûtes ont souvent copié les formes des entrelacs élaborés des styles gothiques tardifs.
Un deuxième type s'appelait une voûte réticulée, qui avait un réseau de nervures décoratives supplémentaires, en triangles et autres formes géométriques, placées entre ou au-dessus des nervures transversales.
Un exemple est le cloître de la cathédrale de Gloucester (vers 1370).
Ils furent utilisés plus tard à Sens, à Notre-Dame de Paris et à Cantorbéry en Angleterre.
À l'époque du haut gothique, une nouvelle forme est introduite, composée d'un noyau central entouré de plusieurs colonnes élancées attachées, ou colonettes, remontant jusqu'aux voûtes.
En Angleterre, les colonnes groupées étaient souvent ornées d'anneaux de pierre, ainsi que de colonnes aux feuilles sculptées.
À la place du chapiteau corinthien, certaines colonnes utilisaient une conception à feuilles rigides.
Dans les structures ultérieures, les contreforts avaient souvent plusieurs arcs, chacun atteignant un niveau différent de la structure.
Les arches avaient un but pratique supplémentaire; ils contenaient des canaux en plomb qui évacuaient l'eau de pluie du toit; il a été expulsé de la bouche de gargouilles de pierre placées en rangées sur les contreforts.
Ils avaient aussi un but pratique; ils servaient souvent de clochers soutenant des beffrois, dont les cloches indiquaient l'heure en annonçant des services religieux, avertissaient d'un incendie ou d'une attaque ennemie et célébraient des occasions spéciales comme des victoires militaires et des couronnements.
Étant donné que la construction de la cathédrale prenait généralement de nombreuses années et était extrêmement coûteuse, au moment où la tour devait être construite, l'enthousiasme du public s'est estompé et les goûts ont changé.
Chartres aurait été encore plus exubérante si le second plan avait été suivi ; il prévoyait sept tours autour du transept et du sanctuaire.
La cathédrale de Laon, gothique primitif et haut, possède une tour-lanterne carrée au-dessus de la croisée du transept ; deux tours sur le front ouest; et deux tours aux extrémités des transepts.
En Normandie, les cathédrales et les grandes églises possédaient souvent plusieurs tours, construites au fil des siècles ; l'Abbaye aux Hommes (commencée en 1066), Caen compte neuf tours et flèches, placées sur la façade, les transepts et le centre.
Une variante de la flèche était la flèche , une flèche élancée en forme de lance, qui était généralement placée sur le transept où elle traversait la nef.
La cathédrale d'Amiens a une flèche.
Elle fut supprimée en 1786 lors d'un programme de modernisation de la cathédrale, mais fut remise sous une nouvelle forme conçue par Eugène Viollet-le-Duc.
Dans le gothique anglais, la tour majeure était souvent placée à la croisée du transept et de la nef, et était beaucoup plus haute que l'autre.
Une tour de passage a été construite à la cathédrale de Canterbury en 1493-1501 par John Wastell, qui avait auparavant travaillé au King's College de Cambridge.
Une double arche inhabituelle a dû être construite au centre du passage à niveau pour donner à la tour le soutien supplémentaire dont elle avait besoin.
La construction a recommencé en 1724 selon la conception de Nicholas Hawksmoor, après que le premier Christopher Wren eut proposé une conception en 1710, mais s'est de nouveau arrêtée en 1727.
La cathédrale de Cologne avait été commencée au XIIIe siècle, selon le plan de la cathédrale d'Amiens, mais seules l'abside et la base d'une tour ont été achevées à l'époque gothique.
La tour de la cathédrale d'Ulm a une histoire similaire, commencée en 1377, arrêtée en 1543 et achevée au XIXe siècle.
La cathédrale de Burgos s'est davantage inspirée de l'Europe du Nord.
L'entrelacs de plaques a été le premier type d'entrelacs à être développé, émergeant dans la phase ultérieure du gothique primitif ou du premier pointu.
L'entrelacs est à la fois pratique et décoratif, car les fenêtres de plus en plus grandes des bâtiments gothiques avaient besoin d'un maximum de soutien contre le vent.
L'entrelacs d'assiettes a atteint le sommet de sa sophistication avec les fenêtres du XIIe siècle de la cathédrale de Chartres et dans la rosace "Dean's Eye" de la cathédrale de Lincoln.
L'entrelacs de barreaux en pierre, élément décoratif important des styles gothiques, a été utilisé pour la première fois à la cathédrale de Reims peu après 1211, dans le chevet construit par Jean d'Orbais.
L'entrelacs de barres est devenu courant après environ 1240, avec une complexité croissante et un poids décroissant.
Rayonnant a également déployé des moulures de deux types différents dans les entrelacs, là où les styles antérieurs utilisaient des moulures d'une seule taille, avec différentes tailles de meneaux.
Les meneaux de style géométrique avaient généralement des chapiteaux avec des barres courbes émergeant d'eux.
Les meneaux étaient en conséquence ramifiés en motifs en forme de Y ornés de cuspides.
Second Pointed (14ème siècle) a vu des entrelacs croisés élaborés avec des ogives, créant une conception réticulaire complexe (en forme de filet) connue sous le nom d'entrelacs réticulés.
Ces formes sont connues sous le nom de poignards, vessies de poisson ou mouchettes.
La verticalité s'est efforcée perpendiculairement et a renoncé aux lignes sinueuses du style curviligne au profit de meneaux droits ininterrompus de haut en bas, coupés par des traverses et des barres horizontales.
Les impostes étaient souvent surmontées de créneaux miniatures.
Il couvrait fréquemment les façades, et les murs intérieurs de la nef et du chœur étaient couverts d'arcades aveugles.
2 Voûtes En berceau ou en arête Nervures Les voûtes nervurées sont apparues à l'époque romane et ont été élaborées à l'époque gothique.
Ils ont une longue nef faisant le corps de l'église, où les paroissiens adoraient ; un bras transversal appelé transept et, au-delà à l'est, le chœur, appelé aussi chœur ou presbytère, habituellement réservé au clergé.
Un passage appelé le déambulatoire faisait le tour du chœur.
Les premières cathédrales, comme Notre-Dame, avaient des voûtes en six parties, avec des colonnes et des piliers alternés, tandis que les cathédrales ultérieures avaient des voûtes en quatre parties plus simples et plus solides, avec des colonnes identiques.
Les transepts étaient généralement courts dans l'architecture gothique française primitive, mais sont devenus plus longs et ont reçu de grandes rosaces à l'époque rayonnante.
En Angleterre, les transepts étaient plus importants et les plans d'étage étaient généralement beaucoup plus complexes que dans les cathédrales françaises, avec l'ajout de Lady Chapels attenantes, d'une salle capitulaire octogonale et d'autres structures (voir les plans de la cathédrale de Salisbury et de York Minster ci-dessous).
Une élévation avait généralement quatre niveaux.
Au-dessus se trouvait une galerie plus étroite, appelée triforium, qui contribuait également à fournir une épaisseur et un support supplémentaires.
Ce système a été utilisé à la cathédrale de Noyon, à la cathédrale de Sens et à d'autres structures anciennes.
La tribune a disparu, ce qui signifiait que les arcades pouvaient être plus hautes.
Un arrangement similaire a été adapté en Angleterre, à la cathédrale de Salisbury, à la cathédrale de Lincoln et à la cathédrale d'Ely.
Cela a été rendu possible par le développement de l'arc-boutant, qui a transféré la poussée du poids du toit aux supports à l'extérieur des murs.
La cathédrale de Beauvais a atteint la limite de ce qui était possible avec la technologie gothique.
Les façades gothiques ont été adaptées du modèle des façades romanes.
La sculpture du tympan central était consacrée au Jugement dernier, celle de gauche à la Vierge Marie et celle de droite aux saints honorés dans cette cathédrale.
Ils ont suivi la doctrine exprimée par saint Thomas d'Aquin selon laquelle la beauté était une "harmonie de contrastes".
En Angleterre, la rosace était souvent remplacée par plusieurs fenêtres à lancette.
Les portails étaient couronnés de hauts pignons arqués, composés d'arcs concentriques remplis de sculptures.
Les tours étaient ornées de leurs propres arcs, souvent couronnés de pinacles.
Alors que les cathédrales françaises mettaient l'accent sur la hauteur de la façade, les cathédrales anglaises, en particulier dans le gothique antérieur, mettaient souvent l'accent sur la largeur.
Il a rompu avec l'accent français sur la hauteur et a éliminé les statuts de colonne et la statuaire dans les entrées voûtées, et a recouvert la façade de mosaïques colorées de scènes bibliques (les mosaïques actuelles sont d'une date ultérieure).
Le sculpteur Andrea Pisano a réalisé les célèbres portes en bronze du baptistère de Florence (1330-1336).
Il y a généralement un déambulatoire simple ou double, ou une allée, autour du chœur et de l'extrémité est, afin que les paroissiens et les pèlerins puissent se promener facilement et librement autour de l'extrémité est.
L'abbé Suger a d'abord utilisé la nouvelle combinaison de voûtes d'ogives et de contreforts pour remplacer les murs épais et les remplacer par des vitraux, ouvrant cette partie de l'église à ce qu'il considérait comme la «lumière divine».
Il existe trois chapelles de ce type à la cathédrale de Chartres, sept à Notre-Dame de Paris, à la cathédrale d'Amiens, à la cathédrale de Prague et à la cathédrale de Cologne, et neuf à la basilique Saint-Antoine de Padoue en Italie.
Un édit du deuxième concile de Nicée en 787 avait déclaré : « La composition des ibames religieux ne doit pas être laissée à l'inspiration des artistes ; elle découle des principes mis en place par l'Église catholique et la tradition religieuse.
Au fur et à mesure que le style évoluait, la sculpture devenait de plus en plus proéminente, reprenant les colonnes du portail, et grimpant progressivement au-dessus des portails, jusqu'à ce que des statues dans des niches recouvrent toute la façade, comme dans la cathédrale de Wells, jusqu'aux transepts, et, comme à la cathédrale d'Amiens, même à l'intérieur de la façade.
Cela a établi un modèle d'iconographie complexe qui a été suivi dans d'autres églises.
Le tympan au-dessus du portail central de la façade ouest de Notre-Dame de Paris illustre de manière vivante le Jugement dernier, avec des figures de pécheurs emmenés en enfer et de bons chrétiens emmenés au paradis.
Les tourments de l'enfer étaient encore plus représentés.
Ils faisaient partie du message visuel pour les fidèles analphabètes, symboles du mal et du danger qui menaçaient ceux qui ne suivaient pas les enseignements de l'église.
Ils ont été remplacés par des personnages de style gothique, conçus par Eugène Viollet-le-Duc lors de la restauration du XIXe siècle.
Les enseignements religieux au Moyen Âge, en particulier les écrits de Pseudo-Denys l'Aréopagite, un mystique du VIe siècle dont le livre, De Coelesti Hierarchia, était populaire parmi les moines en France, enseignaient que toute lumière était divine.
Les fenêtres du côté nord, souvent à l'ombre, avaient des fenêtres représentant l'Ancien Testament.
Les détails ont été peints sur le verre en émail vitrifié, puis cuits dans un four pour fusionner l'émail sur le verre.
La Sainte-Chapelle est devenue le modèle d'autres chapelles à travers l'Europe.
Le verre clair a été trempé dans du verre coloré, puis des portions de verre coloré ont été meulées pour donner exactement la bonne teinte.
L'un des édifices flamboyants les plus célèbres était la Sainte-Chapelle de Vincennes (années 1370), avec des murs de verre du sol au plafond.
Les vitraux étaient extrêmement complexes et coûteux à créer.
La rose était un symbole de la Vierge Marie, et elles étaient notamment utilisées dans les églises qui lui étaient dédiées, dont Notre-Dame de Paris.
Le Palais de la Cité à Paris, près de Notre-Dame de Paris, commencé en 1119, qui fut la résidence principale des rois de France jusqu'en 1417.
Cependant, il fut rapidement rendu obsolète par le développement de l'artillerie et, au XVe siècle, il fut transformé en un confortable palais résidentiel.
Le plus ancien exemple existant en Angleterre est probablement le Mob Quad du Merton College de l'Université d'Oxford, construit entre 1288 et 1378.
Un type similaire de cloître universitaire a été créé au Queen's College d'Oxford dans les années 1140, probablement conçu par Reginald Ely.
Certains collèges, comme le Balliol College d'Oxford, ont emprunté un style militaire aux châteaux gothiques, avec des créneaux et des murs crénelés.
Il écrivit en 1447 qu'il voulait que sa chapelle « procède en grande forme, nette et substantielle, mettant à part le superflu de trop grands travaux curieux de moulage encombrant et occupé ».
Les murs avaient deux niveaux de passerelles à l'intérieur, un parapet crénelé avec merlons et des mâchicoulis en saillie à partir desquels des missiles pouvaient être largués sur les assiégeants.
Les châteaux étaient entourés d'un fossé profond, enjambé par un seul pont-levis.
Un bon exemple survivant est le Château de Dourdan, près de Nemours.
La conversion impliquait des compromis puisque les églises latines sont orientées vers l'Orient et les mosquées vers la Mecque.
Mosquée Lala Mustafa Pasha, à Fabamusta, Chypre du Nord.
Le style gothique a commencé à être décrit comme dépassé, laid et même barbare.
L'Irlande était une île d'architecture gothique aux XVIIe et XVIIIe siècles, avec la construction de la cathédrale de Derry (achevée en 1633), de la cathédrale de Sligo (vers 1730) et de la cathédrale de Down (1790–1818) en sont d'autres exemples.
Les deux tours occidentales de l'abbaye de Westminster ont été construites entre 1722 et 1745 par Nicholas Hawksmoor, ouvrant une nouvelle période de néo-gothique.
Cette période d'attrait plus universel, qui s'étend de 1855 à 1885, est connue en Grande-Bretagne sous le nom de High Victorian Gothic.
À partir de la seconde moitié du XIXe siècle, il est devenu plus courant en Grande-Bretagne d'utiliser le néo-gothique dans la conception de types de bâtiments non ecclésiastiques et non gouvernementaux.
Les architectes paysagistes travaillent sur les structures et les espaces extérieurs dans l'aspect paysager de la conception - grands ou petits, urbains, suburbains et ruraux, et avec des matériaux "durs" (construits) et "mous" (plantés), tout en intégrant la durabilité écologique.
Ils peuvent également examiner les propositions d'autorisation et de supervision des contrats pour les travaux de construction.
La première personne à écrire sur la création d'un paysage fut Joseph Addison en 1712.
Au cours de la fin du XIXe siècle, le terme architecte paysagiste a commencé à être utilisé par les paysagistes professionnels et a été fermement établi après que Frederick Law Olmsted, Jr. et Beatrix Jones (plus tard Farrand) avec d'autres ont fondé l'American Society of Landscape Architects (ASLA) en 1899.
Leurs projets peuvent aller d'études de site à l'évaluation écologique de vastes zones à des fins de planification ou de gestion.
Leur travail s'incarne dans des déclarations écrites de politique et de stratégie, et leur mandat comprend la planification générale des nouveaux développements, les évaluations et évaluations du paysage, et la préparation de plans de gestion ou de politique du paysage.
Ces dernières années, le besoin et l'intérêt des jardins thérapeutiques ont augmenté de plus en plus.
Parmi ceux-ci figuraient Central Park à New York, Prospect Park à Brooklyn, New York et le système de parcs Emerald Necklace de Boston.
Elle a été consultante en design pour plus d'une douzaine d'universités, dont : Princeton à Princeton, New Jersey ; Yale à New Haven, Connecticut ; et l'Arnold Arboretum pour Harvard à Boston, Massachusetts.
Les urbanistes sont qualifiés pour effectuer des tâches indépendantes des architectes paysagistes et, en général, le programme des programmes d'architecture paysagère ne prépare pas les étudiants à devenir urbanistes.
Roberto Burle Marx au Brésil a combiné le style international et les plantes et la culture brésiliennes indigènes pour une nouvelle esthétique.
Il a popularisé un système d'analyse des couches d'un site afin de compiler une compréhension complète des attributs qualitatifs d'un lieu.
Une fois reconnus par l'AILA, les architectes paysagistes utilisent le titre "Architecte paysagiste enregistré" dans les six États et territoires d'Australie.
En Nouvelle-Zélande, les membres de NZILA, lorsqu'ils atteignent leur statut professionnel, peuvent utiliser le titre d'architecte paysagiste enregistré NZILA.
La mission de l'ILASA est de faire progresser la profession d'architecte paysagiste et de maintenir des normes élevées de service professionnel à ses membres, et de représenter la profession d'architecte paysagiste dans toute question pouvant affecter les intérêts des membres de l'institut.
À l'heure actuelle, il existe quinze programmes accrédités au Royaume-Uni.
En 2008, le LI a lancé une grande campagne de recrutement intitulée "Je veux être architecte paysagiste" pour encourager l'étude de l'architecture du paysage.
Plusieurs États exigent également le passage d'un examen d'État.
Au 6ème siècle avant JC, le sable recouvrait déjà les statues du temple principal jusqu'aux genoux.
Un plan pour sauver les temples était basé sur une idée de William MacQuitty de construire un barrage d'eau douce claire autour des temples, avec l'eau à l'intérieur maintenue à la même hauteur que le Nil.
Ils considéraient que l'élévation des temples ignorait l'effet de l'érosion du grès par les vents du désert.
Entre 1964 et 1968, l'ensemble du site a été soigneusement découpé en gros blocs (jusqu'à 30 tonnes, en moyenne 20 tonnes), démantelé, soulevé et remonté dans un nouvel emplacement à 65 mètres plus haut et à 200 mètres en retrait de la rivière, dans l'un des plus grands défis de l'ingénierie archéologique dans l'histoire.
De nombreux visiteurs arrivent également par avion sur un aérodrome spécialement construit pour le complexe du temple, ou par la route depuis Assouan, la ville la plus proche.
Les statues colossales le long du mur de gauche portent la couronne blanche de la Haute-Égypte, tandis que celles du côté opposé portent la double couronne de la Haute et de la Basse-Égypte (pschent).
Le relief le plus célèbre montre le roi sur son char tirant des flèches contre ses ennemis en fuite, qui sont faits prisonniers.
Il y a des représentations de Ramsès et de Néfertari avec les bateaux sacrés d'Amon et de Ra-Horakhty.
Ces dates seraient respectivement l'anniversaire et le jour du couronnement du roi.
En fait, selon les calculs effectués sur la base du lever héliaque de l'étoile Sirius (Sothis) et des inscriptions trouvées par les archéologues, cette date devait être le 22 octobre.
C'était en fait la deuxième fois dans l'histoire de l'Égypte ancienne qu'un temple était dédié à une reine.
Traditionnellement, les statues des reines se tenaient à côté de celles du pharaon, mais n'étaient jamais plus hautes que ses genoux.
Les chapiteaux des piliers portent le visage de la déesse Hathor ; ce type de colonne est connu sous le nom de Hathoric.
Sur les murs sud et nord de cette salle se trouvent deux bas-reliefs gracieux et poétiques du roi et de son épouse présentant des plantes de papyrus à Hathor, qui est représentée comme une vache sur un bateau naviguant dans un fourré de papyrus.
On pense qu'aucun des bâtiments actuels ne date d'avant le XVIIe siècle, mais ils ont probablement été construits avec les mêmes méthodes de construction et les mêmes conceptions que celles utilisées pendant des siècles auparavant.
D'autres kasbahs et ksour étaient situés tout au long de cette route, comme le Tamdaght voisin au nord.
Les bâtiments du village sont regroupés à l'intérieur d'un mur défensif qui comprend des tours d'angle et une porte.
Le village possède également un certain nombre de bâtiments publics ou communautaires tels qu'une mosquée, un caravansérail, une kasbah (fortification en forme de château) et le marabout de Sidi Ali ou Amer.
Il était fait de terre et de boue comprimées, généralement mélangées à d'autres matériaux pour faciliter l'adhérence.
Le barrage d'Assouan, ou plus précisément depuis les années 1960, le haut barrage d'Assouan, est le plus grand barrage en remblai du monde, qui a été construit sur le Nil à Assouan, en Égypte, entre 1960 et 1970.
Comme la mise en œuvre précédente, le haut barrage a eu un effet significatif sur l'économie et la culture de l'Égypte.
Cependant, ces inondations naturelles variaient, car les années de hautes eaux pouvaient détruire toute la récolte, tandis que les années de basses eaux pouvaient créer une sécheresse généralisée et par conséquent une famine.
Au lieu de cela, le plan de la vallée du Nil de l'hydrologue britannique Harold Edwin Hurst a été préféré, qui proposait de stocker l'eau au Soudan et en Éthiopie, où l'évaporation est beaucoup plus faible.
Au départ, les États-Unis et l'URSS étaient intéressés à aider au développement du barrage.
À cette époque, les États-Unis craignaient que le communisme ne se propage au Moyen-Orient et considéraient Nasser comme un leader naturel d'une Ligue arabe procapitaliste anticommuniste.
Après que l'ONU a critiqué un raid israélien contre les forces égyptiennes à Gaza en 1955, Nasser s'est rendu compte qu'il ne pouvait pas se présenter comme le chef du nationalisme panarabe s'il ne pouvait pas défendre militairement son pays contre Israël.
Nasser n'a pas accepté ces conditions et a consulté l'URSS pour obtenir son soutien.
Dulles était plus irrité par la reconnaissance diplomatique de la Chine par Nasser, qui était en conflit direct avec la politique d'endiguement du communisme de Dulles.
Il a également été irrité par la neutralité de Nasser et tente de jouer les deux côtés de la guerre froide.
L'énorme barrage de roche et d'argile a été conçu par l'Institut hydroélectrique soviétique avec des ingénieurs égyptiens.
À l'inverse, le barrage a inondé une vaste zone, provoquant le déplacement de plus de 100 000 personnes.
L'évaluation des coûts et des avantages du barrage reste controversée des décennies après son achèvement.
Sans tenir compte des effets environnementaux et sociaux négatifs du barrage, ses coûts sont donc estimés avoir été récupérés en seulement deux ans.
Un autre observateur n'était pas d'accord et il a recommandé que le barrage soit démoli.
Le barrage a atténué les effets des inondations, comme celles de 1964, 1973 et 1988.
Une nouvelle industrie de la pêche s'est créée autour du lac Nasser, bien qu'elle soit en difficulté en raison de son éloignement de tout marché important.
Environ un demi-million de familles se sont installées sur ces nouvelles terres.
Sur d'autres terres précédemment irriguées, les rendements ont augmenté parce que l'eau pouvait être rendue disponible pendant les périodes critiques d'étiage.
Au Soudan, 50 000 à 70 000 Nubiens soudanais ont été déplacés de la vieille ville de Wadi Halfa et de ses villages environnants.
Le gouvernement a développé un projet d'irrigation, appelé le plan de développement agricole de New Halfa pour cultiver du coton, des céréales, de la canne à sucre et d'autres cultures.
Des logements et des installations ont été construits pour 47 unités villageoises dont les relations les unes avec les autres se rapprochaient de celles de l'ancienne Nubie.
La valeur nutritive ajoutée à la terre par les sédiments n'était que de 6 000 tonnes de potasse, 7 000 tonnes de pentoxyde de phosphore et 17 000 tonnes d'azote.
La salinité du sol a également augmenté parce que la distance entre la surface et la nappe phréatique était suffisamment petite (1 à 2 m selon les conditions du sol et la température) pour permettre à l'eau d'être entraînée par évaporation afin que les concentrations relativement faibles de sel dans les eaux souterraines s'accumulent. à la surface du sol au fil des ans.
Dans les années 1950, seule une petite proportion de la Haute-Égypte n'avait pas été convertie de l'irrigation par bassin (faible transmission) à l'irrigation pérenne (transmission élevée).
S. haematobium a complètement disparu depuis.
Cela signifie que le volume de stockage mort serait rempli après 300 à 500 ans si les sédiments s'accumulaient au même rythme sur toute la surface du lac.
Après la construction du barrage, les herbes aquatiques ont poussé beaucoup plus vite dans l'eau plus claire, aidées par les résidus d'engrais.
La pêche méditerranéenne et la pêche dans les lacs d'eau saumâtre ont diminué après l'achèvement du barrage parce que les nutriments qui coulaient du Nil vers la Méditerranée étaient piégés derrière le barrage.
Une préoccupation avant la construction du haut barrage était la baisse potentielle du niveau du lit de la rivière en aval du barrage en raison de l'érosion causée par l'écoulement d'eau sans sédiments.
L'industrie de la construction en briques rouges, qui se composait de centaines d'usines qui utilisaient les dépôts de sédiments du Nil le long du fleuve, a également été affectée négativement.
En raison de la faible turbidité de l'eau, la lumière du soleil pénètre plus profondément dans l'eau du Nil.
Les travaux de construction ont commencé en 1995 et, après avoir dépensé quelque 220 millions de dollars américains, le complexe a été officiellement inauguré le 16 octobre 2002.
La recréation de l'ancienne bibliothèque n'a pas seulement été adoptée par d'autres individus et agences, elle a également obtenu le soutien des politiciens égyptiens.
L'implication de l'UNESCO à partir de 1986 a créé une excellente opportunité pour que le projet soit véritablement international.
Cette équipe architecturale était composée de dix membres représentant six pays.
Les premiers engagements ont été pris pour financer le projet lors d'une conférence tenue en 1990 à Assouan : 65 millions de dollars américains, principalement des États MENA.
En 2010, la bibliothèque a reçu un don de 500 000 livres de la Bibliothèque nationale de France, Bibliothèque nationale de France (BnF).
La salle de lecture principale se dresse sous un toit en panneaux de verre de 32 mètres de haut, incliné vers la mer comme un cadran solaire et mesurant environ 160 m de diamètre.
Contenant environ 1 316 artefacts, la collection du musée des antiquités donne un aperçu de l'histoire égyptienne de l'ère pharaonique à la conquête d'Alexandre le Grand jusqu'aux civilisations romaines avant l'avènement de l'islam en Égypte.
Microfilm : cette section comprend des microfilms d'environ 30 000 manuscrits rares et 50 000 documents, ainsi qu'une collection de la British Library d'environ 14 000 manuscrits arabes, persans et turcs, considérée comme la plus grande collection d'Europe.
Cependant, en 2010, la bibliothèque a reçu 500 000 livres supplémentaires de la Bibliothèque nationale de France.)
La Grande Mosquée de Djenné est un grand édifice en banco ou en pisé qui est considéré par de nombreux architectes comme l'une des plus grandes réalisations du style architectural soudano-sahélien.
Le premier document mentionnant la mosquée est le Tarikh al-Sudan d'Abd al-Sadi qui donne l'histoire ancienne, vraisemblablement de la tradition orale telle qu'elle existait au milieu du XVIIe siècle.
Son successeur immédiat a construit les tours de la mosquée tandis que le sultan suivant a construit le mur d'enceinte.
Cela aurait été le bâtiment que Caillié a vu.
La nouvelle mosquée était un grand bâtiment bas sans tours ni ornements.
La reconstruction a été achevée en 1907 en utilisant le travail forcé sous la direction d'Ismaila Traoré, chef de la guilde des maçons de Djenné.
Il y a eu un débat sur la mesure dans laquelle la conception de la mosquée reconstruite a été soumise à l'influence française.
Il pensait que les cônes faisaient ressembler le bâtiment à un temple baroque dédié au dieu des suppositoires.
Il dit également que la population locale était si mécontente du nouveau bâtiment qu'elle a refusé de le nettoyer, ne le faisant que lorsqu'elle était menacée de prison.
La plus grande tombe au sud contient les restes d'Almany Ismaïla, un important imam du XVIIIe siècle.
Dans certains cas, les surfaces d'origine d'une mosquée ont même été carrelées, détruisant son aspect historique et, dans certains cas, compromettant l'intégrité structurelle du bâtiment.
En 1996, Vogue bamazine a organisé un shooting de mode à l'intérieur de la mosquée.
On y accède par six séries d'escaliers, chacun orné de pinacles.
Le mur de prière ou qibla de la Grande Mosquée fait face à l'est vers La Mecque et surplombe le marché de la ville.
Les flèches ou pinacles en forme de cône au sommet de chaque minaret sont surmontées d'œufs d'autruche.
Les petites fenêtres disposées de manière irrégulière sur les murs nord et sud laissent peu de lumière naturelle pénétrer à l'intérieur de la salle.
L'imam dirige les prières depuis le mihrab dans la plus grande tour centrale.
A droite du mihrab dans la tour centrale se trouve une deuxième niche, la chaire ou minbar, d'où l'imam prêche son sermon du vendredi.
Les murs des galeries donnant sur la cour sont ponctués d'ouvertures en arc.
Plutôt qu'une seule niche centrale, la tour du mihrab avait à l'origine une paire de grands évidements faisant écho à la forme des arcs d'entrée dans le mur nord.
Il faut plusieurs jours pour durcir mais doit être remué périodiquement, une tâche incombant généralement aux jeunes garçons qui jouent dans le mélange, remuant ainsi le contenu.
Une course est organisée au début du festival pour voir qui sera le premier à livrer le plâtre à la mosquée.
En 1930, une réplique inexacte de la mosquée de Djenné a été construite dans la ville de Fréjus dans le sud de la France.
La mosquée d'origine présidait l'un des centres d'apprentissage islamique les plus importants d'Afrique au Moyen Âge, avec des milliers d'étudiants venant étudier le Coran dans les madrassas de Djenné.
Le 20 janvier 2006, la vue d'une équipe d'hommes piratant le toit de la mosquée a déclenché une émeute dans la ville.
Dans la mosquée, la foule a arraché les ventilateurs de ventilation qui avaient été présentés par l'ambassade des États-Unis au moment de la guerre en Irak, puis s'est déchaînée à travers la ville.
Le Grand Sphinx de Gizeh, communément appelé le Sphinx de Gizeh ou simplement le Sphinx, est une statue en calcaire d'un sphinx couché, une créature mythique.
De plus, l'angle et l'emplacement du mur sud de l'enceinte suggèrent que la chaussée reliant la pyramide de Khafré et le temple de la vallée existait déjà avant la planification du Sphinx.
Lorsque la stèle a été de nouveau fouillée en 1925, les lignes de texte faisant référence à Khaf se sont écaillées et ont été détruites.
Le culte du Sphinx s'est poursuivi jusqu'à l'époque médiévale.
Alexandrie, Rosette, Damiette, Le Caire et les pyramides de Gizeh sont décrites à plusieurs reprises, mais pas nécessairement de manière exhaustive.
Sept ans après sa visite à Gizeh, André Thévet (Cosmographie de Levant, 1556) décrit le Sphinx comme "la tête d'un colosse, faite par Isis, fille d'Inachus, alors si aimée de Jupiter".
Le Sphinx de Johannes Helferich (1579) est une femme au visage pincé et aux seins ronds avec une perruque aux cheveux raides; le seul avantage sur Thévet est que les cheveux suggèrent les pans évasés de la coiffe.
Bien que certains tracts sur la stèle soient probablement exacts, ce passage est contredit par des preuves archéologiques, donc considéré comme du révisionnisme historique de la période tardive, un faux délibéré, créé par les prêtres locaux pour tenter d'imprégner le temple contemporain d'Isis d'une histoire ancienne. jamais eu.
Des découvertes récentes, cependant, montrent fortement qu'il n'a vraiment pas été construit avant le règne de Khephren, sous la IVe dynastie."
Maspero croyait que le Sphinx était "le monument le plus ancien d'Egypte".
Une partie de sa coiffe était tombée en 1926 à cause de l'érosion, qui avait également profondément entaillé son cou.
La couche dans laquelle la tête a été sculptée est beaucoup plus dure.
D'autres contes l'attribuent à l'œuvre des Mamelouks.
Selon al-Maqrīzī, de nombreuses personnes vivant dans la région pensaient que l'augmentation du sable recouvrant le plateau de Gizeh était une rétribution pour l'acte de dégradation d'al-Dahr.
Al-Minufi a déclaré que la croisade d'Alexandrie en 1365 était une punition divine pour un cheikh soufi du khanqah de Sa'id se cassant le nez.
L'idée est considérée comme pseudoarchéologique par le milieu universitaire, car aucune preuve textuelle ou archéologique ne soutient que cela soit la raison de l'orientation du Sphinx.
Il existe une longue histoire de spéculations sur les chambres cachées sous le Sphinx, par des personnalités ésotériques telles que H. Spencer Lewis.
On pense qu'il s'agit du deuxième site historique le plus visité d'Égypte; seul le complexe pyramidal de Gizeh près du Caire reçoit plus de visites.
Les trois autres parties, l'enceinte de Mout, l'enceinte de Montou et le temple démantelé d'Amenhotep IV, sont fermées au public.
Le temple d'origine a été détruit et partiellement restauré par Hatchepsout, bien qu'un autre pharaon ait construit autour de lui afin de changer l'orientation ou l'orientation de la zone sacrée.
La construction de temples a commencé au Moyen Empire et s'est poursuivie à l'époque ptolémaïque.
Les divinités représentées vont de certaines des plus anciennes vénérées à celles vénérées beaucoup plus tard dans l'histoire de la culture égyptienne antique.
Ces architraves peuvent avoir été soulevées à ces hauteurs à l'aide de leviers.
Si la pierre avait été utilisée pour les rampes, elles auraient pu utiliser beaucoup moins de matériaux.
La sculpture finale a été exécutée après la mise en place des tambours afin qu'elle ne soit pas humiliée lors de sa mise en place.
La ville de Thèbes ne semble pas avoir eu une grande importance avant la onzième dynastie et la construction de temples précédents y aurait été relativement petite, les sanctuaires étant dédiés aux premières divinités de Thèbes, la déesse de la Terre Mut et Montu.
Amon (parfois appelé Amen) fut longtemps la divinité tutélaire locale de Thèbes.
D'importants travaux de construction dans l'enceinte d'Amon-Rê ont eu lieu au cours de la XVIIIe dynastie, lorsque Thèbes est devenue la capitale de l'Égypte ancienne unifiée.
Un autre de ses projets sur le site, la Chapelle Rouge ou Chapelle Rouge de Karnak, était conçu comme un sanctuaire de barque et se trouvait peut-être à l'origine entre ses deux obélisques.
Connu sous le nom d'obélisque inachevé, il témoigne de la façon dont les obélisques ont été extraits.
Le dernier changement majeur apporté à l'aménagement de l'enceinte d'Amon-Rê a été l'ajout du premier pylône et des murs d'enceinte massifs qui entourent l'ensemble de l'enceinte, tous deux construits par Nectanebo I de la trentième dynastie.
Le complexe du temple de Karnak est décrit pour la première fois par un Vénitien inconnu en 1589, bien que son récit ne donne aucun nom au complexe.
Les écrits de Protais sur leur voyage ont été publiés par Melchisédech Thévenot (Relations de divers voyages curieux, éditions 1670-1696) et Johann Michael Vansleb (L'état actuel de l'Égypte, 1678).
Suite aux travaux d'excavation et de restauration par l'équipe de l'Université Johns Hopkins, dirigée par Betsy Bryan (voir ci-dessous), l'enceinte de Mut a été ouverte au public.
En 2006, Betsy Bryan a présenté ses découvertes d'un festival qui comprenait une consommation excessive intentionnelle apparente d'alcool.
Ces découvertes ont été faites dans le temple de Mout parce que lorsque Thèbes a pris une plus grande importance, Mout a absorbé les déesses guerrières, Sekhmet et Bast, comme certains de ses aspects.
Dans un mythe ultérieur développé autour du festival annuel de Sekhmet ivre, Ra, alors le dieu solaire de la Haute-Égypte, l'a créée à partir d'un œil ardent acquis de sa mère, pour détruire les mortels qui ont conspiré contre lui (Basse Égypte).
Le temple de Louxor est un grand complexe de temples de l'Égypte ancienne situé sur la rive est du Nil dans la ville aujourd'hui connue sous le nom de Louxor (ancienne Thèbes) et a été construit vers 1400 avant notre ère.
Quatre des principaux temples mortuaires visités par les premiers voyageurs comprennent le temple de Seti I à Gurnah, le temple d'Hatchepsout à Deir el Bahri, le temple de Ramsès II (c'est-à-dire Ramesseum) et le temple de Ramsès III à Médinet Habou.
À l'arrière du temple se trouvent des chapelles construites par Amenhotep III de la 18e dynastie et Alexandre.
Ce grès est appelé grès nubien.
Alexander Badawy, "L'illusionnisme dans l'architecture égyptienne", Studies in the Ancient Oriental Civilization, 35 (1969): 23.
Le long de l'avenue, les stations ont été installées pour des cérémonies telles que la fête d'Opet qui avait une signification pour le temple.
Lalibela est une ville du district de Lasta dans la zone nord de Wollo dans la région d'Amhara, en Éthiopie.
Pour les chrétiens, Lalibela est l'une des villes les plus saintes d'Éthiopie, juste après Axum, et un centre de pèlerinage.
On dit que les noms de plusieurs endroits de la ville moderne et la disposition générale des églises taillées dans la roche elles-mêmes imitent les noms et les modèles observés par Lalibela pendant le temps qu'il a passé dans sa jeunesse à Jérusalem et en Terre Sainte.
La foi chrétienne inspire de nombreuses caractéristiques avec des noms bibliques - même la rivière de Lalibela est connue sous le nom de Jourdain.
Le prêtre portugais Francisco Álvares (1465-1540) accompagna l'ambassadeur du Portugal lors de sa visite à Dawit II dans les années 1520.
Le prochain visiteur européen signalé à Lalibela était Miguel de Castanhoso, qui a servi comme soldat sous Cristóvão da Gama et a quitté l'Éthiopie en 1544.
Ses piliers ont également été taillés dans la montagne."),
Il y a une certaine controverse quant au moment où certaines des églises ont été construites.
Son rapport décrit deux types de logements vernaculaires trouvés dans la région.
Le monastère Sainte-Catherine, officiellement monastère sacré du mont Sinaï foulé par Dieu, est un monastère orthodoxe oriental situé sur la péninsule du Sinaï, à l'embouchure d'une gorge au pied du mont Sinaï, près de la ville de Sainte-Catherine, en Égypte.
Le monastère de Sainte Catherine est situé à l'ombre d'un groupe de trois montagnes ; Ras Sufsafeh (peut-être "Mont Horeb" à environ 1 km à l'ouest), Jebel Arrenziyeb et Jebel Musa, le "Mont Biblique du Sinaï" (pic à environ 2 km au sud).
Catherine elle-même a ordonné le début de l'exécution.
Le monastère a été construit sur ordre de l'empereur Justinien Ier (régné de 527 à 565), renfermant la chapelle du buisson ardent (également connue sous le nom de "chapelle Sainte-Hélène") dont la construction a été ordonnée par l'impératrice consort Helena, mère de Constantin le Grand, à le site où Moïse est censé avoir vu le buisson ardent.
Le site est sacré pour le christianisme, l'islam et le judaïsme.
Au VIIe siècle, les anachorètes chrétiens isolés du Sinaï sont éliminés : seul subsiste le monastère fortifié.
Dès la première croisade, la présence des croisés dans le Sinaï jusqu'en 1270 stimula l'intérêt des chrétiens européens et augmenta le nombre de pèlerins intrépides qui visitèrent le monastère.
Le statut administratif exact de l'Église au sein de l'Église orthodoxe orientale est ambigu : par certains, y compris l'Église elle-même, elle est considérée comme autocéphale, par d'autres comme une Église autonome sous la juridiction de l'Église grecque orthodoxe de Jérusalem.
Mais en 2003, des érudits russes ont découvert l'acte de donation du manuscrit signé par le concile du Caire Metochion et l'archevêque Callistratus le 13 novembre 1869.
Les palimpsestes sont remarquables pour avoir été réutilisés une ou plusieurs fois au cours des siècles.
Chaque page a pris environ huit minutes pour être entièrement numérisée.
La grande collection d'icônes commence par quelques-unes datant des 5ème (peut-être) et 6ème siècles, qui sont des survivances uniques ; le monastère n'ayant pas été touché par l'iconoclasme byzantin et n'ayant jamais été saccagé.
La conservation de ses structures architecturales, de ses peintures et de ses livres constitue une grande partie de l'objectif de la Fondation.
Sa taille reflète la relative prospérité de l'époque.
Il a été construit sur le site d'un ancien temple plus petit également dédié à Horus, bien que la structure précédente ait été orientée est-ouest plutôt que nord-sud comme dans le site actuel.
Le temple d'Edfou est tombé en désuétude en tant que monument religieux après la persécution des païens par Théodose Ier et l'édit interdisant le culte non chrétien dans l'Empire romain en 391.
Au fil des siècles, le temple a été enterré à une profondeur de 12 mètres (39 pieds) sous le sable du désert à la dérive et les couches de limon de la rivière déposées par le Nil.
En 1860, Auguste Mariette, un égyptologue français, a commencé les travaux de libération du temple d'Edfou des sables.
Le Grand Zimbabwe est une ville médiévale située dans les collines du sud-est du Zimbabwe, près du lac Mutirikwe et de la ville de Masvingo.
On pense que le Grand Zimbabwe a servi de palais royal au monarque local.
Ils ont été construits sans mortier (pierre sèche).
Les premières visites confirmées par les Européens ont eu lieu à la fin du 19ème siècle, avec des enquêtes sur le site à partir de 1871.
La région du Grand Zimbabwe a été colonisée au 4ème siècle après JC.
David Beach pense que la ville et son état, le Royaume du Zimbabwe, ont prospéré de 1200 à 1500, bien qu'une date un peu plus ancienne pour sa disparition soit impliquée par une description transmise au début des années 1500 à João de Barros.
Ils sont connus sous le nom de Hill Complex, Valley Complex et Great Enclosure.
Le complexe de la vallée est divisé en ruines de la haute et de la basse vallée, avec différentes périodes d'occupation.
Le centre du pouvoir s'est déplacé du complexe de collines au 12ème siècle, à la grande enceinte, à la haute vallée et enfin à la basse vallée au début du 16ème siècle.
D'autres artefacts comprennent des figurines en stéatite (dont l'une se trouve au British Museum), de la poterie, des gongs en fer, de l'ivoire, des fils de fer et de cuivre minutieusement travaillés, des houes en fer, des fers de lance en bronze, des lingots et des creusets en cuivre, ainsi que des perles, des bracelets, des pendentifs et des gaines en or. .
Ce commerce international s'ajoutait au commerce agricole local, dans lequel le bétail était particulièrement important.
Les commerçants portugais ont entendu parler des vestiges de la ville médiévale au début du XVIe siècle, et des archives d'interviews et de notes faites par certains d'entre eux ont survécu, reliant le Grand Zimbabwe à la production d'or et au commerce à longue distance.
Il a affirmé que la figurine semblait plutôt dater de l'ère ptolémaïque suivante (vers 323-30 avant JC), lorsque les marchands grecs basés à Alexandrie exportaient des antiquités et des pseudo-antiquités égyptiennes vers l'Afrique australe.
Bent n'avait aucune formation archéologique formelle, mais avait beaucoup voyagé en Arabie, en Grèce et en Asie Mineure.
Ils ont une tradition d'ancienne descendance juive ou sud-arabe à travers leur lignée masculine.
La revendication Lemba a également été signalée par un William Bolts (en 1777, aux autorités autrichiennes des Habsbourg) et par un AA Anderson (écrivant sur ses voyages au nord de la rivière Limpopo au 19ème siècle).
Elle avait d'abord creusé trois fosses d'essai dans ce qui avait été des tas d'ordures sur les terrasses supérieures du complexe de la colline, produisant un mélange de poterie et de ferronnerie banale.
Caton Thompson a immédiatement annoncé sa théorie de l'origine bantoue lors d'une réunion de l'Association britannique à Johannesburg.
La preuve au radiocarbone est une suite de 28 mesures, pour lesquelles toutes sauf les quatre premières, datant des premiers jours de l'utilisation de cette méthode et maintenant considérées comme inexactes, soutiennent la chronologie des XIIe au XVe siècles.
L'élimination de l'or et des artefacts dans les fouilles amateurs par les premiers antiquaires coloniaux a provoqué une dabame généralisée, notamment les fouilles de Richard Nicklin Hall.
Preben Kaarsholm écrit que les groupes nationalistes coloniaux et noirs ont invoqué le passé du Grand Zimbabwe pour soutenir leur vision du présent du pays, à travers les médias de l'histoire populaire et de la fiction.
Pikirayi et Kaarsholm suggèrent que cette présentation du Grand Zimbabwe visait en partie à encourager la colonisation et l'investissement dans la région.
En 1980, le nouveau pays indépendant internationalement reconnu a été renommé pour le site, et ses célèbres sculptures d'oiseaux en stéatite ont été conservées du drapeau rhodésien et des armoiries en tant que symbole national et représentées dans le nouveau drapeau zimbabwéen.
Un exemple du premier est le livret de Ken Mufuka, bien que le travail ait été fortement critiqué.
Il a été créé pour préserver la riche histoire de ce pays qui faisait face à un avenir sombre en raison de la mondialisation.
Le site présente une multitude de styles architecturaux, rappelant les styles observés dans le centre du Mexique et les styles Puuc et Chenes des basses terres mayas du nord.
La ville avait peut-être la population la plus diversifiée du monde maya, un facteur qui aurait pu contribuer à la variété des styles architecturaux du site.
Une traduction possible pour Itza est "enchanteur (ou enchantement) de l'eau", de son (itz), "sorcier", et ha, "eau".
Cette forme préserve la distinction phonémique entre chʼ et ch, puisque le mot de base chʼeʼen (qui, cependant, n'est pas accentué en maya) commence par une consonne affriquée éjective postvéolaire.
Parmi ces cenotes, le "Cenote Sagrado" ou Cenote Sacré (également connu sous le nom de Puits Sacré ou Puits du Sacrifice), est le plus célèbre.
Au lieu de cela, l'organisation politique de la ville aurait pu être structurée par un système «multepal», qui se caractérise par un gouvernement par le biais d'un conseil composé de membres de lignées dirigeantes d'élite.
C'est cependant vers la fin du Classique tardif et au début du Classique terminal que le site est devenu une capitale régionale majeure, centralisant et dominant la vie politique, socioculturelle, économique et idéologique dans les basses terres mayas du nord.
Hunac Ceel aurait prophétisé sa propre montée au pouvoir.
Bien qu'il existe des preuves archéologiques indiquant que Chichén Itzá a été pillée et saccagée à un moment donné, il semble y avoir de plus grandes preuves que cela n'aurait pas pu être par Mayapan, du moins pas lorsque Chichén Itzá était un centre urbain actif.
Après la fin des activités d'élite de Chichén Itzá, la ville n'a peut-être pas été abandonnée.
Montejo retourna au Yucatán en 1531 avec des renforts et établit sa base principale à Campeche sur la côte ouest.
Montejo le Jeune est finalement arrivé à Chichen Itza, qu'il a rebaptisé Ciudad Real.
Les mois passèrent, mais aucun renfort n'arriva.
En 1535, tous les Espagnols avaient été chassés de la péninsule du Yucatán.
En 1860, Désiré Charnay arpente Chichén Itzá et prend de nombreuses photographies qu'il publie dans Cités et ruines américaines (1863).
Augustus Le Plongeon l'appelait "Chaacmol" (rebaptisé plus tard "Chac Mool", qui a été le terme pour décrire tous les types de cette statuaire trouvée en Mésoamérique).
En 1894, le consul des États-Unis au Yucatán, Edward Herbert Thompson, acheta l'Hacienda Chichén, qui comprenait les ruines de Chichen Itza.
Thompson est surtout connu pour avoir dragué le Cenote Sagrado (Cénote sacré) de 1904 à 1910, où il a récupéré des artefacts d'or, de cuivre et de jade sculpté, ainsi que les tout premiers exemples de ce que l'on croyait être des tissus mayas précolombiens et armes en bois.
La révolution mexicaine et l'instabilité gouvernementale qui a suivi, ainsi que la Première Guerre mondiale, ont retardé le projet d'une décennie.
Au même moment, le gouvernement mexicain a fouillé et restauré El Castillo (temple de Kukulcán) et le grand terrain de balle.
Thompson, qui était aux États-Unis à l'époque, n'est jamais retourné au Yucatán.
En 1944, la Cour suprême mexicaine a statué que Thompson n'avait enfreint aucune loi et a rendu Chichen Itza à ses héritiers.
Le premier était parrainé par le National Geographic et le second par des intérêts privés.
La ville a été construite sur un terrain accidenté, qui a été nivelé artificiellement afin de construire les principaux groupes architecturaux, le plus grand effort étant consacré au nivellement des zones de la pyramide de Castillo et des groupes Las Monjas, Osario et Main Southwest.
Beaucoup de ces bâtiments en pierre étaient à l'origine peints en rouge, vert, bleu et violet.
Tout comme les cathédrales gothiques en Europe, les couleurs donnaient un plus grand sens de complétude et contribuaient grandement à l'impact symbolique des bâtiments.
Le bâtiment de style Puuc présente les façades supérieures habituelles décorées de mosaïques caractéristiques du style mais diffère de l'architecture du cœur de Puuc par leurs murs en maçonnerie de blocs, par opposition aux placages fins de la région de Puuc proprement dite.
A la base des balustrades de l'escalier nord-est sont sculptées des têtes de serpent.
Après plusieurs faux départs, ils découvrent un escalier sous le côté nord de la pyramide.
Le gouvernement mexicain a creusé un tunnel à partir de la base de l'escalier nord, jusqu'à l'escalier de la pyramide antérieure menant au temple caché, et l'a ouvert aux touristes.
Dans un panneau, l'un des joueurs a été décapité ; la plaie émet des filets de sang sous forme de serpents qui se tortillent.
À l'extrémité sud se trouve un autre temple, beaucoup plus grand, mais en ruines.
A l'intérieur se trouve une grande fresque, très détruite, qui représente une scène de bataille.
Il est construit dans une combinaison de styles maya et toltèque, avec un escalier montant chacun de ses quatre côtés.
Dans son intérieur, les archéologues ont découvert une collection de grands cônes taillés dans la pierre, dont le but est inconnu.
Son nom vient d'une série d'autels au sommet de la structure qui sont soutenus par de petites figures sculptées d'hommes aux bras levés, appelés "atlantes".
Ce complexe est analogue au temple B de la capitale toltèque de Tula et indique une certaine forme de contact culturel entre les deux régions.
Ce temple renferme ou ensevelit une ancienne structure appelée le Temple du Chac Mool.
Au sud du Groupe des Mille Colonnes se trouve un groupe de trois bâtiments plus petits et interconnectés.
Une section de la façade supérieure avec un motif de x et de o est affichée devant la structure.
Le temple de Xtoloc est un temple récemment restauré à l'extérieur de la plate-forme d'Osario.
Entre le temple Xtoloc et l'Osario se trouvent plusieurs structures alignées : la plate-forme de Vénus, dont la conception est similaire à la structure du même nom à côté de Kukulkan (El Castillo), la plate-forme des tombes et une petite structure ronde qui est sans nom.
La Casa Colorada (espagnol pour "Red House") est l'un des bâtiments les mieux conservés de Chichen Itza.
En 2009, l'INAH a restauré un petit terrain de balle qui jouxtait le mur du fond de la Casa Colorada.
Le nom de ce bâtiment a longtemps été utilisé par les Mayas locaux, et certains auteurs mentionnent qu'il a été nommé d'après un cerf peignant sur du stuc qui n'existe plus.
Les Espagnols appelaient ce complexe Las Monjas ("Les Soeurs" ou "Le Couvent"), mais c'était un palais gouvernemental.
Ces textes mentionnent fréquemment un souverain du nom de Kʼakʼupakal.
Il tire son nom de l'escalier en colimaçon en pierre à l'intérieur.
La longue façade orientée à l'ouest est percée de sept portes.
L'extrémité sud du bâtiment a une entrée.
À l'intérieur de l'une des chambres, près du plafond, se trouve une empreinte de main peinte.
L'emplacement de la grotte est bien connu à l'époque moderne.
E. Wyllys Andrews IV a également exploré la grotte dans les années 1930.
Le 15 septembre 1959, José Humberto Gómez, un guide local, découvre un faux mur dans la grotte.
Avant même la publication du livre, Benjamin Norman et le baron Emanuel von Friedrichsthal se sont rendus à Chichen après avoir rencontré Stephens, et ont tous deux publié les résultats de leurs découvertes.
En 1923, le gouverneur Carrillo Puerto a officiellement ouvert l'autoroute vers Chichen Itza.
En 1930, l'hôtel Mayaland a ouvert ses portes, juste au nord de l'Hacienda Chichén, qui avait été reprise par la Carnegie Institution.
En 1972, le Mexique a promulgué la Ley Federal Sobre Monumentos y Zonas Arqueológicas, Artísticas e Históricas (loi fédérale sur les monuments et les sites archéologiques, artistiques et historiques) qui place tous les monuments précolombiens du pays, y compris ceux de Chichen Itza, sous la propriété fédérale.
Les guides touristiques démontreront également un effet acoustique unique à Chichen Itza : un coup de main devant l'escalier que la pyramide El Castillo produira par un écho qui ressemble au gazouillis d'un oiseau, similaire à celui du quetzal étudié par Declercq .
L'INAH, qui gère le site, a fermé un certain nombre de monuments à l'accès public.
À l'origine un projet du promoteur immobilier et ancien sénateur de l'État de New York William H. Reynolds, le bâtiment a été construit par Walter Chrysler, le chef de la Chrysler Corporation.
Une annexe a été achevée en 1952 et le bâtiment a été vendu par la famille Chrysler l'année suivante, avec de nombreux propriétaires ultérieurs.
L'époque a été caractérisée par de profonds changements sociaux et technologiques.
L'année suivante, Chrysler a été nommée "Personne de l'année" par Time bamazine.
Après la fin de la Première Guerre mondiale, les architectes européens et américains en sont venus à considérer le design simplifié comme l'incarnation de l'ère moderne et les gratte-ciel Art Déco comme symbolisant le progrès, l'innovation et la modernité.
Avant son implication dans la planification du bâtiment, Reynolds était surtout connu pour avoir développé le parc d'attractions Dreamland de Coney Island.
En 1927, après plusieurs années de retard, Reynolds engagea l'architecte William Van Alen pour y concevoir un immeuble de quarante étages.
Van Alen et Severance se complétaient, Van Alen étant un architecte original et ibaminatif et Severance étant un homme d'affaires avisé qui gérait les finances de l'entreprise.
La proposition a de nouveau été modifiée deux semaines plus tard, avec des plans officiels pour un immeuble de 63 étages.
Le bâtiment Chanin adjacent de 56 étages était également en construction.
Ces plans furent approuvés en juin 1928.
Il a plutôt conçu une conception alternative pour le bâtiment Reynolds, qui a été publiée en août 1928.
Un contrat a été attribué le 28 octobre et la démolition a été achevée le 9 novembre.
De la fin de 1928 au début de 1929, les modifications apportées à la conception du dôme se sont poursuivies.
Plus bas, la conception a été affectée par l'intention de Walter Chrysler de faire du bâtiment le siège social de Chrysler Corporation, et en tant que tel, divers détails architecturaux ont été calqués sur les produits automobiles Chrysler, tels que les ornements de capot de la Plymouth (voir ).
La construction du bâtiment proprement dit a commencé le 21 janvier 1929.
Malgré un rythme effréné de construction de charpente métallique d'environ quatre étages par semaine, aucun travailleur n'est mort pendant la construction de la charpente métallique du gratte-ciel.
Le 40 Wall Street et le Chrysler Building ont commencé à concourir pour la distinction de "bâtiment le plus haut du monde".
Le 23 octobre 1929, une semaine après avoir dépassé la hauteur du Woolworth Building et un jour avant le début du catastrophique crash de Wall Street en 1929, la flèche a été assemblée.
Même le New York Herald Tribune , qui avait une couverture pratiquement continue de la construction de la tour, n'a rendu compte de l'installation de la flèche que quelques jours après que la flèche ait été élevée.
Dans le hall de l'immeuble, une plaque de bronze sur laquelle on pouvait lire "en reconnaissance de la contribution de M. Chrysler à l'avancement civique" a été dévoilée.
Le Chrysler Building a été évalué à 14 millions de dollars, mais était exonéré de taxes municipales en vertu d'une loi de 1859 qui accordait des exonérations fiscales aux sites appartenant à la Cooper Union.
La satisfaction de Van Alen face à ces réalisations a probablement été atténuée par le refus ultérieur de Walter Chrysler de payer le solde de ses honoraires d'architecte.
Cependant, le procès contre Chrysler a considérablement diminué la réputation de Van Alen en tant qu'architecte, ce qui, avec les effets de la Grande Dépression et des critiques négatives, a fini par ruiner sa carrière.
En 1944, la société a déposé des plans pour construire une annexe de 38 étages à l'est du bâtiment, au 666 Third Avenue.
La pierre du bâtiment d'origine n'était plus fabriquée et devait être spécialement reproduite.
La famille a vendu le bâtiment en 1953 à William Zeckendorf pour son prix estimé de 18 millions de dollars.
À l'époque, il s'agissait de la plus grande vente immobilière de l'histoire de New York.
En 1961, les éléments en acier inoxydable du bâtiment, y compris l'aiguille, la couronne, les gargouilles et les portes d'entrée, ont été polis pour la première fois.
L'entreprise a acheté le bâtiment pour 35 millions de dollars.
La flèche a subi une restauration qui s'est achevée en 1995.
Le nettoyage a reçu le prix Lucy G. Moses Preservation du New York Landmarks Conservancy en 1997.
En juin 2008, il a été signalé que le Conseil d'investissement d'Abu Dhabi était en négociation pour acheter la participation économique de 75% de TMW, une participation de 15% de Tishman Speyer Properties dans le bâtiment et une part de la structure de vente au détail Trylons à côté pour 800 millions de dollars américains. .
Cela s'est traduit par une diminution de 21 % de la consommation totale d'énergie du bâtiment, une diminution de 64 % de la consommation d'eau et un taux de recyclage des déchets de 81 %.
L'éthique de la pratique philosophique."
"La philosophie est une pensée rationnellement critique, de type plus ou moins systématique sur la nature générale du monde (métaphysique ou théorie de l'existence), la justification de la croyance (épistémologie ou théorie de la connaissance) et la conduite de la vie (éthique ou théorie de valeur)."
La métaphysique remplace les hypothèses non argumentées contenues dans une telle conception par un ensemble rationnel et organisé de croyances sur le monde dans son ensemble.
Au XIXe siècle, la croissance des universités de recherche modernes a conduit la philosophie universitaire et d'autres disciplines à se professionnaliser et à se spécialiser.
Dans Against the Logicians, le philosophe pyrrhoniste Sextus Empiricus a détaillé la variété des façons dont les anciens philosophes grecs avaient divisé la philosophie, notant que cette division en trois parties avait été acceptée par Platon, Aristote, Xénocrate et les stoïciens.
D'autres traditions philosophiques anciennes influencées par Socrate comprenaient le cynisme, le cyrénaïsme, le stoïcisme et le scepticisme académique.
Parmi les principaux penseurs médiévaux figurent saint Augustin, Thomas d'Aquin, Boèce, Anselme et Roger Bacon.
Les principaux philosophes modernes incluent Spinoza, Leibniz, Locke, Berkeley, Hume et Kant.
L'astronomie babylonienne comprenait également de nombreuses spéculations philosophiques sur la cosmologie qui ont pu influencer les Grecs de l'Antiquité.
Plus tard, la philosophie juive a subi de fortes influences intellectuelles occidentales et comprend les œuvres de Moses Mendelssohn qui a inauguré la Haskalah (les Lumières juives), l'existentialisme juif et le judaïsme réformé.
La philosophie islamique est le travail philosophique issu de la tradition islamique et se fait principalement en arabe.
La philosophie islamique primitive a développé les traditions philosophiques grecques dans de nouvelles directions innovantes.
L'œuvre d'Aristote a été très influente parmi des philosophes tels qu'Al-Kindi (IXe siècle), Avicenne (980 - juin 1037) et Averroès (XIIe siècle).
Ibn Khaldun était un penseur influent de la philosophie de l'histoire.
Les traditions philosophiques indiennes partagent divers concepts et idées clés, qui sont définis de différentes manières et acceptés ou rejetés par les différentes traditions.
La philosophie indienne est généralement regroupée en fonction de sa relation avec les Vedas et les idées qu'ils contiennent.
Les écoles qui s'alignent sur la pensée des Upanishads, les traditions dites « orthodoxes » ou « hindoues », sont souvent classées en six darśanas ou philosophies : Sankhya, Yoga, Nyāya, Vaisheshika, Mimāmsā et Vedānta.
Ils reflètent également une tolérance pour une diversité d'interprétations philosophiques au sein de l'hindouisme tout en partageant le même fondement.
Il existe également d'autres écoles de pensée qui sont souvent considérées comme "hindoues", bien que pas nécessairement orthodoxes (puisqu'elles peuvent accepter différentes écritures comme normatives, telles que les Shaiva Agamas et les Tantras), celles-ci incluent différentes écoles de Shavisme telles que Pashupata, Shaiva Siddhanta, Shavisme tantrique non duel (c'est-à-dire Trika, Kaula, etc.).
Le déni qu'un être humain possède un "soi" ou une "âme" est probablement l'enseignement bouddhiste le plus célèbre.
La philosophie jaïn est l'une des deux seules traditions «non orthodoxes» survivantes (avec le bouddhisme).
La pensée jaïn soutient que toute existence est cyclique, éternelle et incréée.
Dans ces régions, la pensée bouddhiste s'est développée en différentes traditions philosophiques qui utilisaient diverses langues (comme le tibétain, le chinois et le pali).
La philosophie de l'école Theravada est dominante dans les pays d'Asie du Sud-Est comme le Sri Lanka, la Birmanie et la Thaïlande.
Après la mort du Bouddha, divers groupes ont commencé à systématiser ses principaux enseignements, développant finalement des systèmes philosophiques complets appelés Abhidharma.
Il y avait de nombreuses écoles, sous-écoles et traditions de philosophie bouddhiste dans l'Inde ancienne et médiévale.
Ces traditions philosophiques ont développé des théories métaphysiques, politiques et éthiques telles que Tao, Yin et yang, Ren et Li.
Le néo-confucianisme en est venu à dominer le système éducatif sous la dynastie Song (960-1297), et ses idées ont servi de base philosophique aux examens impériaux pour la classe officielle des savants.
Au cours des dynasties chinoises ultérieures comme la dynastie Ming (1368-1644) ainsi que dans la dynastie coréenne Joseon (1392-1897), un néo-confucianisme renaissant dirigé par des penseurs tels que Wang Yangming (1472-1529) est devenu l'école de pensée dominante, et a été promu par l'État impérial.
À l'ère moderne, les penseurs chinois ont incorporé des idées de la philosophie occidentale.
Par exemple, le nouveau confucianisme, dirigé par des personnalités telles que Xiong Shili, est devenu très influent.
Une autre tendance de la philosophie japonaise moderne était la tradition des « études nationales » (Kokugaku).
Au XVIIe siècle, la philosophie éthiopienne a développé une solide tradition littéraire, comme en témoigne Zera Yacob.
Une autre caractéristique des visions du monde indigènes américaines était leur extension de l'éthique aux animaux et aux plantes non humains.
La théorie de Teotl peut être considérée comme une forme de panthéisme.
Néanmoins, les rapports du Département américain de l'éducation des années 1990 indiquent que peu de femmes se sont retrouvées en philosophie, et que la philosophie est l'un des domaines les moins proportionnés au sexe dans les sciences humaines, les femmes représentant entre 17% et 30% des professeurs de philosophie selon à certaines études.
Voir aussi "Caractéristiques et attitudes du corps professoral et du personnel enseignant en sciences humaines".
Ses enquêtes principales incluent comment vivre une bonne vie et identifier les normes de moralité.
Les épistémologues examinent les sources putatives de connaissances, y compris l'expérience perceptive, la raison, la mémoire et le témoignage.
Il est apparu au début de la philosophie présocratique et s'est formalisé avec Pyrrho, le fondateur de la première école occidentale de scepticisme philosophique.
L'empirisme met l'accent sur les preuves d'observation via l'expérience sensorielle comme source de connaissances.
Le rationalisme est associé à des connaissances a priori indépendantes de l'expérience (comme la logique et les mathématiques).
La métaphysique comprend la cosmologie, l'étude du monde dans son intégralité et l'ontologie, l'étude de l'être.
L'essence est l'ensemble des attributs qui font d'un objet ce qu'il est fondamentalement et sans lesquels il perd son identité tandis que l'accident est une propriété que possède l'objet, sans laquelle l'objet peut encore conserver son identité.
Parce que le bon raisonnement est un élément essentiel de toutes les disciplines des sciences, des sciences sociales et des sciences humaines, la logique est devenue une science formelle.
New York : presse universitaire d'Oxford.
Cependant, la plupart des étudiants en philosophie académique contribuent plus tard au droit, au journalisme, à la religion, aux sciences, à la politique, aux affaires ou à divers arts.
Dans la philosophie analytique, la philosophie du langage étudie la nature du langage, les relations entre le langage, les utilisateurs du langage et le monde.
Ces écrivains ont été suivis par Ludwig Wittgenstein (Tractatus Logico-Philosophicus), le Cercle de Vienne ainsi que les positivistes logiques et Willard Van Orman Quine.
Il a critiqué le conventionnalisme parce qu'il a conduit à la conséquence bizarre que tout peut être conventionnellement dénommé par n'importe quel nom.
Pour ce faire, il a souligné que les mots et les phrases composés ont une gamme d'exactitude.
Cependant, à la fin du Cratylus, il avait admis que certaines conventions sociales étaient également impliquées et qu'il y avait des défauts dans l'idée que les phonèmes avaient des significations individuelles.
Il a séparé toutes les choses en catégories d'espèces et de genres.
Cependant, comme Aristote considère ces similitudes comme constituées d'une réelle communauté de forme, il est plus souvent considéré comme un partisan du « réalisme modéré ».
Ce lektón était la signification (ou le sens) de chaque terme.
Il y avait plusieurs philosophes remarquables du langage à l'époque médiévale.
Les scolastiques du haut Moyen Âge, comme Ockham et John Duns Scot, considéraient la logique comme une scientia sermocinalis (science du langage).
Les phénomènes d'imprécision et d'ambiguïté ont été analysés en profondeur, ce qui a conduit à un intérêt croissant pour les problèmes liés à l'utilisation de mots syncatégoriématiques tels que et, ou, non, si et tous.
La supposition d'un terme est l'interprétation qui en est donnée dans un contexte spécifique.
Un tel système de classification est le précurseur des distinctions modernes entre usage et mention, et entre langue et métalangage.
Une partie de la phrase courante est le mot lexical, composé de noms, de verbes et d'adjectifs.
La sémantique philosophique a tendance à se concentrer sur le principe de compositionnalité pour expliquer la relation entre les parties significatives et les phrases entières.
Il est possible d'utiliser le concept de fonctions pour décrire plus que le simple fonctionnement des significations lexicales : elles peuvent également être utilisées pour décrire le sens d'une phrase.
Une fonction propositionnelle est une opération du langage qui prend une entité (dans ce cas, le cheval) comme entrée et produit un fait sémantique (c'est-à-dire la proposition qui est représentée par "Le cheval est rouge").
L'acquisition du langage est-elle une faculté spéciale de l'esprit ?
La première est la perspective behavioriste, qui dicte que non seulement l'essentiel du langage est appris, mais qu'il est appris via le conditionnement.
Les modèles nativistes affirment qu'il existe des dispositifs spécialisés dans le cerveau qui sont dédiés à l'acquisition du langage.
Les linguistes Sapir et Whorf ont suggéré que la langue limitait la mesure dans laquelle les membres d'une «communauté linguistique» peuvent réfléchir à certains sujets (une hypothèse mise en parallèle dans le roman de George Orwell Nineteen Eighty-Four ).
Le contraire de la position Sapir-Whorf est la notion selon laquelle la pensée (ou, plus largement, le contenu mental) a priorité sur le langage.
Un autre argument est qu'il est difficile d'expliquer comment des signes et des symboles sur papier peuvent représenter quelque chose de significatif à moins qu'une sorte de sens ne leur soit insufflé par le contenu de l'esprit.
Une autre tradition de philosophes a tenté de montrer que le langage et la pensée sont coextensifs – qu'il n'y a pas moyen d'expliquer l'un sans l'autre.
Dans une certaine mesure, les fondements théoriques de la sémantique cognitive (y compris la notion de cadrage sémantique) suggèrent l'influence du langage sur la pensée.
Il existe des études qui prouvent que les langues façonnent la façon dont les gens comprennent la causalité.
Cependant, les locuteurs espagnols ou japonais seraient plus susceptibles de dire "le vase s'est cassé".
Les hispanophones et les japonais ne se souvenaient pas des agents des événements accidentels aussi bien que les anglophones.
Dans une étude, des locuteurs allemands et espagnols ont été invités à décrire des objets ayant une attribution de genre opposé dans ces deux langues.
Pour décrire un « pont », féminin en allemand et masculin en espagnol, les germanophones disaient « beau », « élégant », « fragile », « paisible », « joli » et « élancé », et les hispanophones disaient "gros", "dangereux", "long", "fort", "robuste" et "imposant".
Le fait que chaque extraterrestre soit amical ou hostile était déterminé par certaines caractéristiques subtiles, mais les participants n'étaient pas informés de ce qu'elles étaient.
Pour le reste, les extraterrestres sont restés sans nom.
Il a été conclu que nommer des objets nous aide à les catégoriser et à les mémoriser.
Dans ce domaine, les questions comprennent : la nature de la synonymie, les origines du sens lui-même, notre appréhension du sens et la nature de la composition (la question de savoir comment les unités significatives du langage sont composées de parties significatives plus petites, et comment le sens du tout est dérivé de la signification de ses parties).
La théorie idéationnelle du sens, le plus souvent associée à l'empiriste britannique John Locke, affirme que les significations sont des représentations mentales provoquées par des signes.
(Voir aussi la théorie des images du langage de Wittgenstein.)
Cambridge, Massachusetts : Harvard University Press.
La théorie de référence du sens, également connue collectivement sous le nom d'externalisme sémantique, considère le sens comme équivalent à ces choses dans le monde qui sont réellement liées aux signes.
La formulation traditionnelle d'une telle théorie est que le sens d'une phrase est sa méthode de vérification ou de falsification.
Dans cette version, la compréhension (et donc le sens) d'une phrase consiste en la capacité de l'auditeur à reconnaître la démonstration (mathématique, empirique ou autre) de la vérité de la phrase.
Une théorie pragmatique du sens est toute théorie dans laquelle le sens (ou la compréhension) d'une phrase est déterminé par les conséquences de son application.
Gottlob Frege était un défenseur d'une théorie de la référence médiatisée.
Une telle pensée est abstraite, universelle et objective.
Les référents sont les objets du monde que les mots sélectionnent.
Il considérait les noms propres du type décrit ci-dessus comme des "descriptions définies abrégées" (voir Théorie des descriptions ).
De telles phrases dénotent dans le sens qu'il y a un objet qui satisfait la description.
Selon Frege, toute expression de référence a un sens en plus d'un référent.
Malgré les différences entre les points de vue de Frege et de Russell, ils sont généralement regroupés en tant que descriptivistes des noms propres.
Considérez le nom d'Aristote et les descriptions "le plus grand étudiant de Platon", "le fondateur de la logique" et "le professeur d'Alexandre".
Il a peut-être existé et n'est pas du tout devenu connu de la postérité ou il peut être mort en bas âge.
Mais c'est profondément contre-intuitif.
Des questions surgissent inévitablement sur des sujets environnants.
David Kellogg Lewis a proposé une réponse valable à la première question en exposant le point de vue selon lequel une convention est une régularité de comportement rationnellement auto-entretenue.
Noam Chomsky a proposé que l'étude du langage puisse se faire en termes de I-Langage, ou langage interne des personnes.
Une source fructueuse de recherche implique l'investigation des conditions sociales qui donnent lieu à, ou sont associées à, des significations et des langues.
Les présomptions qui étayent chaque point de vue théorique intéressent le philosophe du langage.
La rhétorique est l'étude des mots particuliers que les gens utilisent pour obtenir l'effet émotionnel et rationnel approprié chez l'auditeur, que ce soit pour persuader, provoquer, aimer ou enseigner.
Il a également des applications à l'étude et à l'interprétation du droit et aide à donner un aperçu du concept logique du domaine du discours.
L'idée de langage est souvent liée à celle de logique dans son sens grec de « logos », c'est-à-dire de discours ou de dialectique.
Heidegger combine la phénoménologie avec l'herméneutique de Wilhelm Dilthey.
Par exemple, Sein (être), le mot lui-même, est saturé de multiples sens.
Heidegger affirme que l'écriture n'est qu'un complément à la parole, car même les lecteurs construisent ou contribuent leur propre « discours » en lisant.
Dans Truth and Method, Gadamer décrit le langage comme « le moyen par lequel une compréhension et un accord substantiels ont lieu entre deux personnes.
Paul Ricœur, au contraire, propose une herméneutique qui, renouant avec le sens originel grec du terme, met l'accent sur la découverte de sens cachés dans les termes (ou « symboles ») équivoques du langage courant.
Elle leur permet de profiter et de manipuler efficacement le monde extérieur afin de se créer du sens et de transmettre ce sens aux autres.
Certaines figures importantes de l'histoire de la sémiotique sont Charles Sanders Peirce, Roland Barthes et Roman Jakobson.
Le romantisme du XIXe siècle a mis l'accent sur l'action humaine et le libre arbitre dans la construction du sens.
Les visions humanistes sont remises en question par les théories biologiques du langage qui considèrent les langues comme des phénomènes naturels.
Dans le néo-darwinisme, Richard Dawkins et d'autres partisans des théories du réplicateur culturel considèrent les langues comme des populations de virus mentaux.
Certains ont dit que l'expression représente un universel réel et abstrait dans le monde appelé "roches".
Le problème ici peut être expliqué si nous examinons la proposition "Socrate est un homme".
Ces deux choses se connectent d'une manière ou d'une autre ou se chevauchent.
Une autre perspective est de considérer "l'homme" comme une propriété de l'entité, "Socrate".
Certains des membres les plus éminents de cette tradition de sémantique formelle incluent Tarski, Carnap, Richard Montague et Donald Davidson.
Ils ne croyaient pas que les dimensions sociales et pratiques du sens linguistique pouvaient être saisies par des tentatives de formalisation utilisant les outils de la logique.
Beaucoup de ses idées ont été absorbées par des théoriciens tels que Kent Bach, Robert Brandom, Paul Horwich et Stephen Neale.
Dans Word and Object, Quine demande aux lecteurs d'ibaminer une situation dans laquelle ils sont confrontés à un groupe d'indigènes auparavant sans papiers où ils doivent tenter de donner un sens aux paroles et aux gestes de ses membres.
Tout ce qui peut être fait est d'examiner l'énoncé comme faisant partie du comportement linguistique global de l'individu, puis d'utiliser ces observations pour interpréter le sens de tous les autres énoncés.
Pour Quine, comme pour Wittgenstein et Austin, le sens n'est pas quelque chose qui est associé à un seul mot ou à une seule phrase, mais plutôt quelque chose qui, s'il peut être attribué du tout, ne peut être attribué qu'à une langue entière.
Les cas spécifiques d'imprécision qui intéressent le plus les philosophes du langage sont ceux où l'existence de « cas limites » rend apparemment impossible de dire si un prédicat est vrai ou faux.
La philosophie des mathématiques est la branche de la philosophie qui étudie les hypothèses, les fondements et les implications des mathématiques.
Aujourd'hui, certains philosophes des mathématiques visent à rendre compte de cette forme d'enquête et de ses produits tels qu'ils sont, tandis que d'autres soulignent un rôle pour eux-mêmes qui va au-delà de la simple interprétation à l'analyse critique.
La philosophie grecque sur les mathématiques a été fortement influencée par leur étude de la géométrie.
Ainsi, 3, par exemple, représentait une certaine multitude d'unités, et n'était donc pas "vraiment" un nombre.
Ces premières idées grecques sur les nombres ont ensuite été bouleversées par la découverte de l'irrationalité de la racine carrée de deux.
Selon la légende, les autres pythagoriciens ont été tellement traumatisés par cette découverte qu'ils ont assassiné Hippase pour l'empêcher de répandre son idée hérétique.
C'est un casse-tête profond que d'une part les vérités mathématiques semblent avoir une inévitabilité convaincante, mais d'autre part la source de leur "véracité" reste insaisissable.
Trois écoles, le formalisme, l'intuitionnisme et le logicisme, ont émergé à cette époque, en partie en réponse à l'inquiétude de plus en plus répandue selon laquelle les mathématiques telles qu'elles étaient, et l'analyse en particulier, n'étaient pas à la hauteur des normes de certitude et de rigueur qui avaient été prises pour accordé.
Au fil du siècle, le centre d'intérêt initial s'est étendu à une exploration ouverte des axiomes fondamentaux des mathématiques, l'approche axiomatique étant considérée comme allant de soi depuis l'époque d'Euclide vers 300 avant notre ère comme base naturelle des mathématiques.
En mathématiques, comme en physique, des idées nouvelles et inattendues avaient surgi et des changements significatifs s'annonçaient.
Je ne pense pas que les difficultés que la philosophie rencontre aujourd'hui avec les mathématiques classiques soient de véritables difficultés ; et je pense que les interprétations philosophiques des mathématiques qui nous sont proposées de toutes parts sont fausses, et que "l'interprétation philosophique" est exactement ce dont les mathématiques n'ont pas besoin.
De nombreux mathématiciens actifs ont été des réalistes mathématiques ; ils se considèrent comme des découvreurs d'objets naturels.
Certains principes (par exemple, pour deux objets quelconques, il existe une collection d'objets constituée précisément de ces deux objets) pourraient être directement considérés comme vrais, mais la conjecture de l'hypothèse du continuum pourrait s'avérer indécidable uniquement sur la base de tels principes.
La caverne de Platon et le platonisme ont tous deux des liens significatifs, et pas seulement superficiels, car les idées de Platon ont été précédées et probablement influencées par les pythagoriciens extrêmement populaires de la Grèce antique, qui croyaient que le monde était, littéralement, généré par les nombres.
Ce point de vue ressemble à beaucoup de choses que Husserl a dites sur les mathématiques et soutient l'idée de Kant selon laquelle les mathématiques sont synthétiques a priori.)
Le platonisme de sang pur est une variante moderne du platonisme, qui est en réaction au fait que différents ensembles d'entités mathématiques peuvent être prouvés en fonction des axiomes et des règles d'inférence employés (par exemple, la loi du tiers exclu et la axiome du choix).
Le réalisme de la théorie des ensembles (également platonisme de la théorie des ensembles), une position défendue par Penelope Maddy, est l'idée que la théorie des ensembles concerne un seul univers d'ensembles.
Ils ont attribué le paradoxe à la « circularité vicieuse » et ont construit ce qu'ils ont appelé la théorie des types ramifiés pour y faire face.
Même Russell disait que cet axiome n'appartenait pas vraiment à la logique.
Frege a exigé que la loi fondamentale V puisse donner une définition explicite des nombres, mais toutes les propriétés des nombres peuvent être dérivées du principe de Hume.
Mais cela permet au mathématicien en exercice de poursuivre son travail et de laisser ces problèmes au philosophe ou au scientifique.
Hilbert visait à montrer la cohérence des systèmes mathématiques à partir de l'hypothèse que «l'arithmétique finitaire» (un sous-système de l'arithmétique habituelle des entiers positifs, choisie pour être philosophiquement non controversée) était cohérente.
Ainsi, afin de montrer que tout système axiomatique de mathématiques est en fait cohérent, il faut d'abord supposer la cohérence d'un système de mathématiques qui est en un sens plus fort que le système à prouver cohérent.
D'autres formalistes, tels que Rudolf Carnap, Alfred Tarski et Haskell Curry, considéraient les mathématiques comme l'étude des systèmes d'axiomes formels.
Plus nous étudions de jeux, mieux c'est.
La principale critique du formalisme est que les idées mathématiques réelles qui occupent les mathématiciens sont très éloignées des jeux de manipulation de cordes mentionnés ci-dessus.
Brouwer, le fondateur du mouvement, soutenait que les objets mathématiques naissent des formes a priori des volitions qui informent la perception des objets empiriques.
L'axiome du choix est également rejeté dans la plupart des théories des ensembles intuitionnistes, bien que dans certaines versions, il soit accepté.
De ce point de vue, les mathématiques sont un exercice de l'intuition humaine, pas un jeu joué avec des symboles sans signification.
De même, tous les autres nombres entiers sont définis par leur place dans une structure, la droite numérique.
Cependant, sa revendication centrale ne concerne que le type d'entité qu'est un objet mathématique, et non le type d'existence des objets ou structures mathématiques (pas, en d'autres termes, leur ontologie).
Les structures sont tenues pour avoir une existence réelle mais abstraite et immatérielle.
Les structures sont tenues pour exister dans la mesure où un système concret les illustre.
Comme le nominalisme, l'approche post rem nie l'existence d'objets mathématiques abstraits avec des propriétés autres que leur place dans une structure relationnelle.
On soutient que les mathématiques ne sont pas universelles et n'existent dans aucun sens réel, autre que dans le cerveau humain.
Cependant, l'esprit humain n'a aucune prétention particulière à la réalité ou à ses approches fondées sur les mathématiques.
Le traitement le plus accessible, le plus célèbre et le plus infâme de cette perspective est Where Mathematics Comes From, de George Lakoff et Rafael E. Núñez.
Franklin, James (2014), "Une philosophie réaliste aristotélicienne des mathématiques", Palgrave Macmillan, Basingstoke; Franklin, James (2021), "Les mathématiques en tant que science de la réalité non abstraite: philosophies réalistes aristotéliciennes des mathématiques", Foundations of Science 25.
L'arithmétique euclidienne développée par John Penn Mayberry dans son livre The Foundations of Mathematics in the Theory of Sets s'inscrit également dans la tradition réaliste aristotélicienne.
Edmund Husserl, dans le premier volume de ses Recherches logiques, intitulé "Les prolégomènes de la logique pure", a critiqué à fond le psychologisme et a cherché à s'en distancier.
Autrement dit, puisque la physique a besoin de parler d'électrons pour dire pourquoi les ampoules se comportent comme elles le font, alors les électrons doivent exister.
Il plaide pour l'existence d'entités mathématiques comme la meilleure explication de l'expérience, privant ainsi les mathématiques d'être distinctes des autres sciences.
Cela est né de l'affirmation de plus en plus populaire à la fin du XXe siècle selon laquelle l'existence d'un seul fondement des mathématiques ne pouvait jamais être prouvée.
Un argument mathématique peut transmettre le faux de la conclusion aux prémisses aussi bien qu'il peut transmettre le vrai des prémisses à la conclusion.
Il a donné un argument détaillé pour cela dans New Directions.
Si les mathématiques sont tout aussi empiriques que les autres sciences, cela suggère que leurs résultats sont tout aussi faillibles que les leurs, et tout aussi contingents.
Pour une philosophie des mathématiques qui tente de surmonter certaines des lacunes des approches de Quine et Gödel en prenant des aspects de chacun, voir Penelope Maddy's Realism in Mathematics.
Il a commencé par "l'intermédiarité" des axiomes de Hilbert pour caractériser l'espace sans le coordonner, puis a ajouté des relations supplémentaires entre les points pour effectuer le travail autrefois effectué par les champs vectoriels.
De ce point de vue, il n'y a pas de problèmes métaphysiques ou épistémologiques particuliers aux mathématiques.
Cependant, alors que d'un point de vue empiriste l'évaluation est une sorte de comparaison avec la "réalité", les constructivistes sociaux soulignent que l'orientation de la recherche mathématique est dictée par les modes du groupe social qui l'exécute ou par les besoins de la société qui la finance.
Mais les constructivistes sociaux soutiennent que les mathématiques sont en fait fondées sur une grande incertitude : à mesure que la pratique mathématique évolue, le statut des mathématiques antérieures est mis en doute et est corrigé dans la mesure où il est requis ou souhaité par la communauté mathématique actuelle.
La nature sociale des mathématiques est mise en évidence dans ses sous-cultures.
Les constructivistes sociaux voient le processus de « faire des mathématiques » comme créant réellement le sens, tandis que les réalistes sociaux voient une déficience soit de la capacité humaine d'abstraction, soit du biais cognitif humain, soit de l'intelligence collective des mathématiciens comme empêchant la compréhension d'un univers réel de objets mathématiques.
Plus récemment, Paul Ernest a explicitement formulé une philosophie sociale constructiviste des mathématiques.
Par exemple, les outils de la linguistique ne sont généralement pas appliqués aux systèmes de symboles des mathématiques, c'est-à-dire que les mathématiques sont étudiées d'une manière nettement différente des autres langues.
Cependant, les méthodes développées par Frege et Tarski pour l'étude du langage mathématique ont été considérablement étendues par l'étudiant de Tarski, Richard Montague, et d'autres linguistes travaillant dans la sémantique formelle pour montrer que la distinction entre le langage mathématique et le langage naturel n'est peut-être pas aussi grande qu'il n'y paraît. .
L'affirmation selon laquelle "toutes" les entités postulées dans les théories scientifiques, y compris les nombres, devraient être acceptées comme réelles est justifiée par le holisme de confirmation.
Field a développé ses vues dans le romantisme.
L'argument repose sur l'idée qu'un compte rendu naturaliste satisfaisant des processus de pensée en termes de processus cérébraux peut être donné pour le raisonnement mathématique avec tout le reste.
Une autre ligne de défense consiste à maintenir que les objets abstraits sont pertinents pour le raisonnement mathématique d'une manière non causale et non analogue à la perception.
A titre d'exemple, ils fournissent deux preuves de l'irrationalité de .
Paul Erdős était bien connu pour sa notion d'un "livre" hypothétique contenant les preuves mathématiques les plus élégantes ou les plus belles.
De la même manière, cependant, les philosophes des mathématiques ont cherché à caractériser ce qui rend une preuve plus désirable qu'une autre lorsque les deux sont logiquement valables.
La philosophie de l'esprit est une branche de la philosophie qui étudie l'ontologie et la nature de l'esprit et sa relation avec le corps.
Le dualisme et le monisme sont les deux écoles de pensée centrales sur le problème corps-esprit, bien que des points de vue nuancés soient apparus qui ne correspondent pas parfaitement à l'une ou l'autre catégorie.
Hart, WD (1996) "Dualism", dans Samuel Guttenplan (org) A Companion to the Philosophy of Mind, Blackwell, Oxford, 265-7.
Pinel, J. Psychobiology, (1990) Prentice Hall, Inc. LeDoux, J. (2002) Le Soi Synaptique : Comment Notre Cerveau Devient Qui Nous Sommes, New York : Viking Penguin.
Prédicats psychologiques", dans WH Capitan et DD Merrill, eds.,
Deuxièmement, les états intentionnels de conscience n'ont pas de sens sur le physicalisme non réducteur.
Le désir de quelqu'un pour une part de pizza, par exemple, aura tendance à amener cette personne à bouger son corps d'une manière spécifique et dans une direction spécifique pour obtenir ce qu'elle veut.
Robinson, H. (1983): "Dualisme aristotélicien", Oxford Studies in Ancient Philosophy 1, 123–44.
Ils nieraient presque certainement que l'esprit est simplement le cerveau, ou vice versa, trouvant l'idée qu'il n'y a qu'une seule entité ontologique en jeu trop mécaniste ou inintelligible.
Ainsi, par exemple, on peut raisonnablement se demander à quoi ressemble un doigt brûlé, ou à quoi ressemble un ciel bleu, ou à quoi ressemble une belle musique pour une personne.
Il y a des qualia impliqués dans ces événements mentaux qui semblent particulièrement difficiles à réduire à quelque chose de physique.
Le dualisme doit donc expliquer comment la conscience affecte la réalité physique.
La connaissance, cependant, est appréhendée en raisonnant du fondement au conséquent.
L'idée de base est qu'on peut ibaminer son corps, et donc concevoir l'existence de son corps, sans qu'aucun état de conscience ne soit associé à ce corps.
D'autres, comme Dennett, ont soutenu que la notion de zombie philosophique est un concept incohérent ou improbable.
C'est le point de vue selon lequel les états mentaux, tels que les croyances et les désirs, interagissent de manière causale avec les états physiques.
L'argument de Descartes repose sur la prémisse que ce que Seth croit être des idées "claires et distinctes" dans son esprit sont nécessairement vraies.
Cambridge, MA : MIT Press (Bradford) Par exemple, Joseph Agassi suggère que plusieurs découvertes scientifiques faites depuis le début du XXe siècle ont sapé l'idée d'un accès privilégié à ses propres idées.
Ce point de vue a été principalement défendu par Gottfried Leibniz.
Ces propriétés émergentes ont un statut ontologique indépendant et ne peuvent être réduites ou expliquées en termes de substrat physique d'où elles émergent.
L'épiphénoménisme est une doctrine formulée pour la première fois par Thomas Henry Huxley.
Ce point de vue a été défendu par Frank Jackson.
Le panpsychisme est l'idée que toute matière a un aspect mental ou, alternativement, que tous les objets ont un centre unifié d'expérience ou de point de vue.
Un exemple de ces degrés de liberté disparates est donné par Allan Wallace qui note qu'il est "expérimentalement évident que l'on peut être physiquement mal à l'aise - par exemple, tout en s'engageant dans un entraînement physique intense - tout en étant mentalement joyeux ; à l'inverse, on peut être mentalement désemparé tout en éprouvant un confort physique".
Les états mentaux peuvent provoquer des changements dans les états physiques et vice versa.
Le dualisme expérientiel est accepté comme cadre conceptuel du bouddhisme madhyamaka.
En niant l'existence autonome indépendante de tous les phénomènes qui composent le monde de notre expérience, le point de vue du Madhyamaka s'écarte à la fois du dualisme de la substance de Descartes et du monisme de la substance - à savoir le physicalisme - qui est caractéristique de la science moderne.
En effet, le physicalisme, ou l'idée que la matière est la seule substance fondamentale de la réalité, est explicitement rejeté par le bouddhisme.
Alors que les premiers ont généralement une masse, un emplacement, une vitesse, une forme, une taille et de nombreux autres attributs physiques, ceux-ci ne sont généralement pas caractéristiques des phénomènes mentaux.
La nature fondamentalement disparate de la réalité est au cœur des formes de philosophies orientales depuis plus de deux millénaires.
Le monisme physicaliste affirme que la seule substance existante est physique, dans un certain sens de ce terme à clarifier par notre meilleure science.
Bien que l'idéalisme pur, tel que celui de George Berkeley, soit rare dans la philosophie occidentale contemporaine, une variante plus sophistiquée appelée panpsychisme, selon laquelle l'expérience et les propriétés mentales peuvent être à la base de l'expérience et des propriétés physiques, a été adoptée par certains philosophes tels que comme Alfred North Whitehead et David Ray Griffin.
Une troisième possibilité est d'accepter l'existence d'une substance de base qui n'est ni physique ni mentale.
Les rapports introspectifs sur sa propre vie mentale intérieure ne sont pas soumis à un examen minutieux de l'exactitude et ne peuvent pas être utilisés pour former des généralisations prédictives.
Parallèlement à ces développements en psychologie, un behaviorisme philosophique (parfois appelé behaviorisme logique) s'est développé.
Ces philosophes ont estimé que, si les états mentaux sont quelque chose de matériel, mais pas de comportement, alors les états mentaux sont probablement identiques aux états internes du cerveau.
Selon les théories de l'identité symbolique, le fait qu'un certain état cérébral soit lié à un seul état mental d'une personne ne signifie pas nécessairement qu'il existe une corrélation absolue entre les types d'état mental et les types d'état cérébral.
Enfin, l'idée de Wittgenstein du sens comme usage a conduit à une version du fonctionnalisme en tant que théorie du sens, développée plus avant par Wilfrid Sellars et Gilbert Harman.
Dès lors, la question se pose de savoir s'il peut encore y avoir un physicalisme non réducteur.
Davidson utilise la thèse de la survenance : les états mentaux surviennent sur les états physiques, mais ne leur sont pas réductibles. "
Le cerveau continue d'un instant à l'autre ; le cerveau a donc une identité dans le temps.
Une analogie du moi ou du « je » serait la flamme d'une bougie.
La flamme présente une sorte de continuité dans la mesure où la bougie ne s'éteint pas pendant qu'elle brûle, mais il n'y a pas vraiment d'identité de la flamme d'un instant à l'autre dans le temps.
De même, c'est une illusion que l'on est le même individu qui est entré en classe ce matin.
Ceci est analogue aux propriétés physiques du cerveau donnant naissance à un état mental.
Les Churchlands invoquent souvent le sort d'autres théories et ontologies populaires erronées qui ont surgi au cours de l'histoire.
Certains philosophes soutiennent que c'est parce qu'il existe une confusion conceptuelle sous-jacente.
Au contraire, il faudrait simplement accepter que l'expérience humaine puisse être décrite de différentes manières, par exemple, dans un vocabulaire mental et dans un vocabulaire biologique.
Le cerveau est simplement le mauvais contexte pour l'utilisation du vocabulaire mental - la recherche des états mentaux du cerveau est donc une erreur de catégorie ou une sorte d'erreur de raisonnement.
Et il est caractéristique d'un état mental qu'il ait une certaine qualité expérientielle, par exemple de la douleur, qu'il fasse mal.
L'existence d'événements cérébraux, en eux-mêmes, ne peut pas expliquer pourquoi ils sont accompagnés de ces expériences qualitatives correspondantes.
Cela découle d'une hypothèse sur la possibilité d'explications réductrices.
Le philosophe allemand du XXe siècle Martin Heidegger a critiqué les hypothèses ontologiques qui sous-tendent un modèle aussi réducteur et a affirmé qu'il était impossible de donner un sens à l'expérience en ces termes.
Ce problème d'explication des aspects introspectifs à la première personne des états mentaux et de la conscience en général en termes de neurosciences quantitatives à la troisième personne est appelé le fossé explicatif.
Il s'agit de deux catégories distinctes et l'une ne peut être réduite à l'autre.
Pour Nagel, la science n'est pas encore capable d'expliquer l'expérience subjective parce qu'elle n'a pas encore atteint le niveau ou le type de connaissance requis.
Cette propriété des états mentaux implique qu'ils ont des contenus et des référents sémantiques et peuvent donc se voir attribuer des valeurs de vérité.
Mais les idées ou les jugements mentaux sont vrais ou faux, alors comment les états mentaux (idées ou jugements) peuvent-ils être des processus naturels ?
Si le fait est vrai, alors l'idée est vraie ; sinon, c'est faux.
Puisque les processus mentaux sont intimement liés aux processus corporels, les descriptions que les sciences naturelles fournissent des êtres humains jouent un rôle important dans la philosophie de l'esprit.
Dans le domaine de la neurobiologie, de nombreuses sous-disciplines s'intéressent aux relations entre les états et processus mentaux et physiques : La neurophysiologie sensorielle étudie la relation entre les processus de perception et de stimulation.
Enfin, la biologie évolutive étudie les origines et le développement du système nerveux humain et, dans la mesure où celui-ci est à la base de l'esprit, décrit également le développement ontogénétique et phylogénétique des phénomènes mentaux à partir de leurs stades les plus primitifs.
Un exemple simple est la multiplication.
Cette question a été propulsée au premier plan de nombreux débats philosophiques en raison des enquêtes dans le domaine de l'intelligence artificielle (IA).
L'objectif de l'IA forte, au contraire, est un ordinateur doté d'une conscience similaire à celle des êtres humains.
Le test de Turing a fait l'objet de nombreuses critiques, dont la plus célèbre est probablement l'expérience de pensée de chambre chinoise formulée par Searle.
La psychologie étudie les lois qui lient ces états mentaux les uns aux autres ou avec des entrées et des sorties de l'organisme humain.
Une loi de la psychologie des formes dit que les objets qui se déplacent dans la même direction sont perçus comme liés les uns aux autres.
Il comprend des recherches sur l'intelligence et le comportement, en particulier sur la manière dont l'information est représentée, traitée et transformée (dans des facultés telles que la perception, le langage, la mémoire, le raisonnement et l'émotion) dans les systèmes nerveux (humains ou autres animaux) et les machines (par exemple, les ordinateurs ).
Néanmoins, l'œuvre de Hegel diffère radicalement du style de la philosophie anglo-américaine de l'esprit.
La phénoménologie, fondée par Edmund Husserl, se concentre sur le contenu de l'esprit humain (voir noème) et sur la façon dont les processus façonnent nos expériences.
C'est le cas des déterministes matérialistes.
Certains poussent ce raisonnement un peu plus loin : les gens ne peuvent pas déterminer par eux-mêmes ce qu'ils veulent et ce qu'ils font.
Ceux qui adoptent cette position suggèrent que la question « Sommes-nous libres ?
Il n'est pas approprié d'identifier la liberté avec l'indétermination.
Le compatibiliste le plus important de l'histoire de la philosophie était David Hume.
Ces philosophes affirment que le cours du monde est soit a) non complètement déterminé par la loi naturelle où la loi naturelle est interceptée par une agence physiquement indépendante, b) déterminé par la loi naturelle indéterministe uniquement, ou c) déterminé par la loi naturelle indéterministe en accord avec le subjectif effort d'agence physiquement non réductible.
Ils argumentent ainsi : si notre volonté n'est déterminée par rien, alors nous désirons ce que nous désirons par pur hasard.
L'idée d'un moi comme noyau essentiel immuable découle de l'idée d'une âme immatérielle.
Mantranga, le principal organe directeur de ces États, était composé du roi, premier ministre, commandant en chef de l'armée, grand prêtre du roi.
L'Arthashastra rend compte de la science de la politique pour un dirigeant sage, des politiques pour les affaires étrangères et les guerres, du système d'un État espion et de la surveillance et de la stabilité économique de l'État.
Les principales philosophies de l'époque, le confucianisme, le légalisme, le mohisme, l'agrarisme et le taoïsme, avaient chacune un aspect politique dans leurs écoles philosophiques.
Le légalisme préconisait un gouvernement hautement autoritaire basé sur des sanctions et des lois draconiennes.
À la fin de l'Antiquité, cependant, la vision « traditionaliste » asharite de l'islam avait en général triomphé.
Cependant, dans la pensée occidentale, on suppose généralement qu'il s'agissait d'un domaine spécifique propre aux seuls grands philosophes de l'islam : al-Kindi (Alkindus), al-Farabi (Abunaser), İbn Sina (Avicenne), Ibn Bajjah (Avempace) et Ibn Rushd (Averroès).
Par exemple, les idées des Khawarij dans les toutes premières années de l'histoire islamique sur le Khilafa et la Ummah, ou celle de l'islam chiite sur le concept d'Imamah sont considérées comme des preuves de la pensée politique.
L'aristotélisme a prospéré alors que l'âge d'or islamique a vu la naissance d'une continuation des philosophes itinérants qui ont mis en œuvre les idées d'Aristote dans le contexte du monde islamique.
Parmi les autres philosophes politiques notables de l'époque, citons Nizam al-Mulk , un érudit persan et vizir de l' empire seldjoukide qui a composé le Siyasatnama , ou le «Livre du gouvernement» en anglais.
Le philosophe politique le plus influent de l'Europe médiévale était peut-être saint Thomas d'Aquin qui a aidé à réintroduire les œuvres d'Aristote, qui n'avaient été transmises à l'Europe catholique que par l'Espagne musulmane, avec les commentaires d'Averroès.
D'autres, comme Nicole Oresme dans son Livre de Politiques, ont catégoriquement nié ce droit de renverser un dirigeant injuste.
Cet ouvrage, ainsi que Les Discours, une analyse rigoureuse de l'Antiquité classique, ont beaucoup influencé la pensée politique moderne en Occident.
Quoi qu'il en soit, Machiavel présente une vision pragmatique et quelque peu conséquentialiste de la politique, selon laquelle le bien et le mal ne sont que des moyens utilisés pour atteindre une fin, c'est-à-dire l'acquisition et le maintien du pouvoir absolu.
Ces théoriciens étaient motivés par deux questions fondamentales : premièrement, par quel droit ou par quel besoin les gens forment-ils des États ; et deuxièmement, quelle pourrait être la meilleure forme pour un État.
Le terme «gouvernement» ferait référence à un groupe spécifique de personnes qui occupaient les institutions de l'État et créeraient les lois et ordonnances par lesquelles le peuple, lui-même inclus, serait lié.
Il peut également être compris comme l'idée de marché libre appliquée au commerce international.
Le critique le plus virulent de l'Église en France était François Marie Arouet de Voltaire, une figure représentative des Lumières.
Mon seul regret en mourant est de ne pouvoir vous aider dans cette noble entreprise, la plus belle et la plus respectable que l'esprit humain puisse indiquer."
Locke se tenait debout pour réfuter la théorie politique paternellement fondée de Sir Robert Filmer en faveur d'un système naturel basé sur la nature dans un système donné particulier.
Contrairement au point de vue prépondérant d'Aquin sur le salut de l'âme du péché originel, Locke croit que l'esprit de l'homme vient dans ce monde comme table rase.
Bien que l'on puisse s'inquiéter des restrictions à la liberté imposées par des monarques ou des aristocrates bienveillants, l'inquiétude traditionnelle est que lorsque les dirigeants ne sont politiquement pas responsables devant les gouvernés, ils gouverneront dans leur propre intérêt plutôt que dans l'intérêt des gouvernés.
La justice implique des devoirs qui sont des devoirs parfaits, c'est-à-dire des devoirs corrélés à des droits.
Il utilise On Liberty pour discuter de l'égalité des sexes dans la société.
La Liberté des Anciens était la liberté républicaine participative, qui donnait aux citoyens le droit d'influencer directement la politique par des débats et des votes dans l'assemblée publique.
L'ancienne liberté était également limitée à des sociétés relativement petites et homogènes, dans lesquelles les gens pouvaient être commodément réunis en un seul endroit pour traiter les affaires publiques.
Au lieu de cela, les électeurs éliraient des représentants, qui délibéreraient au Parlement au nom du peuple et éviteraient aux citoyens la nécessité d'une implication politique quotidienne.
Dans Léviathan, Hobbes a exposé sa doctrine de la fondation des États et des gouvernements légitimes et créé une science objective de la moralité.
Dans cet état, chaque personne aurait un droit, ou une licence, sur tout dans le monde.
Publié en 1762, il est devenu l'un des ouvrages de philosophie politique les plus influents de la tradition occidentale.
Ceux qui se croient les maîtres des autres sont en effet de plus grands esclaves qu'eux."
La révolution industrielle a produit une révolution parallèle dans la pensée politique.
Au milieu du XIXe siècle, le marxisme s'est développé et le socialisme en général a gagné un soutien populaire croissant, principalement de la classe ouvrière urbaine.
Contrairement à Marx qui croyait au matérialisme historique, Hegel croyait à la Phénoménologie de l'Esprit.
Dans le monde anglo-américain, l'anti-impérialisme et le pluralisme ont commencé à gagner du terrain au tournant du XXe siècle.
C'était l'époque de Jean-Paul Sartre et de Louis Althusser, et les victoires de Mao Zedong en Chine et de Fidel Castro à Cuba, ainsi que les événements de mai 1968, ont conduit à un intérêt accru pour l'idéologie révolutionnaire, en particulier de la part de la Nouvelle Gauche.
Le colonialisme et le racisme ont été des problèmes importants qui se sont posés.
La montée du féminisme, des mouvements sociaux LGBT et la fin de la domination coloniale et de l'exclusion politique de minorités telles que les Afro-Américains et les minorités sexuelles dans le monde développé ont conduit la pensée féministe, postcoloniale et multiculturelle à devenir significative.
Rawls a utilisé une expérience de pensée, la position originale, dans laquelle les partis représentatifs choisissent des principes de justice pour la structure de base de la société derrière un voile d'ignorance.
Parallèlement à la montée de l'éthique analytique dans la pensée anglo-américaine, en Europe, plusieurs nouvelles lignes de philosophie dirigées vers la critique des sociétés existantes ont vu le jour entre les années 1950 et les années 1980.
Dans des directions quelque peu différentes, un certain nombre d'autres penseurs continentaux - encore largement influencés par le marxisme - ont mis un nouvel accent sur le structuralisme et sur un «retour à Hegel».
Un autre débat s'est développé autour des critiques (distinctes) de la théorie politique libérale faites par Michael Walzer, Michael Sandel et Charles Taylor.
Les communautariens ont tendance à soutenir un plus grand contrôle local ainsi que des politiques économiques et sociales qui encouragent la croissance du capital social.
Une paire de perspectives politiques qui se chevauchent vers la fin du XXe siècle sont le républicanisme (ou néo-républicanisme ou civisme) et l'approche par les capacités.
Pour un républicain, le simple statut d'esclave, quelle que soit la manière dont cet esclave est traité, est répréhensible.
L'approche par les capacités et le républicanisme traitent le choix comme quelque chose qui doit être doté de ressources.
Remarquable pour les théories selon lesquelles les humains sont des animaux sociaux et que la polis (ville-état de la Grèce antique) existait pour apporter la bonne vie appropriée à ces animaux.
Burke était l'un des plus grands partisans de la Révolution américaine.
Chomsky est l'un des principaux critiques de la politique étrangère américaine, du néolibéralisme et du capitalisme d'État contemporain, du conflit israélo-palestinien et des principaux médias d'information.
William E. Connolly : A aidé à introduire la philosophie postmoderne dans la théorie politique et a promu de nouvelles théories du pluralisme et de la démocratie agonistique.
Thomas Hill Green : Penseur libéral moderne et premier partisan de la liberté positive.
Ses premiers travaux ont été fortement influencés par l'école de Francfort.
Il a préconisé le capitalisme de marché libre dans lequel le rôle principal de l'État est de maintenir l'État de droit et de laisser l'ordre spontané se développer.
David Hume : Hume a critiqué la théorie du contrat social de John Locke et d'autres comme reposant sur le mythe d'un accord réel.
Le plus célèbre pour la déclaration d'indépendance des États-Unis.
A soutenu qu'une organisation internationale était nécessaire pour préserver la paix mondiale.
Il s'est éloigné de Hobbes en ce que, basé sur l'hypothèse d'une société dans laquelle les valeurs morales sont indépendantes de l'autorité gouvernementale et largement partagées, il a plaidé pour un gouvernement avec un pouvoir limité à la protection de la propriété personnelle.
L'un des fondateurs du marxisme occidental.
A rendu compte de l'art de gouverner d'un point de vue réaliste au lieu de s'appuyer sur l'idéalisme.
En tant que théoricien politique, il croyait en la séparation des pouvoirs et a proposé un ensemble complet de freins et contrepoids nécessaires pour protéger les droits d'un individu de la tyrannie de la majorité.
Introduit le concept de "désublimation répressive", dans lequel le contrôle social peut fonctionner non seulement par contrôle direct, mais aussi par manipulation du désir.
Création du concept d'idéologie au sens de croyances (vraies ou fausses) qui façonnent et contrôlent les actions sociales.
Mencius : L'un des penseurs les plus importants de l'école confucéenne, il est le premier théoricien à présenter un argument cohérent en faveur d'une obligation des gouvernants envers les gouvernés.
Montesquieu : Analyse la protection du peuple par un « équilibre des pouvoirs » dans les découpages d'un État.
Ses interprètes ont débattu du contenu de sa philosophie politique.
Platon : A écrit un long dialogue La République dans lequel il expose sa philosophie politique : les citoyens doivent être divisés en trois catégories.
Ayn Rand : Fondateur de l'objectivisme et moteur des mouvements objectivistes et libertaires dans l'Amérique du milieu du XXe siècle.
Le gouvernement devait être séparé de l'économie de la même manière et pour les mêmes raisons, il était séparé de la religion.
Adam Smith : On dit souvent qu'il a fondé l'économie moderne ; a expliqué l'émergence des avantages économiques du comportement intéressé ("la main invisible") des artisans et des commerçants.
Socrate : Largement considéré comme le fondateur de la philosophie politique occidentale, via son influence parlée sur les contemporains athéniens ; puisque Socrate n'a jamais rien écrit, une grande partie de ce que nous savons de lui et de ses enseignements vient de son élève le plus célèbre, Platon.
Max Stirner : penseur important au sein de l'anarchisme et principal représentant du courant anarchiste connu sous le nom d'anarchisme individualiste.
D'autres formes de philosophie sociale comprennent la philosophie politique et la jurisprudence, qui sont largement concernées par les sociétés d'État et de gouvernement et leur fonctionnement.
La philosophie présocratique, également connue sous le nom de philosophie grecque primitive, est la philosophie grecque ancienne avant Socrate.
Leur travail et leurs écrits ont été presque entièrement perdus.
La philosophie présocratique a commencé au 6ème siècle avant notre ère avec les trois Milésiens : Thalès, Anaximandre et Anaximène.
Xénophane est connu pour sa critique de l'anthropomorphisme des dieux.
L'école éléatique (Parménide, Zénon d'Elée et Melissus) a suivi au 5ème siècle avant notre ère.
Anaxagore et Empédocle ont offert un récit pluraliste de la création de l'univers.
Il a été utilisé pour la première fois par le philosophe allemand JA Eberhard comme "vorsokratische Philosophie" à la fin du 18ème siècle.
Le terme présente des inconvénients, car plusieurs des pré-socratiques étaient très intéressés par l'éthique et la façon de vivre la meilleure vie.
Selon James Warren, la distinction entre les philosophes présocratiques et les philosophes de l'ère classique est délimitée non pas tant par Socrate, mais par la géographie et les textes qui ont survécu.
Le savant André Laks distingue deux traditions de séparation des pré-socratiques des socratiques, remontant à l'ère classique et traversant l'époque actuelle.
De nombreux ouvrages sont intitulés Peri Physeos, ou On Nature, titre probablement attribué plus tard par d'autres auteurs.
Ajoutant plus de difficulté à leur interprétation est le langage obscur qu'ils ont utilisé.
Théophraste , le successeur d'Aristote, a écrit un livre encyclopédique Opinion des physiciens qui était l'ouvrage standard sur les pré-socratiques dans les temps anciens.
Les chercheurs utilisent maintenant ce livre pour référencer les fragments à l'aide d'un schéma de codage appelé numérotation Diels-Kranz.
Après cela, il y a un code indiquant si le fragment est un témoignage, codé comme "A" ou "B" s'il s'agit d'une citation directe du philosophe.
L'ère présocratique a duré environ deux siècles, au cours desquels l'Empire perse achéménide en expansion s'étendait vers l'ouest, tandis que les Grecs avançaient sur les routes commerciales et maritimes, atteignant Chypre et la Syrie.
Les Grecs se sont révoltés en 499 avant notre ère, mais ont finalement été vaincus en 494 avant notre ère.
Plusieurs facteurs ont contribué à la naissance de la philosophie présocratique dans la Grèce antique.
Un autre facteur était la facilité et la fréquence des voyages intra-grecs, ce qui a conduit au mélange et à la comparaison des idées.
Le système politique démocratique des pôles indépendants a également contribué à l'essor de la philosophie.
Les idées des philosophes étaient, dans une certaine mesure, des réponses à des questions subtilement présentes dans l'œuvre d'Homère et d'Hésiode.
Ils sont considérés comme les prédécesseurs des présocratiques car ils cherchent à aborder l'origine du monde et à organiser systématiquement le folklore traditionnel et les légendes.
Les premiers philosophes pré-socratiques ont également beaucoup voyagé dans d'autres pays, ce qui signifie que la pensée pré-socratique avait des racines à l'étranger ainsi que dans le pays.
Les philosophes pré-socratiques partageaient l'intuition qu'il y avait une explication unique qui pouvait expliquer à la fois la pluralité et la singularité de l'ensemble - et cette explication ne serait pas les actions directes des dieux.
Beaucoup ont cherché le principe matériel (arche) des choses, et la méthode de leur origine et de leur disparition.
Dans leur effort pour donner un sens au cosmos, ils ont inventé de nouveaux termes et concepts tels que rythme, symétrie, analogie, déductionnisme, réductionnisme, mathématicisation de la nature et autres.
Cela pourrait signifier le début ou l'origine avec la nuance qu'il y a un effet sur les choses à suivre.
Cela peut être dû à un manque d'instruments, ou à une tendance à considérer le monde comme une unité, indéconstructible, de sorte qu'il serait impossible pour un œil extérieur d'observer de minuscules fractions de la nature sous contrôle expérimental.
Systématique parce qu'ils ont essayé d'universaliser leurs découvertes.
Les présocratiques n'étaient pas athées ; cependant, ils ont minimisé l'étendue de l'implication des dieux dans des phénomènes naturels tels que le tonnerre ou ont totalement éliminé les dieux du monde naturel.
La première phase de la philosophie présocratique, principalement les Milésiens, Xénophane et Héraclite, consistait à rejeter la cosmogonie traditionnelle et à tenter d'expliquer la nature à partir d'observations et d'interprétations empiriques.
Les Éléates étaient également monistes (croyant qu'une seule chose existe et que tout le reste n'en est qu'une transformation).
Il est considéré comme le premier philosophe occidental puisqu'il a été le premier à utiliser la raison, à utiliser la preuve et à généraliser.
Thalès était peut-être d'ascendance phénicienne.
Thales, cependant, a fait progresser la géométrie avec son raisonnement déductif abstrait atteignant des généralisations universelles.
Thales a visité Sardes, comme beaucoup de Grecs à l'époque, où des archives astronomiques étaient conservées et utilisaient des observations astronomiques pour des questions pratiques (récolte de pétrole).
Il a attribué l'origine du monde à un élément au lieu d'un être divin.
Il était membre de l'élite de Milet, riche et homme d'État.
En réponse à Thalès, il postule comme premier principe une substance indéfinie, illimitée, sans qualités (apeiron), à partir de laquelle les contraires primaires, chaud et froid, humide et sec, se différencient.
Il est également connu pour spéculer sur l'origine de l'humanité.
Il a également écrit un livre sur la nature en prose.
Il était un poète bien voyagé dont les principaux intérêts étaient la théologie et l'épistémologie.
Il a dit que si les bœufs, les chevaux ou les lions pouvaient dessiner, ils dessineraient leurs dieux sous forme de bœufs, de chevaux ou de lions.
Xénophane a également offert des explications naturalistes pour des phénomènes tels que le soleil, l'arc-en-ciel et le feu de Saint-Elme.
Alors que Xénophane était pessimiste quant à la capacité des humains à atteindre la connaissance, il croyait également au progrès progressif grâce à la pensée critique.
Héraclite a postulé que toutes les choses dans la nature sont dans un état de flux perpétuel.
Le feu devient eau et terre et vice versa.
Là, Héraclite affirme que nous ne pouvons pas entrer deux fois dans le même fleuve, une position résumée par le slogan ta panta rhei (tout coule).
Un autre concept clé d'Héraclite est que les contraires se reflètent en quelque sorte, une doctrine appelée unité des contraires.
La doctrine d'Héraclite sur l'unité des contraires suggère que l'unité du monde et de ses différentes parties est maintenue par la tension produite par les contraires.
Une idée fondamentale chez Héraclite est logos, un mot grec ancien avec une variété de significations; Héraclite aurait pu utiliser un sens différent du mot à chaque utilisation dans son livre.
Quelques décennies plus tard, il a dû fuir Croton et déménager à Metapontum.
Ils ont avancé ses idées, parvenant à affirmer que tout est composé de nombres, que l'univers est fait de nombres et que tout est le reflet d'analogies et de relations géométriques.
Leur mode de vie était ascétique, se retenant de divers plaisirs et de la nourriture.
D'autres philosophes pré-socratiques se sont moqués de Pythagore pour sa croyance en la réincarnation.
Le pythagorisme a influencé les courants chrétiens ultérieurs comme le néoplatonisme, et ses méthodes pédagogiques ont été adaptées par Platon.
Selon Aristote et Diogène Laërce, Xénophane était le professeur de Parménide, et on se demande si Xénophane devrait également être considéré comme un Eléatique.
Il a été le premier à déduire que la terre est sphérique.
Parménide a écrit un poème difficile à interpréter, intitulé On Nature ou On What-is , qui a considérablement influencé la philosophie grecque ultérieure.
Le poème se compose de trois parties, le proème (c'est-à-dire la préface), la Voie de la Vérité et la Voie de l'Opinion.
La Voie de la Vérité était alors, et est encore aujourd'hui, considérée comme beaucoup plus importante.
Par conséquent, toutes les choses que nous pensons être vraies, même nous-mêmes, sont de fausses représentations.
La déesse apprend à Kouros à utiliser son raisonnement pour comprendre si diverses affirmations sont vraies ou fausses, rejetant les sens comme fallacieux.
Zénon et Melissus ont poursuivi la pensée de Parménide sur la cosmologie.
Il a essayé d'expliquer pourquoi nous pensons que divers objets inexistants existent.
Anaxagore est né en Ionie, mais a été le premier grand philosophe à émigrer à Athènes.
Anaxagore a également eu une influence majeure sur Socrate.
Les interprétations diffèrent quant à ce qu'il voulait dire.
Tous les objets étaient des mélanges de divers éléments, tels que l'air, l'eau et autres.
Nous était également considéré comme un élément constitutif du cosmos, mais il n'existe que dans les objets vivants.
Anaxagore a avancé la pensée milésienne sur l'épistémologie, s'efforçant d'établir une explication qui pourrait être valable pour tous les phénomènes naturels.
Selon Diogène Laërce, Empédocle a écrit deux livres sous forme de poèmes : Peri Physeos (Sur la nature) et le Katharmoi (Purifications).
Il poursuit également la pensée d'Anaxagore sur les quatre "racines" (c'est-à-dire les éléments classiques) qui, en se mélangeant, créent toutes les choses qui nous entourent.
Ces deux forces sont opposées et en agissant sur le matériau des quatre racines unissent en harmonie ou déchirent les quatre racines, le mélange résultant étant toutes les choses qui existent.
Ils sont surtout connus pour leur cosmologie atomique même si leur pensée comprenait de nombreux autres domaines de la philosophie, tels que l'éthique, les mathématiques, l'esthétique, la politique et même l'embryologie.
Démocrite et Leucippe étaient sceptiques quant à la fiabilité de nos sens, mais ils étaient convaincus que le mouvement existe.
Les atomes se déplacent dans le vide, interagissent les uns avec les autres et forment la pluralité du monde dans lequel nous vivons, de manière purement mécanique.
Démocrite a conclu que puisque tout est atome et vide, plusieurs de nos sens ne sont pas réels mais conventionnels.
Ils ont attaqué la pensée traditionnelle, des dieux à la morale, ouvrant la voie à de nouvelles avancées de la philosophie et d'autres disciplines telles que le théâtre, les sciences sociales, les mathématiques et l'histoire.
Les sophistes enseignaient la rhétorique et comment aborder les problèmes de plusieurs points de vue.
Gorgias a écrit un livre intitulé On Nature , dans lequel il a attaqué les concepts des Éléates de ce qui est et de ce qui n'est pas.
Antiphon opposait la loi naturelle à la loi de la cité.
Il a tenté d'expliquer à la fois la variété et l'unité du cosmos.
Diogène d'Apollonie revient au monisme milésien, mais avec une pensée un peu plus élégante.
Alors que Pythagore et Empédocle liaient leur sagesse autoproclamée à leur statut d'inspiration divine, ils ont essayé d'enseigner ou d'exhorter les mortels à rechercher la vérité sur le royaume naturel - Pythagore au moyen des mathématiques et de la géométrie et Empédocle par exposition à des expériences.
Ils attaquèrent les représentations traditionnelles des dieux qu'Homère et Hésiode avaient établies et mirent à l'épreuve la religion populaire grecque, initiant le schisme entre la philosophie naturelle et la théologie.
La pensée théologique commence avec les philosophes milésiens.
Xénophane a posé trois conditions préalables à Dieu : il devait être tout bon, immortel et ne ressemblant pas aux humains en apparence, ce qui a eu un impact majeur sur la pensée religieuse occidentale.
Anaxagore affirmait que l'intelligence cosmique (nous) donne vie aux choses.
C'est Hippocrate (souvent salué comme le père de la médecine) qui a séparé – mais pas complètement – les deux domaines.
La nature en constante transformation est résumée par l'axiome panta rhei d'Héraclite (tout est dans un état de flux).
Les présocratiques ont cherché à comprendre les différents aspects de la nature par le biais du rationalisme, des observations et en proposant des explications pouvant être qualifiées de scientifiques, donnant naissance à ce qui est devenu le rationalisme occidental.
Anaximandre a proposé le principe de raison suffisante, un argument révolutionnaire qui conduirait également au principe que rien ne sort de rien.
Xénophane a également avancé une critique de la religion anthropomorphique en soulignant de manière rationnelle l'incohérence des représentations des dieux dans la religion populaire grecque.
D'autres présocratiques ont également cherché à répondre à la question de l'arche, en proposant diverses réponses, mais le premier pas vers la pensée scientifique était déjà franchi.
La pensée philosophique produite par les présocratiques a fortement influencé les philosophes, historiens et dramaturges ultérieurs.
Les naturalistes ont impressionné le jeune Socrate et il s'est intéressé à la quête de la substance du cosmos, mais son intérêt a diminué à mesure qu'il se concentrait de plus en plus sur l'épistémologie, la vertu et l'éthique plutôt que sur le monde naturel.
Cicéron a analysé ses vues sur les pré-socratiques dans ses Tusculanae Disputationes , en distinguant la nature théorique de la pensée pré-socratique des «sages» précédents qui s'intéressaient à des questions plus pratiques.
Aristote a traité des présocratiques dans le premier livre de la Métaphysique, comme une introduction à sa propre philosophie et à la quête de l'arche.
Francis Bacon, philosophe du XVIe siècle connu pour faire progresser la méthode scientifique, a probablement été le premier philosophe de l'ère moderne à utiliser abondamment les axiomes présocratiques dans ses textes.
Friedrich Nietzsche admirait profondément les présocratiques, les qualifiant de "tyrans de l'esprit" pour marquer leur antithèse et sa préférence contre Socrate et ses successeurs.
Selon son récit, limé dans plusieurs de ses livres, l'ère présocratique était l'ère glorieuse de la Grèce, tandis que le soi-disant âge d'or qui a suivi était un âge de décadence, selon Nietzsche.
Même si cette période - connue dans sa première partie comme la période des printemps et des automnes et la période des Royaumes combattants - dans sa dernière partie a été semée de chaos et de batailles sanglantes, elle est également connue sous le nom d'âge d'or de la philosophie chinoise parce qu'un large éventail de les pensées et les idées ont été développées et discutées librement.
Le taoïsme (également appelé taoïsme), une philosophie qui met l'accent sur les trois joyaux du Tao : la compassion, la modération et l'humilité, tandis que la pensée taoïste se concentre généralement sur la nature, la relation entre l'humanité et le cosmos ; santé et longévité; et wu wei (action par l'inaction).
L'agrarisme, ou l'école de l'agrarisme, qui prônait le communautarisme utopique paysan et l'égalitarisme.
Les érudits de cette école étaient de bons orateurs, débatteurs et tacticiens.
L'école des "Minor-talks", qui n'était pas une école de pensée unique, mais une philosophie construite de toutes les pensées qui étaient discutées par et provenaient de gens normaux dans la rue.
Le confucianisme était particulièrement fort pendant la dynastie Han, dont le plus grand penseur était Dong Zhongshu, qui a intégré le confucianisme aux pensées de l'école Zhongshu et à la théorie des cinq éléments.
En particulier, ils ont réfuté l'hypothèse de Confucius comme une figure divine et l'ont considéré comme le plus grand sage, mais simplement un humain et un mortel.
Le bouddhisme est arrivé en Chine vers le 1er siècle de notre ère, mais ce n'est que sous les dynasties du Nord et du Sud, Sui et Tang qu'il a acquis une influence et une reconnaissance considérables.
Cela conduit à la recherche de l'être unique qui sous-tend la diversité des phénomènes empiriques et l'origine de toutes choses.
Sept Rishis - Atri, Bharadwaja, Gautama, Jamadagni, Kasyapa, Vasishtha, Viswamitra.
La philosophie grecque antique est née au 6ème siècle avant JC, marquant la fin de l'âge des ténèbres grec.
Il traitait d'une grande variété de sujets, y compris l'astronomie, l'épistémologie, les mathématiques, la philosophie politique, l'éthique, la métaphysique, l'ontologie, la logique, la biologie, la rhétorique et l'esthétique.
Des lignes d'influence claires et ininterrompues mènent des anciens philosophes grecs et hellénistiques à la philosophie romaine, à la philosophie islamique primitive, à la scolastique médiévale, à la Renaissance européenne et au siècle des Lumières.
Mais ils ont appris à raisonner.
Thales a inspiré l'école de philosophie milésienne et a été suivi par Anaximandre, qui a soutenu que le substratum ou l'arche ne pouvait pas être de l'eau ou l'un des éléments classiques, mais était plutôt quelque chose "illimité" ou "indéfini" (en grec, l'apeiron).
Contrairement à l'école milésienne, qui postule un élément stable comme l'arche, Héraclite a enseigné que panta rhei ("tout coule"), l'élément le plus proche de ce flux éternel étant le feu.
L'être, selon lui, implique par définition l'éternité, alors que seul ce qui est peut être pensé ; une chose qui est, d'ailleurs, ne peut être plus ou moins, et ainsi la raréfaction et la condensation des Milésiens est impossible quant à l'Etre ; enfin, comme le mouvement exige que quelque chose existe en dehors de la chose en mouvement (c'est-à-dire.
À l'appui de cela, l'élève de Parménide, Zeno of Elea, a tenté de prouver que le concept de mouvement était absurde et qu'en tant que tel, le mouvement n'existait pas.
Leucippe a également proposé un pluralisme ontologique avec une cosmogonie basée sur deux éléments principaux : le vide et les atomes.
Alors que la philosophie était une activité établie avant Socrate, Cicéron le considère comme "le premier qui a fait descendre la philosophie du ciel, l'a placée dans les villes, l'a introduite dans les familles et l'a obligée à examiner la vie et la morale, le bien et le mal. "
Le fait que de nombreuses conversations impliquant Socrate (telles que racontées par Platon et Xénophon) se terminent sans avoir abouti à une conclusion définitive, ou aporétiquement, a stimulé le débat sur le sens de la méthode socratique.
Socrate a enseigné que personne ne désire ce qui est mauvais, et donc si quelqu'un fait quelque chose qui est vraiment mauvais, ce doit être à contrecœur ou par ignorance ; par conséquent, toute vertu est connaissance.
Le grand homme d'État Périclès était cependant étroitement associé à ce nouveau savoir et ami d'Anaxagore, et ses adversaires politiques l'ont frappé en profitant d'une réaction conservatrice contre les philosophes; c'est devenu un crime d'enquêter sur les choses au-dessus des cieux ou au-dessous de la terre, sujets considérés comme impies.
Socrate, cependant, est le seul sujet enregistré comme inculpé en vertu de cette loi, reconnu coupable et condamné à mort en 399 avant notre ère (voir Procès de Socrate).
Platon fait de Socrate l'interlocuteur principal de ses dialogues, en dérivant la base du platonisme (et par extension, du néoplatonisme).
Zénon de Citium a adapté à son tour l'éthique du cynisme pour articuler le stoïcisme.
Avec Xénophon, Platon est la principale source d'informations sur la vie et les croyances de Socrate et il n'est pas toujours facile de faire la distinction entre les deux.
Bien que la règle par un homme sage soit préférable à la règle par la loi, le sage ne peut s'empêcher d'être jugé par l'imprudent, et donc dans la pratique, la règle par la loi est jugée nécessaire.
Les dialogues de Platon ont aussi des thèmes métaphysiques, dont le plus célèbre est sa théorie des formes.
Il compare la plupart des humains à des personnes attachées dans une grotte, qui ne regardent que les ombres sur les murs et n'ont aucune autre conception de la réalité.
Si ces voyageurs rentraient ensuite dans la grotte, les personnes à l'intérieur (qui ne connaissent encore que les ombres) ne seraient pas équipées pour croire les rapports de ce « monde extérieur ».
Bertrand Russell, Une histoire de la philosophie occidentale (New York : Simon & Schuster, 1972).
Il critique les régimes décrits dans La République et les Lois de Platon et qualifie la théorie des formes de « mots vides et de métaphores poétiques ».
Antisthène s'inspire de l'ascétisme de Socrate et accuse Platon d'orgueil et de vanité.
Elle a été fondée par Euclide de Mégare, l'un des élèves de Socrate.
Le pyrrhonisme place l'atteinte de l' ataraxie (un état d'équanimité) comme le moyen d'atteindre l' eudaimonia .
Son éthique était basée sur "la poursuite du plaisir et l'évitement de la douleur".
Leurs contributions logiques figurent toujours dans le calcul propositionnel contemporain.
Cette période sceptique de l'ancien platonisme, d'Arcésilaus à Philon de Larissa, est devenue connue sous le nom de Nouvelle Académie, bien que certains auteurs anciens aient ajouté d'autres subdivisions, comme une Académie intermédiaire.
Alors que l'objectif des pyrrhonistes était d'atteindre l'ataraxie, après Arcésilas, les sceptiques académiques n'ont pas retenu l'ataraxie comme objectif central.
Dans l'Empire byzantin, les idées grecques ont été préservées et étudiées, et peu de temps après la première expansion majeure de l'islam, cependant, les califes abbassides ont autorisé la collecte de manuscrits grecs et engagé des traducteurs pour accroître leur prestige.
La philosophie médiévale est la philosophie qui a existé au Moyen Âge, la période s'étendant approximativement de la chute de l'Empire romain d'Occident au Ve siècle à la Renaissance au XVe siècle.
À l'exception possible d'Avicenne et d'Averroès, les penseurs médiévaux ne se considéraient pas du tout comme des philosophes : pour eux, les philosophes étaient les anciens écrivains païens tels que Platon et Aristote.
L'une des choses les plus débattues de l'époque était celle de la foi contre la raison.
On s'accorde généralement à dire qu'elle commence avec Augustin (354-430) qui appartient strictement à la période classique, et qu'elle s'achève avec le renouveau durable du savoir à la fin du XIe siècle, au début du haut Moyen Âge.
Dans les périodes ultérieures, les moines ont été utilisés pour former des administrateurs et des hommes d'église.
Une grande partie de l'œuvre d'Aristote était inconnue en Occident à cette époque.
Augustin est considéré comme le plus grand des Pères de l'Église.
Pendant plus de mille ans, il n'y a guère eu d'œuvre latine de théologie ou de philosophie qui ne cite ses écrits ou n'invoque son autorité.
Il devint consul en 510 dans le royaume des Ostrogoths.
Il a écrit des commentaires sur ces travaux, et sur l'Isagoge de Porphyre (un commentaire sur les Catégories).
Autour de cette période, plusieurs controverses doctrinales ont émergé, comme la question de savoir si Dieu avait prédestiné certains au salut et d'autres à la damnation.
L'hôte est-il le même que le corps historique du Christ ?
Cette période a également été témoin d'un renouveau de l'érudition.
Plus tard, sous saint abbon de Fleury (abbé 988-1004), chef de l'école abbatiale réformée, Fleury connut un second âge d'or.
Le début du XIIIe siècle a vu l'apogée de la récupération de la philosophie grecque.
De puissants rois normands ont rassemblé des hommes de savoir d'Italie et d'autres régions dans leurs cours en signe de leur prestige.
Les universités se sont développées dans les grandes villes d'Europe pendant cette période, et les ordres cléricaux rivaux au sein de l'Église ont commencé à se battre pour le contrôle politique et intellectuel de ces centres de la vie éducative.
Les grands représentants de la pensée dominicaine de cette période étaient Albertus Magnus et (surtout) Thomas d'Aquin, dont la synthèse astucieuse du rationalisme grec et de la doctrine chrétienne finit par définir la philosophie catholique.
Thomas d'Aquin a montré comment il était possible d'incorporer une grande partie de la philosophie d'Aristote sans tomber dans les "erreurs" du commentateur Averroès.
Le problème du mal : Les philosophes classiques avaient spéculé sur la nature du mal, mais le problème de savoir comment un Dieu tout-puissant, omniscient et aimant pourrait créer un système de choses dans lequel le mal existe est apparu pour la première fois à l'époque médiévale.
Cependant, à partir du XIVe siècle, l'utilisation croissante du raisonnement mathématique dans la philosophie naturelle a préparé la voie à l'essor de la science au début de la période moderne.
Dans la période antérieure, des écrivains tels que Peter Abelard ont écrit des commentaires sur les œuvres de la logique ancienne ( Catégories d'Aristote , Sur l'interprétation et l' Isagoge de Porphyre ).
(Le mot «intentionnalité» a été relancé par Franz Brentano, qui avait l'intention de refléter l'usage médiéval).
L'appellation « philosophie de la Renaissance » est utilisée par les spécialistes de l'histoire intellectuelle pour désigner la pensée de la période qui se déroule en Europe entre 1355 et 1650 environ (les dates se déplacent vers l'avant pour l'Europe centrale et septentrionale et pour des régions telles que l'Amérique espagnole, l'Inde, le Japon , et la Chine sous influence européenne).
L'hypothèse selon laquelle les œuvres d'Aristote étaient fondamentales pour la compréhension de la philosophie n'a pas faibli pendant la Renaissance, qui a vu fleurir de nouvelles traductions, commentaires et autres interprétations de ses œuvres, à la fois en latin et en langue vernaculaire.
Ces derniers, similaires à certains égards aux débats modernes, examinaient les avantages et les inconvénients de positions ou d'interprétations philosophiques particulières.
Platon, connu directement uniquement à travers deux dialogues et demi au Moyen Âge, est devenu connu grâce à de nombreuses traductions latines dans l'Italie du XVe siècle, culminant avec la traduction extrêmement influente de ses œuvres complètes par Marsile Ficin à Florence en 1484.
Tous les humanistes de la Renaissance n'ont pas suivi son exemple en toutes choses, mais Pétrarque a contribué à élargir le «canon» de son temps (la poésie païenne était auparavant considérée comme frivole et dangereuse), ce qui s'est également produit en philosophie.
D'autres mouvements de la philosophie ancienne ont également réintégré le courant dominant.
Cette position a été mise à rude épreuve à la Renaissance, car divers penseurs ont affirmé que les classifications de Thomas étaient inexactes et que l'éthique était la partie la plus importante de la moralité.
Comme nous l'avons vu, ils croyaient que la philosophie pouvait être placée sous l'aile de la rhétorique.
En 1416-1417, Leonardo Bruni, l'humaniste prééminent de son temps et chancelier de Florence, a retraduit l'Éthique d'Aristote dans un latin plus fluide, idiomatique et classique.
La conviction motrice était que la philosophie devait être libérée de son jargon technique afin que davantage de personnes puissent la lire.
Desiderius Erasmus, le grand humaniste néerlandais, a même préparé une édition grecque d'Aristote, et finalement ceux qui enseignent la philosophie dans les universités ont dû au moins prétendre qu'ils connaissaient le grec.
Cependant, une fois qu'il a été déterminé que l'italien était une langue à valeur littéraire et qu'elle pouvait porter le poids de la discussion philosophique, de nombreux efforts en ce sens ont commencé à apparaître, en particulier à partir des années 1540.
On sait que les débats sur la liberté de la volonté ont continué à s'enflammer (par exemple, dans les fameux échanges entre Érasme et Martin Luther), que les penseurs espagnols étaient de plus en plus obsédés par la notion de noblesse, que le duel était une pratique qui générait une large la littérature au XVIe siècle (était-ce permis ou non ?).
Il ne faut pas oublier que la plupart des philosophes de l'époque étaient des chrétiens au moins de nom, sinon dévots, que le XVIe siècle a vu à la fois les réformes protestantes et catholiques, et que la philosophie de la Renaissance culmine avec la période de la guerre de Trente Ans (1618 –1648).
En conclusion, comme à tout autre moment de l'histoire de la pensée, la philosophie de la Renaissance ne peut être considérée comme ayant apporté quelque chose d'entièrement nouveau ni comme continuant pendant des siècles à répéter les conclusions de ses prédécesseurs.
La philosophie moderne est une philosophie développée à l'ère moderne et associée à la modernité.
Aux XVIIe et XVIIIe siècles, les figures majeures de la philosophie de l'esprit, de l'épistémologie et de la métaphysique étaient grossièrement divisées en deux groupes principaux.
Les « empiristes », au contraire, soutenaient que la connaissance doit commencer par l'expérience sensorielle.
Parmi les autres personnalités importantes de la philosophie politique figurent Thomas Hobbes et Jean-Jacques Rousseau.
Kant a déclenché une tempête de travaux philosophiques en Allemagne au début du XIXe siècle, à commencer par l'idéalisme allemand.
Karl Marx s'est approprié à la fois la philosophie de l'histoire de Hegel et l'éthique empirique dominante en Grande-Bretagne, transformant les idées de Hegel en une forme strictement matérialiste, jetant les bases du développement d'une science de la société.
Arthur Schopenhauer a amené l'idéalisme à la conclusion que le monde n'était rien d'autre que l'interaction futile et sans fin des ibames et des désirs, et a prôné l'athéisme et le pessimisme.
Descartes a soutenu que de nombreuses doctrines métaphysiques scolastiques prédominantes étaient dénuées de sens ou fausses.
Il essaie de mettre de côté autant qu'il le peut toutes ses croyances, pour déterminer ce qu'il sait avec certitude.
A partir de cette base, il reconstruit ses connaissances.
Si l'historicisme reconnaît également le rôle de l'expérience, il diffère de l'empirisme en supposant que les données sensorielles ne peuvent être comprises sans tenir compte des circonstances historiques et culturelles dans lesquelles les observations sont faites.
En tant que tel, l'empirisme se caractérise avant tout par l'idéal de laisser les données d'observation "parler d'elles-mêmes", alors que les points de vue concurrents s'opposent à cet idéal.
En d'autres termes : l'empirisme en tant que concept doit être construit avec d'autres concepts qui, ensemble, permettent de faire des distinctions importantes entre les différents idéaux qui sous-tendent la science contemporaine.
Épistémologiquement, l'idéalisme se manifeste par un scepticisme quant à la possibilité de connaître n'importe quelle chose indépendante de l'esprit.
Il décrit un processus où la théorie est extraite de la pratique et appliquée à la pratique pour former ce qu'on appelle la pratique intelligente.
Brian Leiter (2006) page Web "Philosophie analytique" et "continentale".
La philosophie contemporaine est la période actuelle de l'histoire de la philosophie occidentale qui commence au début du XXe siècle avec la professionnalisation croissante de la discipline et l'essor de la philosophie analytique et continentale.
L'Allemagne a été le premier pays à professionnaliser la philosophie.
En outre, contrairement à de nombreuses sciences pour lesquelles il existe une industrie saine de livres, de bamazines et d'émissions de télévision destinées à vulgariser la science et à communiquer les résultats techniques d'un domaine scientifique au grand public, les travaux de philosophes professionnels dirigés vers un public hors profession restent rares.
Chaque division organise une grande conférence annuelle.
Parmi ses nombreuses autres tâches, l'association est responsable de l'administration de plusieurs des plus hautes distinctions de la profession.
Cette évolution est à peu près contemporaine des travaux de Gottlob Frege et Bertrand Russell inaugurant une nouvelle méthode philosophique basée sur l'analyse du langage par la logique moderne (d'où le terme « philosophie analytique »).
Certains philosophes, tels que Richard Rorty et Simon Glendinning , soutiennent que cette division «analytique-continentale» est hostile à la discipline dans son ensemble.
Par la suite, les philosophes analytiques et continentaux diffèrent sur l'importance et l'influence des philosophes ultérieurs sur leurs traditions respectives.
Bien que, puisque la philosophie analytique et continentale aient des vues si radicalement différentes de la philosophie après Kant, la philosophie continentale est également souvent comprise dans un sens étendu pour inclure tous les philosophes ou mouvements post-kant importants pour la philosophie continentale mais pas la philosophie analytique.
Ainsi la philosophie continentale tend vers l'historicisme, là où la philosophie analytique tend à traiter la philosophie en termes de problèmes discrets, susceptibles d'être analysés indépendamment de leurs origines historiques.
Les principales écoles orthodoxes sont apparues entre le début de l'ère commune et l'empire Gupta.
Ces traditions religio-philosophiques ont ensuite été regroupées sous l'étiquette d'hindouisme.
Les érudits occidentaux considèrent l'hindouisme comme une fusion ou une synthèse de diverses cultures et traditions indiennes, avec des racines diverses et pas de fondateur unique.
Les philosophes indiens ont développé un système de raisonnement épistémologique (pramana) et de logique et ont étudié des sujets tels que l'ontologie (métaphysique, Brahman-Atman, Sunyata-Anatta), des moyens fiables de connaissance (épistémologie, Pramanas), un système de valeurs (axiologie) et d'autres sujets.
Les développements ultérieurs incluent le développement du tantra et des influences irano-islamiques.
Nyāya accepte traditionnellement quatre Pramanas comme moyens fiables d'acquérir des connaissances - Pratyakṣa (perception), Anumāṇa (inférence), Upamāṇa (comparaison et analogie) et Śabda (mot, témoignage d'experts fiables passés ou présents).
Cette philosophie soutenait que l'univers était réductible à paramāṇu (atomes), qui sont indestructibles (anitya), indivisibles et ont une dimension particulière, appelée « petite » (aṇu).
Plus tard, les Vaiśeṣikas (Śrīdhara et Udayana et Śivāditya) ont ajouté une autre catégorie abhava (non-existence).
En raison de leur concentration sur l'étude et l'interprétation textuelles, les Mīmāṃsā ont également développé des théories de la philologie et de la philosophie du langage qui ont influencé d'autres écoles indiennes.
Les caractéristiques distinctives de la philosophie jaïn comprennent un dualisme corps-esprit, le déni d'un Dieu créateur et omnipotent, le karma, un univers éternel et incréé, la non-violence, la théorie des multiples facettes de la vérité et une moralité basée sur la libération de l'âme. .
Il a également été qualifié de modèle de libéralisme philosophique pour son insistance sur le fait que la vérité est relative et à multiples facettes et pour sa volonté de s'adapter à tous les points de vue possibles des philosophies rivales.
Les philosophes Cārvāka comme Brihaspati étaient extrêmement critiques à l'égard des autres écoles de philosophie de l'époque.
C'est la tradition philosophique dominante au Tibet et dans les pays d'Asie du Sud-Est comme le Sri Lanka et la Birmanie.
Les traditions philosophiques bouddhistes ultérieures ont développé des psychologies phénoménologiques complexes appelées «Abhidharma».
Cette tradition a contribué à ce qu'on a appelé un "tournant épistémologique" dans la philosophie indienne.
Parmi les représentants importants du modernisme bouddhiste figurent Anagarika Dharmapala (1864–1933) et le converti américain Henry Steel Olcott, les modernistes chinois Taixu (1890–1947) et Yin Shun (1906–2005), l'érudit zen DT Suzuki et le tibétain Gendün Chöphel ( 1903-1951 ) .
L'anthropologie est l'étude scientifique de l'humanité, concernée par le comportement humain, la biologie humaine, les cultures et les sociétés, dans le présent et le passé, y compris les espèces humaines passées.
L'anthropologie biologique ou physique étudie le développement biologique de l'homme.
Diverses organisations éphémères d'anthropologues avaient déjà été formées.
Lorsque l'esclavage est aboli en France en 1848, la Société est abandonnée.
Pour eux, la publication de De l'origine des espèces de Charles Darwin était l'épiphanie de tout ce qu'ils avaient commencé à soupçonner.
Il y a eu une ruée immédiate pour l'introduire dans les sciences sociales.
Sa définition devient alors « l'étude du groupe humain, considéré dans son ensemble, dans ses détails, et en relation avec le reste de la nature ».
Il a découvert le centre de la parole du cerveau humain, aujourd'hui appelé l'aire de Broca après lui.
Les deux derniers volumes ont été publiés à titre posthume.
Il souligne que les données de comparaison doivent être empiriques, recueillies par expérimentation.
Waitz était influent parmi les ethnologues britanniques.
Des représentants de la Société française étaient présents, mais pas Broca.
Auparavant, Edward s'était qualifié d'ethnologue; puis anthropologue.
Une exception notable était la Société berlinoise d'anthropologie, d'ethnologie et de préhistoire (1869) fondée par Rudolph Virchow, connu pour ses attaques injurieuses contre les évolutionnistes.
Les théoriciens majeurs appartenaient à ces organisations.
L'anthropologie pratique, l'utilisation des connaissances et techniques anthropologiques pour résoudre des problèmes spécifiques, est arrivée ; par exemple, la présence de victimes enterrées pourrait inciter à faire appel à un archéologue médico-légal pour recréer la scène finale.
Cela a été particulièrement important aux États-Unis, des arguments de Boas contre l'idéologie raciale du XIXe siècle, en passant par le plaidoyer de Margaret Mead pour l'égalité des sexes et la libération sexuelle, jusqu'aux critiques actuelles de l'oppression postcoloniale et de la promotion du multiculturalisme.
En Grande-Bretagne et dans les pays du Commonwealth, la tradition britannique de l'anthropologie sociale tend à dominer.
L'anthropologie culturelle est l'étude comparative des multiples façons dont les gens donnent un sens au monde qui les entoure, tandis que l'anthropologie sociale est l'étude des relations entre les individus et les groupes.
Il n'y a pas de distinction absolue entre eux, et ces catégories se chevauchent dans une large mesure.
Ce projet est souvent logé dans le domaine de l'ethnographie.
L'observation participante est l'une des méthodes fondamentales de l'anthropologie sociale et culturelle.
L'étude de la parenté et de l'organisation sociale est au centre de l'anthropologie socioculturelle, car la parenté est un universel humain.
L'ethnographie considère l'expérience de première main et le contexte social comme importants.
L'ethnomusicologie peut être utilisée dans une grande variété de domaines, tels que l'enseignement, la politique, l'anthropologie culturelle, etc.
L'anthropologie économique reste, pour l'essentiel, centrée sur l'échange.
Le premier de ces domaines concernait les sociétés « précapitalistes » qui étaient sujettes à des stéréotypes « tribaux » évolutifs.
Pourquoi ceux qui travaillent dans le développement sont-ils si disposés à ignorer l'histoire et les leçons qu'elle pourrait offrir ?
Au sein de la parenté, vous avez deux familles différentes.
L'anthropologie s'engage souvent avec des féministes de traditions non occidentales, dont les perspectives et les expériences peuvent différer de celles des féministes blanches d'Europe, d'Amérique et d'ailleurs.
L'anthropologie politique s'est développée comme une discipline concernée principalement par la politique dans les sociétés sans État, un nouveau développement a commencé à partir des années 1960 et se poursuit : les anthropologues ont commencé de plus en plus à étudier des contextes sociaux plus « complexes » dans lesquels la présence des États, des bureaucraties et des marchés entraient à la fois récits ethnographiques et analyse des phénomènes locaux.
Deuxièmement, les anthropologues ont lentement commencé à développer une préoccupation disciplinaire avec les États et leurs institutions (et sur la relation entre les institutions politiques formelles et informelles).
Elle est parfois regroupée avec l'anthropologie socioculturelle, et parfois considérée comme faisant partie de la culture matérielle.
C'est aussi l'étude de l'histoire de divers groupes ethniques qui peuvent ou non exister aujourd'hui.
Divers processus sociaux dans le monde occidental ainsi que dans le "tiers monde" (ce dernier étant le centre habituel de l'attention des anthropologues) ont rapproché de chez eux l'attention des "spécialistes des 'autres cultures'".
C'est un domaine interdisciplinaire qui chevauche un certain nombre d'autres disciplines, notamment l'anthropologie, l'éthologie, la médecine, la psychologie, la médecine vétérinaire et la zoologie.
C'est l'étude des anciens humains, comme on le trouve dans les preuves fossiles d'hominidés telles que les os pétrifiés et les empreintes de pas.
En 1989, un groupe d'universitaires européens et américains dans le domaine de l'anthropologie a créé l'Association européenne des anthropologues sociaux (EASA) qui sert d'organisation professionnelle majeure pour les anthropologues travaillant en Europe.
C'est la notion que les cultures ne doivent pas être jugées par les valeurs ou les points de vue d'autrui, mais doivent être examinées sans passion selon leurs propres termes.
Franz Boas s'est publiquement opposé à la participation des États-Unis à la Première Guerre mondiale et, après la guerre, il a publié un bref exposé et une condamnation de la participation de plusieurs archéologues américains à l'espionnage au Mexique sous leur couvert de scientifiques.
Dans le même temps, les travaux de David H. Price sur l'anthropologie américaine pendant la guerre froide fournissent des comptes rendus détaillés de la poursuite et du licenciement de plusieurs anthropologues de leurs emplois pour des sympathies communistes.
De nombreuses résolutions condamnant la guerre sous tous ses aspects ont été adoptées à une écrasante majorité lors des réunions annuelles de l'American Anthropological Association (AAA).
L'Association des anthropologues sociaux du Royaume-Uni et du Commonwealth (ASA) a qualifié certaines bourses d'études de dangereuses sur le plan éthique.
L'une des caractéristiques centrales est que l'anthropologie a tendance à fournir un compte rendu relativement plus holistique des phénomènes et a tendance à être très empirique.
Ces relations dynamiques, entre ce qui peut être observé sur le terrain, par opposition à ce qui peut être observé en compilant de nombreuses observations locales, restent fondamentales dans toute anthropologie, qu'elle soit culturelle, biologique, linguistique ou archéologique.
Sur le plan biologique ou physique, des mesures humaines, des échantillons génétiques, des données nutritionnelles peuvent être recueillies et publiées sous forme d'articles ou de monographies.
D'autres subdivisions culturelles selon les types d'outils, tels que Olduwan ou Moustérien ou Levalloisien aident les archéologues et autres anthropologues à comprendre les grandes tendances du passé humain.
Une norme culturelle codifie une conduite acceptable dans la société ; il sert de ligne directrice pour le comportement, l'habillement, le langage et le comportement dans une situation, qui sert de modèle pour les attentes dans un groupe social.
Celles-ci incluent des formes expressives telles que l'art, la musique, la danse, les rituels, la religion et des technologies telles que l'utilisation d'outils, la cuisine, l'abri et les vêtements.
Le niveau de sophistication culturelle a aussi parfois été utilisé pour distinguer les civilisations des sociétés moins complexes.
La culture de masse fait référence aux formes de culture de consommation produites et médiatisées en masse qui ont émergé au XXe siècle.
Dans les sciences sociales au sens large, la perspective théorique du matérialisme culturel soutient que la culture symbolique humaine découle des conditions matérielles de la vie humaine, car les humains créent les conditions de la survie physique, et que la base de la culture se trouve dans des dispositions biologiques évoluées.
En ce sens, le multiculturalisme valorise la coexistence pacifique et le respect mutuel entre différentes cultures habitant la même planète.
En 1986, le philosophe Edward S. Casey écrivait : « Le mot même culture signifiait « lieu labouré » en moyen anglais, et le même mot remonte au latin colere, « habiter, soigner, cultiver, adorer » et cultus, « A culte, surtout religieux.
Ainsi, un contraste entre "culture" et "civilisation" est généralement sous-entendu chez ces auteurs, même lorsqu'il n'est pas exprimé comme tel.
Cette capacité est apparue avec l'évolution de la modernité comportementale chez l'homme il y a environ 50 000 ans et est souvent considérée comme unique à l'homme.
Rein Raud, s'appuyant sur les travaux d'Umberto Eco, Pierre Bourdieu et Jeffrey C. Alexander, a proposé un modèle de changement culturel basé sur des revendications et des offres, qui sont jugées sur leur adéquation cognitive et endossées ou non par l'autorité symbolique du communauté culturelle en question.
Le repositionnement de la culture signifie la reconstruction du concept culturel d'une société.
Les conflits sociaux et le développement des technologies peuvent produire des changements au sein d'une société en modifiant la dynamique sociale et en promouvant de nouveaux modèles culturels, et en stimulant ou en permettant une action générative.
Les conditions environnementales peuvent également entrer en tant que facteurs.
La guerre ou la concurrence pour les ressources peuvent avoir un impact sur le développement technologique ou la dynamique sociale.
Par exemple, les chaînes de restaurants et les marques culinaires occidentales ont suscité la curiosité et la fascination des Chinois lorsque la Chine a ouvert son économie au commerce international à la fin du XXe siècle. "
Il a fait valoir que cette immaturité ne vient pas d'un manque de compréhension, mais d'un manque de courage pour penser de manière indépendante.
De plus, Herder a proposé une forme collective de Bildung : "Pour Herder, Bildung était la totalité des expériences qui fournissent une identité cohérente et un sens de destin commun à un peuple."
Selon cette école de pensée, chaque groupe ethnique a une vision du monde distincte qui est sans commune mesure avec les visions du monde des autres groupes.
Il a proposé qu'une comparaison scientifique de toutes les sociétés humaines révélerait que des visions du monde distinctes se composaient des mêmes éléments de base.
"un mode de vie particulier, qu'il soit d'un peuple, d'une époque ou d'un groupe."
En d'autres termes, l'idée de « culture » qui s'est développée en Europe au XVIIIe et au début du XIXe siècle reflétait les inégalités au sein des sociétés européennes.
Selon cette façon de penser, on pourrait classer certains pays et nations comme plus civilisés que d'autres et certaines personnes comme plus cultivées que d'autres.
D'autres critiques du XIXe siècle, à la suite de Rousseau, ont accepté cette différenciation entre culture supérieure et inférieure, mais ont vu le raffinement et la sophistication de la haute culture comme des développements corrompus et contre nature qui obscurcissent et déforment la nature essentielle des gens.
En 1870, l'anthropologue Edward Tylor (1832–1917) a appliqué ces idées de culture supérieure par rapport à inférieure pour proposer une théorie de l'évolution de la religion.
Pour le sociologue Georg Simmel (1858–1918), la culture faisait référence à «la culture des individus par l'intermédiaire de formes externes qui ont été objectivées au cours de l'histoire».
La culture non matérielle fait référence aux idées non physiques que les individus ont sur leur culture, y compris les valeurs, les systèmes de croyances, les règles, les normes, la morale, la langue, les organisations et les institutions, tandis que la culture matérielle est la preuve physique d'une culture dans les objets. et l'architecture qu'ils font ou ont fait.
La sociologie culturelle a ensuite été «réinventée» dans le monde anglophone en tant que produit du «tournant culturel» des années 1960, qui a inauguré des approches structuralistes et postmodernes des sciences sociales.
La culture » est depuis devenue un concept important dans de nombreuses branches de la sociologie, y compris des domaines résolument scientifiques comme la stratification sociale et l'analyse des réseaux sociaux.
Ils considéraient les modèles de consommation et de loisirs comme déterminés par les relations de production, ce qui les a amenés à se concentrer sur les relations de classe et l'organisation de la production.
Il est depuis devenu fortement associé à Stuart Hall, qui a succédé à Hoggart en tant que directeur.
Ces pratiques comprennent les façons dont les gens font certaines choses (comme regarder la télévision ou manger au restaurant) dans une culture donnée.
Regarder la télévision pour voir un point de vue public sur un événement historique ne devrait pas être considéré comme de la culture, à moins de se référer au média de la télévision lui-même, qui peut avoir été choisi culturellement ; cependant, les écoliers qui regardent la télévision après l'école avec leurs amis pour «s'intégrer» sont certainement éligibles car il n'y a aucune raison fondée de participer à cette pratique.
La culture » pour un chercheur en études culturelles comprend non seulement la haute culture traditionnelle (la culture des groupes sociaux dominants) et la culture populaire, mais aussi les significations et les pratiques quotidiennes.
Des chercheurs du Royaume-Uni et des États-Unis ont développé des versions quelque peu différentes des études culturelles après la fin des années 1970.
La distinction entre les brins américains et britanniques, cependant, s'est estompée.
L'objectif principal d'une approche marxiste orthodoxe se concentre sur la production de sens.
D'autres approches des études culturelles, telles que les études culturelles féministes et les développements américains ultérieurs du domaine, s'éloignent de cette vision.
Les psychologues de la culture ont commencé à essayer d'explorer la relation entre les émotions et la culture et à déterminer si l'esprit humain est indépendant de la culture.
D'autre part, certains chercheurs tentent de rechercher les différences entre les personnalités des personnes à travers les cultures.
Par exemple, les personnes qui sont élevées dans une culture avec un boulier sont formées avec un style de raisonnement distinctif.
Fondamentalement, la Convention de La Haye pour la protection des biens culturels en cas de conflit armé et la Convention de l'UNESCO pour la protection de la diversité culturelle traitent de la protection de la culture.
En vertu du droit international, l'ONU et l'UNESCO tentent d'établir et d'appliquer des règles à cet égard.
La cible de l'attaque est l'identité de l'adversaire, c'est pourquoi les biens culturels symboliques deviennent une cible principale.
Un festival est un événement généralement célébré par une communauté et centré sur un aspect caractéristique de cette communauté et de sa religion ou de ses cultures.
A côté de la religion et du folklore, une origine importante est agricole.
Les fêtes servent souvent à remplir des objectifs communaux précis, notamment en matière de commémoration ou de remerciement aux dieux, déesses ou saints : on les appelle fêtes patronales.
Dans la Grèce antique et à Rome, des festivals tels que les Saturnales étaient étroitement associés à l'organisation sociale et aux processus politiques ainsi qu'à la religion.
En moyen anglais, un "festival dai" était une fête religieuse.
Le terme «fête» est également utilisé dans le langage profane courant comme synonyme de tout repas copieux ou élaboré.
Les fêtes religieuses les plus importantes telles que Noël, Roch Hachana, Diwali, l'Aïd al-Fitr et l'Aïd al-Adha servent à marquer l'année.
Un des premiers exemples est le festival établi par l'ancien pharaon égyptien Ramsès III célébrant sa victoire sur les Libyens.
Il existe de nombreux types de festivals dans le monde et la plupart des pays célèbrent des événements ou des traditions importantes avec des événements et des activités culturelles traditionnelles.
Les fêtes de l'Égypte ancienne pouvaient être religieuses ou politiques.
Le festival Sed, par exemple, célébrait la trentième année du règne d'un pharaon égyptien, puis tous les trois (ou quatre dans un cas) ans par la suite.
Dans le calendrier liturgique chrétien, il y a deux fêtes principales, connues sous le nom de Fête de la Nativité de notre Seigneur (Noël) et Fête de la Résurrection (Pâques), mais des fêtes mineures en l'honneur des saints patrons locaux sont célébrées dans presque tous pays influencés par le christianisme.
Des festivals religieux bouddhistes, comme Esala Perahera, ont lieu au Sri Lanka et en Thaïlande.
Les festivals de cinéma impliquent la projection de plusieurs films différents et ont généralement lieu chaque année.
Il existe également des festivals de boissons spécifiques, comme le célèbre Oktoberfest en Allemagne pour la bière.
Les anciens Égyptiens comptaient sur les inondations saisonnières causées par le Nil, une forme d'irrigation qui fournissait des terres fertiles pour les cultures.
Le festival Dree des Apatanis vivant dans le district inférieur de Subansiri de l'Arunachal Pradesh est célébré chaque année du 4 au 7 juillet en priant pour une récolte exceptionnelle.
Un jour férié est un jour fixé par la coutume ou par la loi au cours duquel les activités normales, notamment les affaires ou le travail, y compris l'école, sont suspendues ou réduites.
La mesure dans laquelle les activités normales sont réduites par un jour férié peut dépendre des lois locales, des coutumes, du type d'emploi occupé ou de choix personnels.
Dans la plupart des sociétés modernes, cependant, les vacances remplissent autant une fonction récréative que les autres jours ou activités du week-end.
Dans certains cas, un jour férié ne peut être observé que nominalement.
L'utilisation moderne varie géographiquement.
Par exemple, la journée des singes est célébrée le 14 décembre, la journée internationale Talk Like a Pirate est observée le 19 septembre et la journée du blasphème a lieu le 30 septembre.
Les Témoins de Jéhovah commémorent chaque année "Le Mémorial de la mort de Jésus-Christ", mais ne célèbrent pas d'autres fêtes ayant une signification religieuse telles que Pâques, Noël ou le Nouvel An.
Les musulmans ahmadis célèbrent en outre le jour du Messie promis, le jour du réformateur promis et le jour du Khilafat, mais contrairement à la croyance populaire, aucun des deux n'est considéré comme un jour férié.
Les fêtes celtiques, nordiques et neopagan suivent l'ordre de la roue de l'année.
Les chercheurs en bioarchéologie combinent les compétences de l'ostéologie humaine, de la paléopathologie et de l'archéologie, et tiennent souvent compte du contexte culturel et mortuaire des restes.
La psychologie évolutive est l'étude des structures psychologiques dans une perspective évolutive moderne.
L'écologie comportementale humaine est l'étude des adaptations comportementales ( recherche de nourriture , reproduction , ontogénie ) du point de vue évolutif et écologique (voir écologie comportementale ).
La paléoanthropologie est l'étude des preuves fossiles de l'évolution humaine, utilisant principalement des restes d'homininés éteints et d'autres espèces de primates pour déterminer les changements morphologiques et comportementaux dans la lignée humaine, ainsi que l'environnement dans lequel l'évolution humaine s'est produite.
Le nom est même relativement nouveau, ayant été «l'anthropologie physique» pendant plus d'un siècle, certains praticiens appliquant encore ce terme.
Certains éditeurs, voir ci-dessous, ont enraciné le domaine encore plus profondément que la science formelle.
C'est devenu le principal système par lequel les érudits ont pensé à la nature pendant les 2 000 prochaines années environ.
Il a également écrit sur la physionomie, une idée dérivée des écrits du Corpus hippocratique.
Au XIXe siècle, les anthropologues physiques français, dirigés par Paul Broca (1824-1880), se sont concentrés sur la craniométrie tandis que la tradition allemande, dirigée par Rudolf Virchow (1821-1902), a mis l'accent sur l'influence de l'environnement et de la maladie sur le corps humain.
Il a changé l'orientation de la typologie raciale pour se concentrer sur l'étude de l'évolution humaine, s'éloignant de la classification vers le processus évolutif.
Une race est un groupement d'humains basé sur des qualités physiques ou sociales partagées dans des catégories généralement considérées comme distinctes par la société.
La science moderne considère la race comme une construction sociale, une identité qui est attribuée sur la base de règles établies par la société.
D'autres encore soutiennent que, chez les humains, la race n'a aucune signification taxonomique parce que tous les humains vivants appartiennent à la même sous-espèce, Homo sapiens sapiens.
En Afrique du Sud, la loi de 1950 sur l'enregistrement de la population ne reconnaissait que les Blancs, les Noirs et les Métis, les Indiens étant ajoutés plus tard.
Le Bureau du recensement des États-Unis a proposé, mais a ensuite retiré son projet d'ajouter une nouvelle catégorie pour classer les peuples du Moyen-Orient et d'Afrique du Nord dans le recensement américain de 2020, en raison d'un différend sur la question de savoir si cette classification devait être considérée comme une ethnie blanche ou une race distincte.
L'établissement de frontières raciales implique souvent l'assujettissement de groupes définis comme racialement inférieurs, comme dans la règle de la goutte unique utilisée aux États-Unis au XIXe siècle pour exclure ceux qui ont une quelconque ascendance africaine du groupe racial dominant, défini comme « blanc ». ".
Selon le généticien David Reich, "alors que la race peut être une construction sociale, les différences d'ascendance génétique qui se corrèlent à de nombreuses constructions raciales d'aujourd'hui sont réelles".
D'autres dimensions des groupements raciaux comprennent l'histoire, les traditions et la langue partagées.
Les facteurs socio-économiques, combinés à des conceptions anciennes mais persistantes de la race, ont entraîné des souffrances considérables au sein des groupes raciaux défavorisés.
Le racisme a conduit à de nombreux cas de tragédie, y compris l'esclavage et le génocide.
Parce que dans certaines sociétés, les groupements raciaux correspondent étroitement aux modèles de stratification sociale, pour les spécialistes des sciences sociales qui étudient l'inégalité sociale, la race peut être une variable significative.
Par exemple, en 2008, John Hartigan, Jr. a plaidé pour une vision de la race qui se concentre principalement sur la culture, mais qui n'ignore pas la pertinence potentielle de la biologie ou de la génétique.
C'est ainsi que l'idée de race telle que nous la comprenons aujourd'hui est née au cours du processus historique d'exploration et de conquête qui a mis en contact les Européens avec des groupes de différents continents, et de l'idéologie de classification et de typologie que l'on retrouve dans les sciences naturelles.
Un ensemble de croyances populaires s'est imposé qui liait les différences physiques héritées entre les groupes aux qualités intellectuelles, comportementales et morales héritées.
La classification de 1735 de Carl Linnaeus, inventeur de la taxonomie zoologique, divise l'espèce humaine Homo sapiens en variétés continentales d'europaeus, asiaticus, americanus et afer, chacune associée à une humeur différente : sanguine, mélancolique, colérique et flegmatique, respectivement.
Blumenbach a également noté la transition graduelle des apparences d'un groupe à des groupes adjacents et a suggéré qu '"une variété de l'humanité passe si sensiblement dans l'autre, que vous ne pouvez pas marquer les limites entre elles".
Il a en outre été soutenu que certains groupes peuvent être le résultat d'un mélange entre des populations autrefois distinctes, mais qu'une étude approfondie pourrait distinguer les races ancestrales qui s'étaient combinées pour produire des groupes mixtes.
De nouvelles études sur la culture et le domaine naissant de la génétique des populations ont sapé la réputation scientifique de l'essentialisme racial, amenant les anthropologues de la race à réviser leurs conclusions sur les sources de variation phénotypique.
Les études sur la variation génétique humaine montrent que les populations humaines ne sont pas géographiquement isolées et que leurs différences génétiques sont bien moindres que celles des sous-espèces comparables.
Andreasen a cité des diagrammes en arbre des distances génétiques relatives entre les populations publiés par Luigi Cavalli-Sforza comme base d'un arbre phylogénétique des races humaines (p. 661).
Marks, Templeton et Cavalli-Sforza concluent tous que la génétique ne fournit pas de preuves des races humaines.
Par exemple, en ce qui concerne la couleur de la peau en Europe et en Afrique, Brace écrit : À ce jour, la couleur de la peau s'échelonne par des moyens imperceptibles de l'Europe vers le sud autour de l'extrémité orientale de la Méditerranée et du Nil jusqu'en Afrique.
Il a en outre soutenu que l'on pouvait utiliser le terme race si l'on faisait la distinction entre les «différences raciales» et «le concept de race».
En bref, Livingstone et Dobzhansky conviennent qu'il existe des différences génétiques entre les êtres humains ; ils conviennent également que l'utilisation du concept de race pour classer les personnes et la manière dont le concept de race est utilisé sont une question de convention sociale.
Comme l'ont observé les anthropologues Leonard Lieberman et Fatimah Linda Jackson, "des modèles discordants d'hétérogénéité falsifient toute description d'une population comme si elle était génotypiquement ou même phénotypiquement homogène".
L'anthropologue du milieu du XXe siècle, William C. Boyd, a défini la race comme : "Une population qui diffère considérablement des autres populations en ce qui concerne la fréquence d'un ou plusieurs des gènes qu'elle possède.
De plus, l'anthropologue Stephen Molnar a suggéré que la discordance des clines entraîne inévitablement une multiplication des races qui rend le concept lui-même inutile.
Joanna Mountain et Neil Risch ont averti que si les clusters génétiques pourraient un jour s'avérer correspondre à des variations phénotypiques entre les groupes, de telles hypothèses étaient prématurées car la relation entre les gènes et les traits complexes reste mal comprise.
Toute catégorie que vous proposez sera imparfaite, mais cela ne vous empêche pas de l'utiliser ou du fait qu'elle a une utilité."
Cela supposait trois groupes de population séparés par de vastes aires géographiques (européen, africain et est-asiatique).
Des anthropologues tels que C. Loring Brace, les philosophes Jonathan Kaplan et Rasmus Winther, et le généticien Joseph Graves, ont soutenu que là-bas, il est certainement possible de trouver des variations biologiques et génétiques qui correspondent à peu près aux groupements normalement définis comme " races continentales ". , cela est vrai pour presque toutes les populations géographiquement distinctes.
Weiss et Fullerton ont noté que si l'on échantillonnait uniquement des Islandais, des Mayas et des Maoris, trois groupes distincts se formeraient et toutes les autres populations pourraient être décrites comme étant cliniquement composées de mélanges de matériel génétique maori, islandais et maya.
De plus, les données génomiques sous-déterminent si l'on souhaite voir des subdivisions (c'est-à-dire des séparateurs) ou un continuum (c'est-à-dire des lumpers).
Outre les problèmes empiriques et conceptuels liés à la «race», après la Seconde Guerre mondiale, les spécialistes de l'évolution et des sciences sociales étaient parfaitement conscients de la manière dont les croyances sur la race avaient été utilisées pour justifier la discrimination, l'apartheid, l'esclavage et le génocide.
Craig Venter et Francis Collins du National Institute of Health ont annoncé conjointement la cartographie du génome humain en 2000.
Ce n'est pas scientifique.
L'anthropologue Stephan Palmié a soutenu que la race "n'est pas une chose mais une relation sociale"; ou, selon les mots de Katya Gibel Mevorach, "une métonymie", "une invention humaine dont les critères de différenciation ne sont ni universels ni fixes mais ont toujours été utilisés pour gérer la différence".
Là-bas, l'identité raciale n'était pas régie par une règle de filiation rigide, telle que la règle de la goutte unique, comme c'était le cas aux États-Unis.
Ces types se classent les uns dans les autres comme les couleurs du spectre, et aucune catégorie n'est significativement isolée des autres.
New Jersey : Prentice Hall Inc, 1984.
Dans le contexte européen, la résonance historique de la « race » souligne sa nature problématique.
Le concept d'origine raciale repose sur l'idée que les êtres humains peuvent être séparés en « races » biologiquement distinctes, une idée généralement rejetée par la communauté scientifique.
Aux États-Unis, la plupart des personnes qui s'identifient comme afro-américaines ont des ancêtres européens, tandis que de nombreuses personnes qui s'identifient comme européennes américaines ont des ancêtres africains ou amérindiens.
Les critères d'appartenance à ces races ont divergé à la fin du XIXe siècle.
Les Amérindiens continuent d'être définis par un certain pourcentage de "sang indien" (appelé quantum sanguin).
Cette règle signifiait que ceux qui étaient métis mais avec une ascendance africaine perceptible étaient définis comme noirs.
Le terme «hispanique» en tant qu'ethnonyme est apparu au XXe siècle avec la montée de la migration des travailleurs des pays hispanophones d'Amérique latine vers les États-Unis.
Trois facteurs, le pays de formation universitaire, la discipline et l'âge, se sont avérés significatifs pour différencier les réponses.
En 2007, Ann Morning a interviewé plus de 40 biologistes et anthropologues américains et a trouvé des désaccords importants sur la nature de la race, aucun point de vue ne détenant la majorité parmi les deux groupes.
Bien qu'il puisse voir de bons arguments pour les deux parties, le déni complet des preuves opposées "semble provenir en grande partie d'une motivation socio-politique et pas du tout de la science".
En réponse partielle à la déclaration de Gill, le professeur d'anthropologie biologique C. Loring Brace soutient que la raison pour laquelle les profanes et les anthropologues biologiques peuvent déterminer l'ascendance géographique d'un individu peut s'expliquer par le fait que les caractéristiques biologiques sont réparties cliniquement sur la planète, et cela ne ne se traduit pas par le concept de race.
Les textes d'anthropologie physique soutenaient que les races biologiques existaient jusque dans les années 1970, lorsqu'ils ont commencé à affirmer que les races n'existaient pas.
En février 2001, les éditeurs des Archives of Pediatrics and Adolescent Medicine ont demandé "aux auteurs de ne pas utiliser la race et l'ethnicité lorsqu'il n'y a aucune raison biologique, scientifique ou sociologique de le faire".
Morning (2008) a examiné les manuels de biologie du secondaire au cours de la période 1952–2002 et a initialement trouvé une tendance similaire avec seulement 35% discutant directement de la race dans la période 1983–92 contre 92% initialement.
En général, le matériel sur la race est passé des traits de surface à la génétique et à l'histoire de l'évolution.
Elle note: "Au mieux, on peut conclure que les biologistes et les anthropologues semblent désormais également divisés dans leurs croyances sur la nature de la race."
33 chercheurs en services de santé de différentes régions géographiques ont été interrogés dans une étude de 2008.
De nombreux sociologues se sont concentrés sur les Afro-Américains, appelés Noirs à l'époque, et ont affirmé qu'ils étaient inférieurs aux Blancs.
Sa solution était largement basée sur le travail d'Al-Khwarizmi.
Cependant, à un moment donné, la formule quadratique commence à perdre de sa précision en raison d'une erreur d'arrondi, tandis que la méthode approximative continue de s'améliorer.
Il existait des méthodes d'approximation numérique, appelées prosthahérèse, qui offraient des raccourcis autour d'opérations chronophages telles que la multiplication et la prise de puissances et de racines.
Les algorithmes de calcul pour trouver les solutions sont une partie importante de l'algèbre linéaire numérique et jouent un rôle de premier plan dans l'ingénierie, la physique, la chimie, l'informatique et l'économie.
Pour les solutions dans un domaine intégral comme l'anneau des entiers, ou dans d'autres structures algébriques, d'autres théories ont été développées, voir Équation linéaire sur un anneau .
Cela permet de faire intervenir tout le langage et la théorie des espaces vectoriels (ou plus généralement des modules).
Un tel système est appelé système sous-déterminé.
Le deuxième système a une seule solution unique, à savoir l'intersection des deux lignes.
Deux de ces équations ont une solution commune.
Un système d'équations dont les membres gauches sont linéairement indépendants est toujours cohérent.
Cela donne un système d'équations avec une équation de moins et une inconnue de moins.
Type 3 : Ajouter à une ligne un multiple scalaire d'une autre.
Par exemple, les systèmes avec une matrice définie positive symétrique peuvent être résolus deux fois plus vite avec la décomposition de Cholesky.
Une approche complètement différente est souvent adoptée pour les très grands systèmes, qui prendraient autrement trop de temps ou de mémoire.
Cela conduit à la classe des méthodes itératives.
En mathématiques, une série est, grosso modo, une description de l'opération consistant à ajouter une infinité de quantités, les unes après les autres, à une quantité de départ donnée.
Outre leur omniprésence en mathématiques, les séries infinies sont également largement utilisées dans d'autres disciplines quantitatives telles que la physique, l'informatique, les statistiques et la finance.
Le paradoxe de Zénon d'Achille et de la tortue illustre cette propriété contre-intuitive des sommes infinies : Achille court après une tortue, mais lorsqu'il atteint la position de la tortue au début de la course, la tortue a atteint une seconde position ; lorsqu'il atteint cette deuxième position, la tortue est à une troisième position, et ainsi de suite.
Cet argument ne prouve pas que la somme est égale à 2 (bien qu'elle le soit), mais il prouve qu'elle est au plus égale à 2.
Les tests de convergence uniforme comprennent le test M de Weierstrass, le test de convergence uniforme d'Abel, le test de Dini et le critère de Cauchy.
La convergence est uniforme sur des sous-ensembles fermés et bornés (c'est-à-dire compacts) de l'intérieur du disque de convergence : c'est-à-dire qu'elle est uniformément convergente sur des ensembles compacts.
La série Hilbert – Poincaré est une série formelle de puissance utilisée pour étudier les algèbres graduées.
Au XVIIe siècle, James Gregory travaille dans le nouveau système décimal sur les séries infinies et publie plusieurs séries de Maclaurin.
Cauchy (1821) a insisté sur des tests stricts de convergence ; il a montré que si deux séries sont convergentes leur produit ne l'est pas nécessairement, et avec lui commence la découverte des critères effectifs.
Une méthode de sommabilité est une telle affectation d'une limite à un sous-ensemble de l'ensemble des séries divergentes qui étend correctement la notion classique de convergence.
Les érudits indiens utilisent des formules factorielles depuis au moins le 12ème siècle.
Dans les langages fonctionnels, la définition récursive est souvent implémentée directement pour illustrer les fonctions récursives.
D'autres implémentations (telles que les logiciels informatiques tels que les tableurs) peuvent souvent gérer des valeurs plus importantes.
Comparé à la définition Pickover du superfactoriel, l'hyperfactoriel croît relativement lentement.
Il n'y a, relativement parlant, aucune solution aussi simple pour les factorielles ; aucune combinaison finie de sommes, de produits, de puissances, de fonctions exponentielles ou de logarithmes ne suffira à exprimer ; mais il est possible de trouver une formule générale pour les factorielles en utilisant des outils tels que les intégrales et les limites du calcul.
Les intégrales dont nous avons discuté jusqu'ici impliquent des fonctions transcendantales, mais la fonction gamma provient également d'intégrales de fonctions purement algébriques.
En prenant des limites, certains produits rationnels avec une infinité de facteurs peuvent également être évalués en fonction de la fonction gamma.
Son histoire, documentée notamment par Philip J. Davis dans un article qui lui valut le prix Chauvenet en 1963, reflète nombre des évolutions majeures des mathématiques depuis le XVIIIe siècle.
Au lieu de trouver une preuve spécialisée pour chaque formule, il serait souhaitable d'avoir une méthode générale d'identification de la fonction gamma.
Cependant, la fonction gamma ne semble satisfaire aucune équation différentielle simple.
Le théorème de Bohr-Mollerup est utile car il est relativement facile de prouver la convexité logarithmique pour l'une des différentes formules utilisées pour définir la fonction gamma.
Au fur et à mesure que les ordinateurs électroniques devenaient disponibles pour la production de tableaux dans les années 1950, plusieurs tableaux détaillés pour la fonction gamma complexe ont été publiés pour répondre à la demande, y compris un tableau précis à 12 décimales du US National Bureau of Standards.
En science, une formule est une manière concise d'exprimer symboliquement des informations, comme dans une formule mathématique ou une formule chimique.
En mathématiques, une formule fait généralement référence à une identité qui assimile une expression mathématique à une autre, les plus importantes étant les théorèmes mathématiques.
Cette convention, bien que moins importante dans une formule relativement simple, signifie que les mathématiciens peuvent manipuler plus rapidement des formules plus grandes et plus complexes.
Par exemple, H2O est la formule chimique de l'eau, spécifiant que chaque molécule est constituée de deux atomes d'hydrogène (H) et d'un atome d'oxygène (O).
Dans les formules empiriques, ces proportions commencent par un élément clé, puis attribuent des nombres d'atomes des autres éléments du composé, sous forme de rapports à l'élément clé.
Certains types de composés ioniques, cependant, ne peuvent pas être écrits sous forme de formules empiriques qui ne contiennent que des nombres entiers.
Il existe plusieurs types de ces formules, y compris les formules moléculaires et les formules condensées.
Les fonctions étaient à l'origine l'idéalisation de la façon dont une quantité variable dépend d'une autre quantité.
Cette définition de "graphe" fait référence à un ensemble de paires d'objets.
Lorsque le domaine et le codomaine sont des ensembles de nombres réels, chacune de ces paires peut être considérée comme les coordonnées cartésiennes d'un point dans le plan.
Parfois, il peut être identifié avec la fonction, mais cela cache l'interprétation habituelle d'une fonction en tant que processus.
Une carte peut avoir n'importe quel ensemble comme codomaine, alors que, dans certains contextes, généralement dans des livres plus anciens, le codomaine d'une fonction est spécifiquement l'ensemble de nombres réels ou complexes.
Un autre exemple courant est la fonction d'erreur.
Les séries de puissance peuvent être utilisées pour définir des fonctions sur le domaine dans lequel elles convergent.
Ensuite, la série de puissances peut être utilisée pour élargir le domaine de la fonction.
Certaines parties de cela peuvent créer un graphique qui représente (des parties de) la fonction.
C'est la factorisation canonique de .
À cette époque, seules les fonctions à valeur réelle d'une variable réelle étaient considérées et toutes les fonctions étaient supposées lisses.
Les fonctions sont maintenant utilisées dans tous les domaines des mathématiques.
C'est ainsi que les fonctions trigonométriques inverses sont définies en termes de fonctions trigonométriques, où les fonctions trigonométriques sont monotones.
L'utilité du concept de fonctions à valeurs multiples est plus claire lorsque l'on considère des fonctions complexes, généralement des fonctions analytiques.
Une telle fonction est appelée la valeur principale de la fonction.
La programmation fonctionnelle est le paradigme de programmation consistant à construire des programmes en utilisant uniquement des sous-programmes qui se comportent comme des fonctions mathématiques.
À l'exception de la terminologie du langage informatique, "fonction" a le sens mathématique habituel en informatique.
Les termes sont manipulés à travers certaines règles (la -équivalence, la -réduction et la -conversion), qui sont les axiomes de la théorie et peuvent être interprétées comme des règles de calcul.
Nicolas Chuquet a utilisé une forme de notation exponentielle au XVe siècle, qui a ensuite été utilisée par Henricus Grammateus et Michael Stifel au XVIe siècle.
Ainsi, ils écriraient des polynômes, par exemple, comme .
Le résultat est toujours un nombre réel positif, et les identités et propriétés présentées ci-dessus pour les exposants entiers restent vraies avec ces définitions pour les exposants réels.
Cette fonction est égale à la racine ème habituelle pour les radicandes réels positifs.
C'est le point de départ de la théorie mathématique des semi-groupes.
Nous pouvons à nouveau remplacer l'ensemble N par un nombre cardinal n pour obtenir Vn, bien que sans choisir un ensemble standard spécifique de cardinal n, celui-ci n'est défini qu'à isomorphisme près.
Nicolas Bourbaki, Éléments de mathématiques, théorie des ensembles, Springer-Verlag, 2004, III.§3.5.
L'itération de la tétration conduit à une autre opération, et ainsi de suite, un concept nommé hyperopération.
Dans les paramètres appliqués, les fonctions exponentielles modélisent une relation dans laquelle un changement constant de la variable indépendante donne le même changement proportionnel (c'est-à-dire un pourcentage d'augmentation ou de diminution) de la variable dépendante.
Cette propriété de fonction conduit à une croissance exponentielle ou à une décroissance exponentielle.
De même, la composition des fonctions onto (surjectives) est toujours sur.
On peut alors former des chaînes de transformations composées ensemble, telles que .
Cette notation alternative est appelée notation postfixée.
La catégorie des ensembles à fonctions comme morphismes est la catégorie prototypique.
Par exemple, le décibel (dB) est une unité utilisée pour exprimer le rapport sous forme de logarithmes, principalement pour la puissance et l'amplitude du signal (dont la pression acoustique est un exemple courant).
Ils aident à décrire les rapports de fréquence des intervalles musicaux, apparaissent dans des formules comptant des nombres premiers ou des factorielles approximatives, informent certains modèles en psychophysique et peuvent aider à la comptabilité médico-légale.
Le prochain entier est 4, qui est le nombre de chiffres de 1430.
Avant l'invention de Napier, il existait d'autres techniques de portées similaires, comme la prosthaphaérèse ou l'utilisation de tables de progressions, largement développées par Jost Bürgi vers 1600.
Parler d'un nombre comme nécessitant autant de chiffres est une allusion grossière au logarithme commun, et a été désigné par Archimède comme «l'ordre d'un nombre».
Ces méthodes sont appelées prosthaphaeresis.
Par exemple, chaque chambre de la coquille d'un nautile est une copie approximative de la suivante, mise à l'échelle par un facteur constant.
Les logarithmes sont également liés à l'auto-similarité.
Il est utilisé pour quantifier la perte de niveaux de tension dans la transmission de signaux électriques, pour décrire les niveaux de puissance des sons en acoustique et l'absorbance de la lumière dans les domaines de la spectrométrie et de l'optique.
Le vinaigre a généralement un pH d'environ 3.
Cette "loi", cependant, est moins réaliste que les modèles plus récents, comme la loi de puissance de Stevens.)
Lorsque le logarithme d'une variable aléatoire a une distribution normale, on dit que la variable a une distribution log-normale.
Pour un tel modèle, la fonction de vraisemblance dépend d'au moins un paramètre qu'il faut estimer.
De même, l'algorithme de tri par fusion trie une liste non triée en divisant la liste en deux et en les triant d'abord avant de fusionner les résultats.
Les exposants de Lyapunov utilisent des logarithmes pour évaluer le degré de chaoticité d'un système dynamique.
Le triangle de Sierpinski (photo) peut être recouvert de trois copies de lui-même, chacun ayant des côtés la moitié de la longueur d'origine.
Un autre exemple est le logarithme p-adique, la fonction inverse de l'exponentielle p-adique.
L'exponentiation peut être effectuée efficacement, mais le logarithme discret est considéré comme très difficile à calculer dans certains groupes.
Les racines carrées des nombres négatifs peuvent être discutées dans le cadre des nombres complexes.
Dans l'Inde ancienne, la connaissance des aspects théoriques et appliqués de la racine carrée et carrée était au moins aussi ancienne que les Sulba Sutras, datés d'environ 800 à 500 avant JC (peut-être beaucoup plus tôt).
La lettre jīm ressemble à la forme actuelle de la racine carrée.
Il définit un concept important d'écart type utilisé dans la théorie des probabilités et les statistiques.
La plupart des calculatrices de poche ont une clé racine carrée.
La complexité temporelle du calcul d'une racine carrée avec n chiffres de précision est équivalente à celle de la multiplication de deux nombres à n chiffres.
Les problèmes de Hilbert sont vingt-trois problèmes de mathématiques publiés par le mathématicien allemand David Hilbert en 1900.
Pour d'autres problèmes, tels que le 5ème, les experts se sont traditionnellement mis d'accord sur une interprétation unique, et une solution à l'interprétation acceptée a été donnée, mais il existe des problèmes non résolus étroitement liés.
Il y a deux problèmes qui non seulement ne sont pas résolus mais qui peuvent en fait être insolubles selon les normes modernes.
Les vingt et un autres problèmes ont tous fait l'objet d'une attention particulière et, à la fin du XXe siècle, les travaux sur ces problèmes étaient encore considérés comme de la plus haute importance.
Hilbert a vécu 12 ans après que Kurt Gödel a publié son théorème, mais ne semble pas avoir écrit de réponse formelle au travail de Gödel.
En discutant de son opinion selon laquelle chaque problème mathématique devrait avoir une solution, Hilbert admet la possibilité que la solution puisse être une preuve que le problème original est impossible.
Le premier d'entre eux a été prouvé par Bernard Dwork ; une preuve complètement différente des deux premiers, via la cohomologie ℓ-adique, a été donnée par Alexander Grothendieck.
Cependant, les conjectures de Weil ressemblaient davantage, dans leur portée, à un seul problème de Hilbert, et Weil ne les a jamais conçues comme un programme pour toutes les mathématiques.
Erdős offrait souvent des récompenses monétaires; la taille de la récompense dépendait de la difficulté perçue du problème.
Au moins dans les médias grand public, l'analogue de facto des problèmes de Hilbert au XXIe siècle est la liste des sept problèmes du prix du millénaire choisis en 2000 par le Clay Mathematics Institute.
L'hypothèse de Riemann est remarquable pour son apparition sur la liste des problèmes de Hilbert, la liste de Smale, la liste des problèmes du prix du millénaire et même les conjectures de Weil, sous sa forme géométrique.
1931, 1936 3ème Étant donné deux polyèdres de volume égal, est-il toujours possible de couper le premier en un nombre fini de morceaux polyédriques qui peuvent être réassemblés pour donner le second ?
— 12e Étendre le théorème de Kronecker–Weber sur les extensions abéliennes des nombres rationnels à tout corps de nombres de base.
1959 15e Fondement rigoureux du calcul énumératif de Schubert.
1927 18e (a) Existe-t-il un polyèdre qui n'admette qu'un pavage anisoédrique en trois dimensions ?(b) Quel est le garnissage de sphères le plus dense ?
Un nombre est un objet mathématique utilisé pour compter, mesurer et étiqueter.
Plus universellement, les nombres individuels peuvent être représentés par des symboles, appelés chiffres ; par exemple, "5" est un chiffre qui représente le nombre cinq.
Les calculs avec des nombres sont effectués avec des opérations arithmétiques, les plus connues étant l'addition, la soustraction, la multiplication, la division et l'exponentiation.
Gilsdorf, Thomas E. Introduction to Cultural Mathematics: With Case Studies in the Otomies and Incas, John Wiley & Sons, 24 février 2012.Restivo, S. Mathematics in Society and History, Springer Science & Business Media, 30 novembre 1992.
Au cours du XIXe siècle, les mathématiciens ont commencé à développer de nombreuses abstractions différentes qui partagent certaines propriétés des nombres et peuvent être considérées comme une extension du concept.
Un système de comptage n'a pas de concept de valeur de position (comme dans la notation décimale moderne), ce qui limite sa représentation des grands nombres.
Le Brāhmasphuṭasiddhānta de Brahbamupta est le premier livre qui mentionne zéro comme nombre, c'est pourquoi Brahbamupta est généralement considéré comme le premier à formuler le concept de zéro.
Dans le même ordre d'idées, Pāṇini (5ème siècle avant JC) a utilisé l'opérateur nul (zéro) dans l' Ashtadhyayi , un des premiers exemples de grammaire algébrique pour la langue sanskrite (voir également Pingala ).
En 130 après JC, Ptolémée, influencé par Hipparque et les Babyloniens, utilisait un symbole pour 0 (un petit cercle avec une longue barre supérieure) dans un système numérique sexagésimal utilisant autrement des chiffres grecs alphabétiques.
La référence précédente de Diophantus a été discutée plus explicitement par le mathématicien indien Brahbamupta, dans Brāhmasphuṭasiddhānta en 628, qui a utilisé des nombres négatifs pour produire la formule quadratique de forme générale qui reste en usage aujourd'hui.
Dans le même temps, les Chinois indiquaient les nombres négatifs en traçant un trait diagonal à travers le chiffre non nul le plus à droite du chiffre du nombre positif correspondant.
Les mathématiciens grecs et indiens classiques ont étudié la théorie des nombres rationnels, dans le cadre de l'étude générale de la théorie des nombres.
Le concept de fractions décimales est étroitement lié à la notation décimale des valeurs de position ; les deux semblent s'être développés en tandem.
Cependant, Pythagore croyait au caractère absolu des nombres et ne pouvait accepter l'existence de nombres irrationnels.
Au 17ème siècle, les mathématiciens utilisaient généralement des fractions décimales avec une notation moderne.
En 1872, la publication des théories de Karl Weierstrass (par son élève E. Kossak), Eduard Heine, Georg Cantor et Richard Dedekind est provoquée.
Weierstrass, Cantor et Heine fondent leurs théories sur les séries infinies, tandis que Dedekind fonde la sienne sur l'idée d'une coupure (Schnitt) dans le système des nombres réels, séparant tous les nombres rationnels en deux groupes ayant certaines propriétés caractéristiques.
Il était donc nécessaire de considérer l'ensemble plus large des nombres algébriques (toutes les solutions aux équations polynomiales).
Aristote a défini la notion occidentale traditionnelle d'infini mathématique.
Mais la prochaine avancée majeure dans la théorie a été faite par Georg Cantor ; en 1895, il publie un livre sur sa nouvelle théorie des ensembles, introduisant, entre autres, les nombres transfinis et formulant l'hypothèse du continu.
Une version géométrique moderne de l'infini est donnée par la géométrie projective, qui introduit des "points idéaux à l'infini", un pour chaque direction spatiale.
L'idée de la représentation graphique des nombres complexes était pourtant apparue dès 1685, dans le De algebra tractatus de Wallis.
En 240 av. J.-C., Ératosthène utilisa le crible d'Ératosthène pour isoler rapidement les nombres premiers.
D'autres résultats concernant la distribution des nombres premiers incluent la preuve d'Euler que la somme des inverses des nombres premiers diverge, et la conjecture de Goldbach, qui prétend que tout nombre pair suffisamment grand est la somme de deux nombres premiers.
Traditionnellement, la séquence des nombres naturels commençait par 1 (0 n'était même pas considéré comme un nombre pour les Grecs de l'Antiquité.)
Dans ce système de base 10, le chiffre le plus à droite d'un nombre naturel a une valeur de position de 1, et tous les autres chiffres ont une valeur de position dix fois supérieure à celle de la valeur de position du chiffre à sa droite.
Les nombres négatifs sont généralement écrits avec un signe négatif (un signe moins).
Ici vient la lettre Z.
Les fractions peuvent être supérieures, inférieures ou égales à 1 et peuvent également être positives, négatives ou 0.
Le paragraphe suivant se concentrera principalement sur les nombres réels positifs.
Ainsi, par exemple, un demi vaut 0,5, un cinquième vaut 0,2, un dixième vaut 0,1 et un cinquantième vaut 0,02.
Non seulement ces exemples importants, mais presque tous les nombres réels sont irrationnels et n'ont donc pas de motifs répétitifs et donc pas de nombre décimal correspondant.
Étant donné que même le deuxième chiffre après la décimale n'est pas conservé, les chiffres suivants ne sont pas significatifs.
Par exemple, 0,999..., 1,0, 1,00, 1,000, ..., tous représentent le nombre naturel 1.
Enfin, si tous les chiffres d'un chiffre sont 0, le nombre est 0, et si tous les chiffres d'un chiffre sont une chaîne sans fin de 9, vous pouvez déposer les neuf à droite de la décimale et ajouter un à la chaîne de 9 à gauche de la décimale.
Ainsi, les nombres réels sont un sous-ensemble des nombres complexes.
Le théorème fondamental de l'algèbre affirme que les nombres complexes forment un corps algébriquement clos, ce qui signifie que tout polynôme à coefficients complexes a une racine dans les nombres complexes.
Les nombres premiers ont été largement étudiés depuis plus de 2000 ans et ont conduit à de nombreuses questions, dont seules certaines ont trouvé une réponse.
Les nombres réels qui ne sont pas des nombres rationnels sont appelés nombres irrationnels.
Les nombres calculables sont stables pour toutes les opérations arithmétiques usuelles, y compris le calcul des racines d'un polynôme, et forment ainsi un véritable corps fermé qui contient les nombres algébriques réels.
L'une des raisons est qu'il n'existe pas d'algorithme pour tester l'égalité de deux nombres calculables.
Le système numérique qui en résulte dépend de la base utilisée pour les chiffres : n'importe quelle base est possible, mais une base de nombres premiers fournit les meilleures propriétés mathématiques.
Le premier donne l'ordre de l'ensemble, tandis que le second donne sa taille.
Cette base standard fait des nombres complexes un plan cartésien, appelé plan complexe.
Les nombres complexes de valeur absolue un forment le cercle unité.
Dans la coloration de domaine, les dimensions de sortie sont représentées respectivement par la couleur et la luminosité.
Les travaux sur le problème des polynômes généraux ont finalement conduit au théorème fondamental de l'algèbre, qui montre qu'avec les nombres complexes, une solution existe pour chaque équation polynomiale de degré un ou plus.
Les mémoires de Wessel sont apparus dans les Actes de l'Académie de Copenhague mais sont passés largement inaperçus.
Les écrivains classiques ultérieurs sur la théorie générale incluent Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass et bien d'autres.
L'utilisation des nombres ibaminaires n'a pas été largement acceptée jusqu'aux travaux de Leonhard Euler (1707–1783) et de Carl Friedrich Gauss (1777–1855).
Les nombres entiers forment le plus petit groupe et le plus petit anneau contenant les nombres naturels.
C'est le prototype de tous les objets d'une telle structure algébrique.
Les types de données d'approximation d'entier de longueur fixe (ou sous-ensembles) sont notés int ou Integer dans plusieurs langages de programmation (tels que Algol68, C, Java, Delphi, etc.).
Ce sont des propriétés prouvables des nombres rationnels et des systèmes de nombres positionnels, et ne sont pas utilisées comme définitions en mathématiques.
Puisque le triangle est isocèle, a = b).
Comme c est pair, diviser c par 2 donne un entier.
Remplacer 4y2 par c2 dans la première équation (c2 = 2b2) nous donne 4y2= 2b2.
Puisque b2 est pair, b doit être pair.
Cependant, cela contredit l'hypothèse selon laquelle ils n'ont pas de facteurs communs.
Hippase, cependant, n'a pas été loué pour ses efforts : selon une légende, il a fait sa découverte alors qu'il était en mer, et a ensuite été jeté par-dessus bord par ses compatriotes pythagoriciens "... pour avoir produit un élément dans l'univers qui a nié la... doctrine qui tous les phénomènes de l'univers peuvent être réduits à des nombres entiers et à leurs rapports.
Par exemple, considérons un segment de ligne : ce segment peut être divisé en deux, cette moitié divisée en deux, la moitié de la moitié en deux, et ainsi de suite.
C'est exactement ce que Zénon a cherché à prouver.
Dans l'esprit des Grecs, réfuter la validité d'un point de vue ne prouvait pas nécessairement la validité d'un autre, et donc une enquête plus approfondie devait avoir lieu.
Une bamnitude «... n'était pas un nombre mais représentait des entités telles que des segments de ligne, des angles, des aires, des volumes et du temps qui pouvaient varier, pour ainsi dire, de façon continue.
Parce qu'aucune valeur quantitative n'a été attribuée aux bamnitudes, Eudoxe a alors pu rendre compte à la fois des rapports commensurables et incommensurables en définissant un rapport en termes de sa bamnitude et une proportion comme une égalité entre deux rapports.
Cette incommensurabilité est traitée dans les Éléments d'Euclide, Livre X, Proposition 9.
En fait, dans de nombreux cas, les conceptions algébriques ont été reformulées en termes géométriques.
La prise de conscience qu'une conception de base de la théorie existante était en contradiction avec la réalité a nécessité une enquête complète et approfondie sur les axiomes et les hypothèses qui sous-tendent cette théorie.
Cependant, l'historien Carl Benjamin Boyer écrit que "de telles affirmations ne sont pas bien étayées et peu susceptibles d'être vraies".
Des mathématiciens comme Brahbamupta (en 628 après JC) et Bhāskara I (en 629 après JC) ont apporté des contributions dans ce domaine, tout comme d'autres mathématiciens qui ont suivi.
L'année 1872 voit la publication des théories de Karl Weierstrass (par son élève Ernst Kossak), Eduard Heine (Crelle's Journal, 74), Georg Cantor (Annalen, 5) et Richard Dedekind.
Weierstrass, Cantor et Heine fondent leurs théories sur les séries infinies, tandis que Dedekind fonde la sienne sur l'idée d'une coupure (Schnitt) dans le système de tous les nombres rationnels, les séparant en deux groupes ayant certaines propriétés caractéristiques.
Dirichlet a également ajouté à la théorie générale, tout comme de nombreux contributeurs aux applications du sujet.
Cela affirme que chaque entier a une factorisation unique en nombres premiers.
Pour le montrer, supposons que nous divisons les entiers n par m (où m est différent de zéro).
Si 0 ne se produit jamais, alors l'algorithme peut exécuter au plus m - 1 étapes sans utiliser de reste plus d'une fois.
En mathématiques, les nombres naturels sont ceux utilisés pour compter (comme dans "il y a six pièces sur la table") et ordonner (comme dans "c'est la troisième plus grande ville du pays").
Ces chaînes d'extensions font que les nombres naturels sont canoniquement intégrés (identifiés) dans les autres systèmes de nombres.
La première avancée majeure dans l'abstraction a été l'utilisation de chiffres pour représenter les nombres.
